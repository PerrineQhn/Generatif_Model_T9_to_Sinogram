{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de1c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/zhongjie/.cache/kagglehub/datasets/noxmoon/chinese-official-daily-news-since-2016/versions/1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import kagglehub\n",
    "import keras\n",
    "import keras_hub\n",
    "import keras_tuner\n",
    "import pandas as pd\n",
    "import pkuseg\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from pypinyin import lazy_pinyin\n",
    "from keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    StringLookup,\n",
    "    TextVectorization,\n",
    ")\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"noxmoon/chinese-official-daily-news-since-2016\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3e85f",
   "metadata": {},
   "source": [
    "# Création du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1de02ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20738 entries, 0 to 20737\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   date      20738 non-null  object\n",
      " 1   tag       20738 non-null  object\n",
      " 2   headline  20738 non-null  object\n",
      " 3   content   20631 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 648.2+ KB\n",
      "None\n",
      "Dataset head:\n",
      "         date   tag                                           headline  \\\n",
      "0  2016-01-01  详细全文  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...   \n",
      "1  2016-01-01  详细全文                             中央军委印发《关于深化国防和军队改革的意见》   \n",
      "2  2016-01-01  详细全文                           《习近平关于严明党的纪律和规矩论述摘编》出版发行   \n",
      "3  2016-01-01  详细全文                                 以实际行动向党中央看齐 向高标准努力   \n",
      "4  2016-01-01  详细全文                                 【年终特稿】关键之年 改革挺进深水区   \n",
      "\n",
      "                                             content  \n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...  \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...  \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...  \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...  \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...  \n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(path+\"/chinese_news.csv\")\n",
    "# Print dataset information\n",
    "print(\"Dataset information:\")\n",
    "print(dataset.info())\n",
    "# Print dataset head\n",
    "print(\"Dataset head:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef64267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after preprocessing:\n",
      "                                             content  \\\n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...   \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...   \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...   \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...   \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会年月日在...  \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。《意见》强调，...  \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...  \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...  \n",
      "4  刚刚过去的年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，集中发力...  \n",
      "0    [中国, 人民, 解放军, 陆军, 领导, 机构, 、, 中国, 人民, 解放军, 火箭军,...\n",
      "1    [经, 中央军委, 主席, 习近平, 批准, ，, 中央军委, 近日, 印发, 了, 《, ...\n",
      "2    [由, 中共中央, 纪律, 检查, 委员会, 、, 中共中央, 文献, 研究室, 编辑, 的...\n",
      "3    [广大, 党员, 干部, 正在, 积极, 学习, 习近平, 总书记, 在, 中央, 政治局,...\n",
      "4    [刚刚, 过去, 的, 年, ，, 是, 全面, 深化, 改革, 的, 关键, 之, 年, ...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Prétraitement de content (suppression des caractères non chinois, normalisation des espaces)\n",
    "def clean_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Garder les caractères chinois et ponctuation chinoise\n",
    "    text = re.sub(r\"[^\\u4e00-\\u9fff\\u3000-\\u303F\\uff00-\\uffef]\", \"\", text)\n",
    "    \n",
    "    # Normaliser les espaces (rare, mais au cas où)\n",
    "    text = text.replace(\" \", \"\").strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remplacer les valeurs manquantes par une chaîne vide\n",
    "dataset[\"content\"] = dataset[\"content\"].fillna(\"\")\n",
    "\n",
    "# Appliquer le prétraitement à la colonne 'content'\n",
    "dataset['cleaned_content'] = dataset['content'].apply(clean_content)\n",
    "\n",
    "# Filtrer les lignes où 'cleaned_content' est vide\n",
    "dataset = dataset[dataset[\"cleaned_content\"].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# Afficher les 5 premières lignes du DataFrame après le prétraitement\n",
    "print(\"Dataset after preprocessing:\")\n",
    "print(dataset[['content', 'cleaned_content']].head())\n",
    "\n",
    "seg = pkuseg.pkuseg()\n",
    "dataset[\"tokens\"] = dataset[\"cleaned_content\"].apply(lambda x: seg.cut(x))\n",
    "\n",
    "# Aperçu\n",
    "print(dataset[\"tokens\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add6bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after generating sequences:\n",
      "                                             content  \\\n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...   \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...   \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...   \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...   \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...   \n",
      "\n",
      "                             char_pinyin_t9_sequence  \n",
      "0  中|zhong|94664 国|guo|486 人|ren|736 民|min|646 解|...  \n",
      "1  经|jing|5464 中|zhong|94664 央|yang|9264 军|jun|58...  \n",
      "2  由|you|968 中|zhong|94664 共|gong|4664 中|zhong|94...  \n",
      "3  广|guang|48264 大|da|32 党|dang|3264 员|yuan|9826 ...  \n",
      "4  刚|gang|4264 刚|gang|4264 过|guo|486 去|qu|78 的|de...  \n"
     ]
    }
   ],
   "source": [
    "# convert the content column to pinyin\n",
    "t9_map = {\n",
    "    \"@\": \"1\", \".\": \"1\", \":\": \"1\",\n",
    "    \"a\": \"2\", \"b\": \"2\", \"c\": \"2\",\n",
    "    \"d\": \"3\", \"e\": \"3\", \"f\": \"3\",\n",
    "    \"g\": \"4\", \"h\": \"4\", \"i\": \"4\",\n",
    "    \"j\": \"5\", \"k\": \"5\", \"l\": \"5\",\n",
    "    \"m\": \"6\", \"n\": \"6\", \"o\": \"6\",\n",
    "    \"p\": \"7\", \"q\": \"7\", \"r\": \"7\", \"s\": \"7\",\n",
    "    \"t\": \"8\", \"u\": \"8\", \"v\": \"8\",\n",
    "    \"w\": \"9\", \"x\": \"9\", \"y\": \"9\", \"z\": \"9\",\n",
    "    \"1\": \"1\", \"2\": \"2\", \"3\": \"3\", \"4\": \"4\",\n",
    "    \"5\": \"5\", \"6\": \"6\", \"7\": \"7\", \"8\": \"8\",\n",
    "    \"9\": \"9\", \"0\": \"0\", \" \": \"0\",\n",
    "    \"。\":\"。\", \"，\":\"，\", \"？\":\"？\", \"！\":\"！\",\n",
    "}\n",
    "\n",
    "# Fonction pour convertir une chaîne de caractères en code T9\n",
    "def pinyin_to_t9(text):\n",
    "    t9_code = \"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    for char in text.lower():\n",
    "        t9_code += t9_map.get(char, char)  # Conserver les caractères non mappés\n",
    "    return t9_code\n",
    "\n",
    "def validate_t9(t9_code):\n",
    "    # Vérifie que le code T9 est numérique (ou vide pour ponctuation)\n",
    "    return bool(re.match(r'^[0-9]+$', t9_code)) or t9_code in {\"。\", \"，\", \"？\", \"！\"}\n",
    "\n",
    "def generer_sequence_contextuelle(row):\n",
    "    tokens = row[\"tokens\"]\n",
    "    sequence = []\n",
    "    for token in tokens:\n",
    "        if not isinstance(token, str) or not re.search(r'[\\u4e00-\\u9fff]', token):\n",
    "            continue\n",
    "        for char, py in zip(token, lazy_pinyin(token)):\n",
    "            t9 = pinyin_to_t9(py)\n",
    "            if validate_t9(t9):  # Vérifier que le T9 est valide\n",
    "                sequence.append(f\"{char}|{py}|{t9}\")\n",
    "    return ' '.join(sequence)\n",
    "\n",
    "dataset[\"char_pinyin_t9_sequence\"] = dataset.apply(generer_sequence_contextuelle, axis=1)\n",
    "\n",
    "# Filtrer les lignes où 'char_pinyin_t9_sequence' est vide\n",
    "dataset = dataset[dataset[\"char_pinyin_t9_sequence\"].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# Sauvegarder le fichier\n",
    "dataset[[\"char_pinyin_t9_sequence\"]].to_csv(\"sequences_char_pinyin_t9.csv\", index=False)\n",
    "\n",
    "# Afficher les 5 premières lignes du DataFrame après le prétraitement\n",
    "print(\"Dataset after generating sequences:\")\n",
    "print(dataset[['content', 'char_pinyin_t9_sequence']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc41c160",
   "metadata": {},
   "source": [
    "# Création du dataset pour le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9336abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer en séquences complètes\n",
    "input_t9_sequences = []\n",
    "target_char_sequences = []\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "for seq in dataset[\"char_pinyin_t9_sequence\"]:\n",
    "    triplets = seq.strip().split(\" \")\n",
    "    t9_seq = []\n",
    "    char_seq = []\n",
    "    \n",
    "    # Extraire les paires char|T9 pour chaque phrase\n",
    "    for triplet in triplets[:MAX_SEQUENCE_LENGTH]:  # Tronquer à MAX_SEQUENCE_LENGTH\n",
    "        parts = triplet.split(\"|\")\n",
    "        if len(parts) == 3:\n",
    "            char, _, t9 = parts\n",
    "            if validate_t9(t9):  # Vérifier que le T9 est valide\n",
    "                char_seq.append(char)\n",
    "                t9_seq.append(t9)\n",
    "    \n",
    "    # Ajouter les séquences T9 et caractères si non vides\n",
    "    if t9_seq and char_seq:\n",
    "        input_t9_sequences.append(\" \".join(t9_seq))\n",
    "        target_char_sequences.append(\"\".join(char_seq)) # A voir si on garde les espaces ou pas\n",
    "\n",
    "# Créer un DataFrame\n",
    "df_sequences = pd.DataFrame({\n",
    "    \"input_t9_sequence\": input_t9_sequences,\n",
    "    \"target_char_sequence\": target_char_sequences\n",
    "})\n",
    "\n",
    "# Filtrer les séquences vides (par précaution)\n",
    "df_sequences = df_sequences[df_sequences[\"input_t9_sequence\"].str.strip() != \"\"]\n",
    "df_sequences = df_sequences[df_sequences[\"target_char_sequence\"].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f096bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame sequences:\n",
      "                                   input_t9_sequence  \\\n",
      "0  94664 486 736 646 543 3264 586 58 586 5464 326...   \n",
      "1  5464 94664 9264 586 934 948 94 94 546 7464 74 ...   \n",
      "2  968 94664 4664 94664 9264 54 58 5426 242 934 9...   \n",
      "3  48264 32 3264 9826 426 28 94364 924 54 54 983 ...   \n",
      "4  4264 4264 486 78 33 6426 744 7826 6426 7436 48...   \n",
      "\n",
      "                                target_char_sequence  \n",
      "0  中国人民解放军陆军领导机构中国人民解放军火箭军中国人民解放军战略支援部队成立大会年月日在八一...  \n",
      "1  经中央军委主席习近平批准中央军委近日印发了关于深化国防和军队改革的意见意见强调党的十八大以来...  \n",
      "2  由中共中央纪律检查委员会中共中央文献研究室编辑的习近平关于严明党的纪律和规矩论述摘编一书近日...  \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话大家纷纷表示要把践...  \n",
      "4  刚刚过去的年是全面深化改革的关键之年改革集中发力在制约经济社会发展的深层次矛盾集中发力在妨碍...  \n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame sequences:\")\n",
    "print(df_sequences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9ffcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b'94664 486 736 646 543 3264 586 58 586 5464 326 54 468 94664 486 736 646 543 3264 586 486 5426 586 94664 486 736 646 543 3264 586 9426 583 944 9826 28 384 24364 54 32 484 6426 983 74 924 22 94 32 568 5664 94664 58 9464 94664 4664 94664 9264 9664 748 54 486 542 948 94 94664 9264 586 934 948 94 94 546 7464 94264 58 586 486 5426 586 9426 583 944 9826 28 384 7468 98 586 74 2464 944 986 24 324 2426 3264 94664 9264 43 94664 9264'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe4\\xb8\\xad\\xe5\\x9b\\xbd\\xe4\\xba\\xba\\xe6\\xb0\\x91\\xe8\\xa7\\xa3\\xe6\\x94\\xbe\\xe5\\x86\\x9b\\xe9\\x99\\x86\\xe5\\x86\\x9b\\xe9\\xa2\\x86\\xe5\\xaf\\xbc\\xe6\\x9c\\xba\\xe6\\x9e\\x84\\xe4\\xb8\\xad\\xe5\\x9b\\xbd\\xe4\\xba\\xba\\xe6\\xb0\\x91\\xe8\\xa7\\xa3\\xe6\\x94\\xbe\\xe5\\x86\\x9b\\xe7\\x81\\xab\\xe7\\xae\\xad\\xe5\\x86\\x9b\\xe4\\xb8\\xad\\xe5\\x9b\\xbd\\xe4\\xba\\xba\\xe6\\xb0\\x91\\xe8\\xa7\\xa3\\xe6\\x94\\xbe\\xe5\\x86\\x9b\\xe6\\x88\\x98\\xe7\\x95\\xa5\\xe6\\x94\\xaf\\xe6\\x8f\\xb4\\xe9\\x83\\xa8\\xe9\\x98\\x9f\\xe6\\x88\\x90\\xe7\\xab\\x8b\\xe5\\xa4\\xa7\\xe4\\xbc\\x9a\\xe5\\xb9\\xb4\\xe6\\x9c\\x88\\xe6\\x97\\xa5\\xe5\\x9c\\xa8\\xe5\\x85\\xab\\xe4\\xb8\\x80\\xe5\\xa4\\xa7\\xe6\\xa5\\xbc\\xe9\\x9a\\x86\\xe9\\x87\\x8d\\xe4\\xb8\\xbe\\xe8\\xa1\\x8c\\xe4\\xb8\\xad\\xe5\\x85\\xb1\\xe4\\xb8\\xad\\xe5\\xa4\\xae\\xe6\\x80\\xbb\\xe4\\xb9\\xa6\\xe8\\xae\\xb0\\xe5\\x9b\\xbd\\xe5\\xae\\xb6\\xe4\\xb8\\xbb\\xe5\\xb8\\xad\\xe4\\xb8\\xad\\xe5\\xa4\\xae\\xe5\\x86\\x9b\\xe5\\xa7\\x94\\xe4\\xb8\\xbb\\xe5\\xb8\\xad\\xe4\\xb9\\xa0\\xe8\\xbf\\x91\\xe5\\xb9\\xb3\\xe5\\x90\\x91\\xe9\\x99\\x86\\xe5\\x86\\x9b\\xe7\\x81\\xab\\xe7\\xae\\xad\\xe5\\x86\\x9b\\xe6\\x88\\x98\\xe7\\x95\\xa5\\xe6\\x94\\xaf\\xe6\\x8f\\xb4\\xe9\\x83\\xa8\\xe9\\x98\\x9f\\xe6\\x8e\\x88\\xe4\\xba\\x88\\xe5\\x86\\x9b\\xe6\\x97\\x97\\xe5\\xb9\\xb6\\xe8\\x87\\xb4\\xe8\\xae\\xad\\xe8\\xaf\\x8d\\xe4\\xbb\\xa3\\xe8\\xa1\\xa8\\xe5\\x85\\x9a\\xe4\\xb8\\xad\\xe5\\xa4\\xae\\xe5\\x92\\x8c\\xe4\\xb8\\xad\\xe5\\xa4\\xae'>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utiliser tf.data.Dataset\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((df_sequences['input_t9_sequence'], df_sequences['target_char_sequence'])).prefetch(tf.data.AUTOTUNE)\n",
    "tf_dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6434642",
   "metadata": {},
   "source": [
    "## Encoder les données pour Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e326d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 15:26:11.665197: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-11 15:26:43.022125: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization\n",
    "input_tv = keras.layers.TextVectorization(output_mode='int',\n",
    "                                          split='character',\n",
    "                                          standardize=None,\n",
    "                                          ragged=True,)\n",
    "\n",
    "target_tv = keras.layers.TextVectorization(output_mode='int',\n",
    "                                           split='character',\n",
    "                                           standardize=None,\n",
    "                                           ragged=True,)\n",
    "tmp_t9_ds = tf_dataset.map(lambda t9, target: tf.strings.reduce_join(tf.strings.split(t9, \" \"), separator=\"\"))\n",
    "t9_ds = tf_dataset.map(lambda t9, target: t9)\n",
    "target_ds = tf_dataset.map(lambda t9, target: target)\n",
    "input_tv.adapt(tmp_t9_ds)\n",
    "target_tv.adapt(target_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042f07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('4'), np.str_('6'), np.str_('2'), np.str_('3'), np.str_('8'), np.str_('9'), np.str_('5'), np.str_('7'), np.str_('。')]\n"
     ]
    }
   ],
   "source": [
    "# Le vocabulaire pour t9 input\n",
    "print(input_tv.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def split_and_vectorize(t9_seq, char_seq, context_size=5):\n",
    "    \n",
    "    # Split chaque séquence T9 (par espace)\n",
    "    t9_parts = tf.strings.split(t9_seq, \" \")  # tf.TensorShape([None])\n",
    "\n",
    "    # Split la séquence de caractères (par caractère UTF-8)\n",
    "    char_parts = tf.strings.unicode_split(char_seq, \"UTF-8\")\n",
    "\n",
    "    # Padder la séquence de caractères pour avoir le contexte pour tous les caractères\n",
    "    char_seq_padded = tf.strings.join([tf.constant(\"     \"), char_seq])\n",
    "    char_parts_padded = tf.strings.unicode_split(char_seq_padded, \"UTF-8\")\n",
    "\n",
    "    # Vérification de la même longueur entre la séquence de t9 et celle de caractères\n",
    "    assert_op = tf.debugging.assert_equal(tf.shape(t9_parts)[0], tf.shape(char_parts)[0])\n",
    "\n",
    "    with tf.control_dependencies([assert_op]):\n",
    "        \n",
    "        # Vectorisation : chaque chiffre dans chaque t9_part est un caractère\n",
    "        vectorized_t9 = input_tv(t9_parts)       # RaggedTensor: [nb_sous_seq, longueur_t9]\n",
    "        vectorized_target = target_tv(char_parts)  # RaggedTensor: [nb_sous_seq, 1]\n",
    "\n",
    "        # Convertit RaggedTensor en Tensor avec padding (0 par défaut, qui sera ignoré lors du Embedding Layer, mask_zero=True)\n",
    "        t9_tensor = vectorized_t9.to_tensor(default_value=0)\n",
    "        target_tensor = vectorized_target.to_tensor(default_value=0)\n",
    "        target_tensor = tf.squeeze(target_tensor, axis=-1)\n",
    "        \n",
    "        # Contexte\n",
    "        # Vectorisation du target padded\n",
    "        vectorized_target_padded = target_tv(char_parts_padded).to_tensor(default_value=0)\n",
    "        # Avoir la longueur de caractères après le padding\n",
    "        padded_seq_len = tf.shape(vectorized_target_padded)[0]\n",
    "        # Initialiser un tenseur pour le contexte\n",
    "        contexts = tf.TensorArray(dtype=tf.int64, size=padded_seq_len - context_size)\n",
    "        # Parcourir chaque caractère sauf le dernier\n",
    "        for i in tf.range(padded_seq_len - context_size):\n",
    "            # Récupérer dynamiquement un contexte de taille fixe\n",
    "            context = vectorized_target_padded[i:i+context_size]\n",
    "            # Écrire dans la position correspondante du tenseur array\n",
    "            contexts = contexts.write(i, context)\n",
    "        # Piler le tenseur\n",
    "        contexts_tensor = contexts.stack()\n",
    "        contexts_tensor = tf.squeeze(contexts_tensor, axis=-1)\n",
    "\n",
    "        return {\"t9_input\": t9_tensor, \"context_input\": contexts_tensor}, target_tensor\n",
    "\n",
    "transformed_dataset = tf_dataset.map(\n",
    "    lambda t9, target: split_and_vectorize(t9, target),\n",
    "    num_parallel_calls=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e36310b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'t9_input': <tf.Tensor: shape=(256, 100, 6), dtype=int64, numpy=\n",
       "  array([[[7, 2, 3, 3, 2, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [9, 5, 3, 0, 0, 0],\n",
       "          ...,\n",
       "          [2, 5, 0, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [7, 4, 3, 2, 0, 0]],\n",
       "  \n",
       "         [[8, 2, 3, 2, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [7, 4, 3, 2, 0, 0],\n",
       "          ...,\n",
       "          [2, 5, 0, 0, 0, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [5, 4, 3, 2, 0, 0]],\n",
       "  \n",
       "         [[7, 3, 6, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [2, 3, 3, 2, 0, 0],\n",
       "          ...,\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [9, 2, 2, 0, 0, 0],\n",
       "          [9, 6, 4, 3, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[7, 2, 5, 3, 2, 0],\n",
       "          [7, 4, 2, 0, 0, 0],\n",
       "          [7, 2, 2, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[7, 5, 2, 0, 0, 0],\n",
       "          [9, 2, 0, 0, 0, 0],\n",
       "          [6, 2, 4, 3, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[9, 2, 3, 6, 0, 0],\n",
       "          [8, 2, 3, 0, 0, 0],\n",
       "          [9, 2, 0, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]]])>,\n",
       "  'context_input': <tf.Tensor: shape=(256, 100, 5), dtype=int64, numpy=\n",
       "  array([[[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,    4],\n",
       "          [   1,    1,    1,    4,    3],\n",
       "          ...,\n",
       "          [  96,   36,   69,    4,   43],\n",
       "          [  36,   69,    4,   43,   13],\n",
       "          [  69,    4,   43,   13,    4]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   71],\n",
       "          [   1,    1,    1,   71,    4],\n",
       "          ...,\n",
       "          [1116,   71,  196,   81,  167],\n",
       "          [  71,  196,   81,  167,   13],\n",
       "          [ 196,   81,  167,   13,    3]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  268],\n",
       "          [   1,    1,    1,  268,    4],\n",
       "          ...,\n",
       "          [  41,   25,   31,   70,  154],\n",
       "          [  25,   31,   70,  154,   65],\n",
       "          [  31,   70,  154,   65,  248]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  134],\n",
       "          [   1,    1,    1,  134,    5],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   30],\n",
       "          [   1,    1,    1,   30,  166],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  217],\n",
       "          [   1,    1,    1,  217,   25],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]]])>},\n",
       " <tf.Tensor: shape=(256, 100), dtype=int64, numpy=\n",
       " array([[  4,   3,   7, ...,  13,   4,  43],\n",
       "        [ 71,   4,  43, ...,  13,   3, 315],\n",
       "        [268,   4,  42, ...,  65, 248,  14],\n",
       "        ...,\n",
       "        [134,   5, 541, ...,   0,   0,   0],\n",
       "        [ 30, 166,  15, ...,   0,   0,   0],\n",
       "        [217,  25, 166, ...,   0,   0,   0]])>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataset.padded_batch(256).take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf779c5f",
   "metadata": {},
   "source": [
    "## Split train-valid-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec477403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du train : 16503\n",
      "Taille du validation : 2064\n",
      "Taille du test : 2062\n"
     ]
    }
   ],
   "source": [
    "c = transformed_dataset.reduce(0, lambda x,_:x+1).numpy()\n",
    "\n",
    "shuffled_ds = transformed_dataset.shuffle(buffer_size=c, seed=42)\n",
    "\n",
    "train_size = c * 80 // 100\n",
    "test_size = c * 10 // 100\n",
    "val_size = c - train_size - test_size\n",
    "\n",
    "ds_train = shuffled_ds.take(train_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = shuffled_ds.skip(train_size).take(val_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = shuffled_ds.skip(train_size+val_size).take(test_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Taille du train :\", ds_train.cardinality().numpy())\n",
    "print(\"Taille du validation :\", ds_val.cardinality().numpy())\n",
    "print(\"Taille du test :\", ds_test.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870dabbe",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f6c70",
   "metadata": {},
   "source": [
    "Sogou T9 est une méthode d’entrée intelligente qui :\n",
    "\n",
    "- Prend des séquences numériques (ex. : \"94664 486\" pour \"zhong guo\").\n",
    "- Génère des séquences de caractères chinois (ex. : \"中国\").\n",
    "- Utilise le contexte (mots précédents) pour désambiguïser les prédictions.\n",
    "- Est optimisé pour la vitesse et la précision, souvent avec des modèles entraînés sur de vastes corpus.\n",
    "\n",
    "Pour reproduire cela, il faut utiliser un modèle seq2seq avec un encodeur-décodeur (2 entrées) :\n",
    "\n",
    "- Encodeur : Lit la séquence T9 et la compresse en une représentation contextuelle.\n",
    "- Décodeur : Génère la séquence de caractères chinois à partir de cette représentation.\n",
    "\n",
    "[Functional API](https://keras.io/guides/functional_api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 5\n",
    "\n",
    "ds_train_padded = ds_train.padded_batch(\n",
    "    128,\n",
    "    padded_shapes=(\n",
    "        {\n",
    "            \"t9_input\": [MAX_SEQUENCE_LENGTH, 6],\n",
    "            \"context_input\": [MAX_SEQUENCE_LENGTH, context_size]\n",
    "        },\n",
    "        [MAX_SEQUENCE_LENGTH]  # Pour les étiquettes\n",
    "    ),\n",
    "    padding_values=(\n",
    "        {\n",
    "            \"t9_input\": tf.constant(0, dtype=tf.int64),\n",
    "            \"context_input\": tf.constant(0, dtype=tf.int64)\n",
    "        },\n",
    "        tf.constant(0, dtype=tf.int64)  # Pour les étiquettes\n",
    "    ),\n",
    "    drop_remainder=True,\n",
    ").map(lambda x, y: (x, y, y>0)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_val_padded = ds_val.padded_batch(\n",
    "    128,\n",
    "    padded_shapes=(\n",
    "        {\n",
    "            \"t9_input\": [MAX_SEQUENCE_LENGTH, 6],\n",
    "            \"context_input\": [MAX_SEQUENCE_LENGTH, context_size]\n",
    "        },\n",
    "        [MAX_SEQUENCE_LENGTH]  # Pour les étiquettes\n",
    "    ),\n",
    "    padding_values=(\n",
    "        {\n",
    "            \"t9_input\": tf.constant(0, dtype=tf.int64),\n",
    "            \"context_input\": tf.constant(0, dtype=tf.int64)\n",
    "        },\n",
    "        tf.constant(0, dtype=tf.int64)  # Pour les étiquettes\n",
    "    ),\n",
    "    drop_remainder=True,\n",
    ").map(lambda x, y: (x, y, y>0)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_test_padded = ds_test.padded_batch(\n",
    "    128,\n",
    "    padded_shapes=(\n",
    "        {\n",
    "            \"t9_input\": [MAX_SEQUENCE_LENGTH, 6],\n",
    "            \"context_input\": [MAX_SEQUENCE_LENGTH, context_size]\n",
    "        },\n",
    "        [MAX_SEQUENCE_LENGTH]  # Pour les étiquettes\n",
    "    ),\n",
    "    padding_values=(\n",
    "        {\n",
    "            \"t9_input\": tf.constant(0, dtype=tf.int64),\n",
    "            \"context_input\": tf.constant(0, dtype=tf.int64)\n",
    "        },\n",
    "        tf.constant(0, dtype=tf.int64)  # Pour les étiquettes\n",
    "    ),\n",
    "    drop_remainder=True,\n",
    ").map(lambda x, y: (x, y, y>0)).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bd73ac4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'t9_input': <tf.Tensor: shape=(128, 100, 6), dtype=int64, numpy=\n",
       "  array([[[9, 6, 4, 3, 0, 0],\n",
       "          [9, 2, 6, 0, 0, 0],\n",
       "          [7, 6, 2, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[5, 2, 0, 0, 0, 0],\n",
       "          [5, 9, 0, 0, 0, 0],\n",
       "          [9, 2, 2, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[5, 4, 0, 0, 0, 0],\n",
       "          [7, 2, 4, 3, 0, 0],\n",
       "          [4, 2, 4, 3, 0, 0],\n",
       "          ...,\n",
       "          [8, 4, 3, 0, 0, 0],\n",
       "          [7, 2, 4, 3, 0, 0],\n",
       "          [5, 5, 0, 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[9, 2, 0, 0, 0, 0],\n",
       "          [7, 2, 0, 0, 0, 0],\n",
       "          [8, 2, 3, 0, 0, 0],\n",
       "          ...,\n",
       "          [7, 2, 2, 0, 0, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [8, 2, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[5, 2, 0, 0, 0, 0],\n",
       "          [9, 2, 4, 3, 0, 0],\n",
       "          [4, 6, 0, 0, 0, 0],\n",
       "          ...,\n",
       "          [7, 6, 0, 0, 0, 0],\n",
       "          [7, 2, 6, 0, 0, 0],\n",
       "          [9, 6, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[9, 2, 0, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [2, 3, 3, 2, 0, 0],\n",
       "          ...,\n",
       "          [8, 2, 3, 0, 0, 0],\n",
       "          [9, 2, 3, 2, 0, 0],\n",
       "          [6, 3, 3, 2, 0, 0]]])>,\n",
       "  'context_input': <tf.Tensor: shape=(128, 100, 5), dtype=int64, numpy=\n",
       "  array([[[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   14],\n",
       "          [   1,    1,    1,   14,  326],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   77],\n",
       "          [   1,    1,    1,   77,  232],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   11],\n",
       "          [   1,    1,    1,   11,   37],\n",
       "          ...,\n",
       "          [  24,  470,   87,  230,  738],\n",
       "          [ 470,   87,  230,  738,  401],\n",
       "          [  87,  230,  738,  401,  403]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   10],\n",
       "          [   1,    1,    1,   10,   41],\n",
       "          ...,\n",
       "          [  89,   41,   25,   31,  312],\n",
       "          [  41,   25,   31,  312,  125],\n",
       "          [  25,   31,  312,  125,    3]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   19],\n",
       "          [   1,    1,    1,   19,  413],\n",
       "          ...,\n",
       "          [   2,  128,   14,  313,  719],\n",
       "          [ 128,   14,  313,  719, 1232],\n",
       "          [  14,  313,  719, 1232,  715]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   10],\n",
       "          [   1,    1,    1,   10,    4],\n",
       "          ...,\n",
       "          [  19,   63,    5,   49,   41],\n",
       "          [  63,    5,   49,   41,   25],\n",
       "          [   5,   49,   41,   25,   31]]])>},\n",
       " <tf.Tensor: shape=(128, 100), dtype=int64, numpy=\n",
       " array([[  14,  326,  137, ...,    0,    0,    0],\n",
       "        [  77,  232,   85, ...,    0,    0,    0],\n",
       "        [  11,   37,  138, ...,  401,  403,    2],\n",
       "        ...,\n",
       "        [  10,   41,   25, ...,  125,    3,   75],\n",
       "        [  19,  413,  106, ..., 1232,  715, 1945],\n",
       "        [  10,    4,   42, ...,   25,   31,   64]])>,\n",
       " <tf.Tensor: shape=(128, 100), dtype=bool, numpy=\n",
       " array([[ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]])>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_padded.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_tv, output_tv, emb_size, gru_size, context_size=5):\n",
    "\n",
    "    # Inputs\n",
    "    t9_input = keras.layers.Input(shape=(None, 6), name=\"t9_input\")  # (batch, seq_len, t9_len)\n",
    "    context_input = keras.layers.Input(shape=(None, context_size), name=\"context_input\")  # (batch, seq_len, context_len)\n",
    "\n",
    "    # ===== T9 processing =====\n",
    "    t9_emb = keras.layers.TimeDistributed(\n",
    "        keras.layers.Embedding(input_dim=input_tv.vocabulary_size(),\n",
    "                         output_dim=emb_size,\n",
    "                         mask_zero=True),\n",
    "        name=\"t9_embedding\")(t9_input)\n",
    "\n",
    "    t9_emb = keras.layers.Dropout(0.3, name=\"t9_embedding_dropout\")(t9_emb)\n",
    "\n",
    "    t9_encoded = keras.layers.TimeDistributed(\n",
    "        keras.layers.GRU(gru_size,\n",
    "                   activation=\"tanh\",\n",
    "                   recurrent_activation=\"sigmoid\",\n",
    "                   dropout=0.3,\n",
    "                   return_sequences=False),\n",
    "        name=\"t9_gru\")(t9_emb)\n",
    "\n",
    "    t9_encoded = keras.layers.LayerNormalization(name=\"t9_encoding_norm\")(t9_encoded)\n",
    "\n",
    "    # ===== Context processing =====\n",
    "    context_emb = keras.layers.TimeDistributed(\n",
    "        keras.layers.Embedding(input_dim=output_tv.vocabulary_size(),\n",
    "                         output_dim=emb_size,\n",
    "                         mask_zero=True),\n",
    "        name=\"context_embedding\")(context_input)\n",
    "\n",
    "    context_emb = keras.layers.Dropout(0.3, name=\"context_embedding_dropout\")(context_emb)\n",
    "\n",
    "    context_encoded = keras.layers.TimeDistributed(\n",
    "        keras.layers.GRU(gru_size,\n",
    "                   activation=\"tanh\",\n",
    "                   recurrent_activation=\"sigmoid\",\n",
    "                   dropout=0.3,\n",
    "                   return_sequences=False),\n",
    "        name=\"context_gru\")(context_emb)\n",
    "\n",
    "    context_encoded = keras.layers.LayerNormalization(name=\"context_encoding_norm\")(context_encoded)\n",
    "\n",
    "    # ===== Merge & Output =====\n",
    "    merged = keras.layers.Concatenate(axis=-1, name=\"merged_features\")([t9_encoded, context_encoded])\n",
    "\n",
    "    output = keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(output_tv.vocabulary_size(), activation=\"softmax\"),\n",
    "        name=\"softmax\")(merged)\n",
    "\n",
    "    model = keras.Model(inputs={\"t9_input\": t9_input, \"context_input\": context_input}, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[keras_hub.metrics.Perplexity(from_logits=False, mask_token_id=0)],\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c6d7e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(input_tv, target_tv, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1765b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 2s/step - loss: 7.3889 - perplexity: 3365.1919 - sparse_categorical_accuracy: 0.0099\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - loss: 6.5704 - perplexity: 1236.8452 - sparse_categorical_accuracy: 0.0228\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - loss: 6.0239 - perplexity: 728.5123 - sparse_categorical_accuracy: 0.0366\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - loss: 5.9294 - perplexity: 631.7459 - sparse_categorical_accuracy: 0.0508\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 5.6945 - perplexity: 490.9893 - sparse_categorical_accuracy: 0.0573\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 5.4202 - perplexity: 360.5275 - sparse_categorical_accuracy: 0.0722\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - loss: 5.1394 - perplexity: 259.8983 - sparse_categorical_accuracy: 0.1062\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 4.8253 - perplexity: 184.4632 - sparse_categorical_accuracy: 0.1459\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 4.4985 - perplexity: 132.2854 - sparse_categorical_accuracy: 0.1948\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 4.0978 - perplexity: 86.9982 - sparse_categorical_accuracy: 0.2570\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - loss: 3.7546 - perplexity: 59.8586 - sparse_categorical_accuracy: 0.3128\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - loss: 3.4154 - perplexity: 41.0794 - sparse_categorical_accuracy: 0.3662\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - loss: 3.1141 - perplexity: 30.1046 - sparse_categorical_accuracy: 0.4078\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - loss: 2.8852 - perplexity: 22.8277 - sparse_categorical_accuracy: 0.4400\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - loss: 2.6758 - perplexity: 17.9226 - sparse_categorical_accuracy: 0.4793\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3s/step - loss: 2.4169 - perplexity: 14.0620 - sparse_categorical_accuracy: 0.5070\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3s/step - loss: 2.2384 - perplexity: 11.4588 - sparse_categorical_accuracy: 0.5339\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 2.0989 - perplexity: 9.8054 - sparse_categorical_accuracy: 0.5556\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 1.8926 - perplexity: 7.9524 - sparse_categorical_accuracy: 0.5876\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - loss: 1.8312 - perplexity: 7.2251 - sparse_categorical_accuracy: 0.5960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x70a1ec690>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train_padded.take(10), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78a4e4",
   "metadata": {},
   "source": [
    "# Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007970b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
