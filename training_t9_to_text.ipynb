{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perrineqhn/Desktop/M2/S2/Méthode en Apprentissage Automatique/tf_metal_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/perrineqhn/.cache/kagglehub/datasets/noxmoon/chinese-official-daily-news-since-2016/versions/1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import kagglehub\n",
    "import keras\n",
    "import keras_hub\n",
    "import keras_tuner\n",
    "import pandas as pd\n",
    "import pkuseg\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from pypinyin import lazy_pinyin\n",
    "from keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    Concatenate,\n",
    "    Attention,\n",
    "    StringLookup,\n",
    "    TextVectorization,\n",
    ")\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"noxmoon/chinese-official-daily-news-since-2016\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3e85f",
   "metadata": {},
   "source": [
    "# Création du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1de02ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20738 entries, 0 to 20737\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   date      20738 non-null  object\n",
      " 1   tag       20738 non-null  object\n",
      " 2   headline  20738 non-null  object\n",
      " 3   content   20631 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 648.2+ KB\n",
      "None\n",
      "Dataset head:\n",
      "         date   tag                                           headline  \\\n",
      "0  2016-01-01  详细全文  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...   \n",
      "1  2016-01-01  详细全文                             中央军委印发《关于深化国防和军队改革的意见》   \n",
      "2  2016-01-01  详细全文                           《习近平关于严明党的纪律和规矩论述摘编》出版发行   \n",
      "3  2016-01-01  详细全文                                 以实际行动向党中央看齐 向高标准努力   \n",
      "4  2016-01-01  详细全文                                 【年终特稿】关键之年 改革挺进深水区   \n",
      "\n",
      "                                             content  \n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...  \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...  \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...  \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...  \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...  \n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(path+\"/chinese_news.csv\")\n",
    "# Print dataset information\n",
    "print(\"Dataset information:\")\n",
    "print(dataset.info())\n",
    "# Print dataset head\n",
    "print(\"Dataset head:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef64267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after preprocessing:\n",
      "                                             content  \\\n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...   \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...   \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...   \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...   \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会年月日在...  \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。《意见》强调，...  \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...  \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...  \n",
      "4  刚刚过去的年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，集中发力...  \n",
      "0    [中国, 人民, 解放军, 陆军, 领导, 机构, 、, 中国, 人民, 解放军, 火箭军,...\n",
      "1    [经, 中央军委, 主席, 习近平, 批准, ，, 中央军委, 近日, 印发, 了, 《, ...\n",
      "2    [由, 中共中央, 纪律, 检查, 委员会, 、, 中共中央, 文献, 研究室, 编辑, 的...\n",
      "3    [广大, 党员, 干部, 正在, 积极, 学习, 习近平, 总书记, 在, 中央, 政治局,...\n",
      "4    [刚刚, 过去, 的, 年, ，, 是, 全面, 深化, 改革, 的, 关键, 之, 年, ...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Prétraitement de content (suppression des caractères non chinois, normalisation des espaces)\n",
    "def clean_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Garder les caractères chinois et ponctuation chinoise\n",
    "    text = re.sub(r\"[^\\u4e00-\\u9fff\\u3000-\\u303F\\uff00-\\uffef]\", \"\", text)\n",
    "    \n",
    "    # Normaliser les espaces (rare, mais au cas où)\n",
    "    text = text.replace(\" \", \"\").strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remplacer les valeurs manquantes par une chaîne vide\n",
    "dataset[\"content\"] = dataset[\"content\"].fillna(\"\")\n",
    "\n",
    "# Appliquer le prétraitement à la colonne 'content'\n",
    "dataset['cleaned_content'] = dataset['content'].apply(clean_content)\n",
    "\n",
    "# Filtrer les lignes où 'cleaned_content' est vide\n",
    "dataset = dataset[dataset[\"cleaned_content\"].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# Afficher les 5 premières lignes du DataFrame après le prétraitement\n",
    "print(\"Dataset after preprocessing:\")\n",
    "print(dataset[['content', 'cleaned_content']].head())\n",
    "\n",
    "seg = pkuseg.pkuseg()\n",
    "dataset[\"tokens\"] = dataset[\"cleaned_content\"].apply(lambda x: seg.cut(x))\n",
    "\n",
    "# Aperçu\n",
    "print(dataset[\"tokens\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add6bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after generating sequences:\n",
      "                                             content  \\\n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...   \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...   \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...   \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...   \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...   \n",
      "\n",
      "                             char_pinyin_t9_sequence  \n",
      "0  中|zhong|94664 国|guo|486 人|ren|736 民|min|646 解|...  \n",
      "1  经|jing|5464 中|zhong|94664 央|yang|9264 军|jun|58...  \n",
      "2  由|you|968 中|zhong|94664 共|gong|4664 中|zhong|94...  \n",
      "3  广|guang|48264 大|da|32 党|dang|3264 员|yuan|9826 ...  \n",
      "4  刚|gang|4264 刚|gang|4264 过|guo|486 去|qu|78 的|de...  \n"
     ]
    }
   ],
   "source": [
    "# convert the content column to pinyin\n",
    "t9_map = {\n",
    "    \"@\": \"1\", \".\": \"1\", \":\": \"1\",\n",
    "    \"a\": \"2\", \"b\": \"2\", \"c\": \"2\",\n",
    "    \"d\": \"3\", \"e\": \"3\", \"f\": \"3\",\n",
    "    \"g\": \"4\", \"h\": \"4\", \"i\": \"4\",\n",
    "    \"j\": \"5\", \"k\": \"5\", \"l\": \"5\",\n",
    "    \"m\": \"6\", \"n\": \"6\", \"o\": \"6\",\n",
    "    \"p\": \"7\", \"q\": \"7\", \"r\": \"7\", \"s\": \"7\",\n",
    "    \"t\": \"8\", \"u\": \"8\", \"v\": \"8\",\n",
    "    \"w\": \"9\", \"x\": \"9\", \"y\": \"9\", \"z\": \"9\",\n",
    "    \"1\": \"1\", \"2\": \"2\", \"3\": \"3\", \"4\": \"4\",\n",
    "    \"5\": \"5\", \"6\": \"6\", \"7\": \"7\", \"8\": \"8\",\n",
    "    \"9\": \"9\", \"0\": \"0\", \" \": \"0\",\n",
    "    \"。\":\"。\", \"，\":\"，\", \"？\":\"？\", \"！\":\"！\",\n",
    "}\n",
    "\n",
    "# Fonction pour convertir une chaîne de caractères en code T9\n",
    "def pinyin_to_t9(text):\n",
    "    t9_code = \"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    for char in text.lower():\n",
    "        t9_code += t9_map.get(char, char)  # Conserver les caractères non mappés\n",
    "    return t9_code\n",
    "\n",
    "def validate_t9(t9_code):\n",
    "    # Vérifie que le code T9 est numérique (ou vide pour ponctuation)\n",
    "    return bool(re.match(r'^[0-9]+$', t9_code)) or t9_code in {\"。\", \"，\", \"？\", \"！\"}\n",
    "\n",
    "def generer_sequence_contextuelle(row):\n",
    "    tokens = row[\"tokens\"]\n",
    "    sequence = []\n",
    "    for token in tokens:\n",
    "        if not isinstance(token, str) or not re.search(r'[\\u4e00-\\u9fff]', token):\n",
    "            continue\n",
    "        for char, py in zip(token, lazy_pinyin(token)):\n",
    "            t9 = pinyin_to_t9(py)\n",
    "            if validate_t9(t9):  # Vérifier que le T9 est valide\n",
    "                sequence.append(f\"{char}|{py}|{t9}\")\n",
    "    return ' '.join(sequence)\n",
    "\n",
    "dataset[\"char_pinyin_t9_sequence\"] = dataset.apply(generer_sequence_contextuelle, axis=1)\n",
    "\n",
    "# Filtrer les lignes où 'char_pinyin_t9_sequence' est vide\n",
    "dataset = dataset[dataset[\"char_pinyin_t9_sequence\"].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# Sauvegarder le fichier\n",
    "dataset[[\"char_pinyin_t9_sequence\"]].to_csv(\"sequences_char_pinyin_t9.csv\", index=False)\n",
    "\n",
    "# Afficher les 5 premières lignes du DataFrame après le prétraitement\n",
    "print(\"Dataset after generating sequences:\")\n",
    "print(dataset[['content', 'char_pinyin_t9_sequence']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc41c160",
   "metadata": {},
   "source": [
    "# Création du dataset pour le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9336abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifier la génération des séquences pour inclure le contexte\n",
    "input_t9_sequences = []\n",
    "context_char_sequences = []\n",
    "target_char_sequences = []\n",
    "MAX_CONTEXT_LENGTH = 50  # Longueur max du contexte\n",
    "MAX_T9_LENGTH = 50      # Longueur max de la séquence T9\n",
    "MAX_TARGET_LENGTH = 50  # Longueur max de la séquence cible\n",
    "\n",
    "for seq in dataset[\"char_pinyin_t9_sequence\"]:\n",
    "    triplets = seq.strip().split(\" \")\n",
    "    t9_seq = []\n",
    "    char_seq = []\n",
    "    \n",
    "    for triplet in triplets:\n",
    "        parts = triplet.split(\"|\")\n",
    "        if len(parts) == 3:\n",
    "            char, _, t9 = parts\n",
    "            if validate_t9(t9):\n",
    "                char_seq.append(char)\n",
    "                t9_seq.append(t9)\n",
    "    \n",
    "    # Créer des paires contexte-T9-cible\n",
    "    for i in range(1, len(t9_seq)):\n",
    "        # Contexte : caractères avant la position i\n",
    "        context = char_seq[:i][-MAX_CONTEXT_LENGTH:]\n",
    "        # Entrée T9 : codes T9 à partir de i\n",
    "        t9_input = t9_seq[i:i+MAX_T9_LENGTH]\n",
    "        # Cible : caractères à partir de i\n",
    "        target = char_seq[i:i+MAX_TARGET_LENGTH]\n",
    "        \n",
    "        if context and t9_input and target:\n",
    "            context_char_sequences.append(\"\".join(context))\n",
    "            input_t9_sequences.append(\" \".join(t9_input))\n",
    "            target_char_sequences.append(\"\".join(target))\n",
    "\n",
    "# Créer un DataFrame\n",
    "df_sequences = pd.DataFrame({\n",
    "    \"context_char_sequence\": context_char_sequences,\n",
    "    \"input_t9_sequence\": input_t9_sequences,\n",
    "    \"target_char_sequence\": target_char_sequences\n",
    "})\n",
    "\n",
    "# Filtrer les séquences vides\n",
    "df_sequences = df_sequences[\n",
    "    (df_sequences[\"context_char_sequence\"].str.strip() != \"\") &\n",
    "    (df_sequences[\"input_t9_sequence\"].str.strip() != \"\") &\n",
    "    (df_sequences[\"target_char_sequence\"].str.strip() != \"\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f096bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame sequences:\n",
      "  context_char_sequence                                  input_t9_sequence  \\\n",
      "0                     中  486 736 646 543 3264 586 58 586 5464 326 54 46...   \n",
      "1                    中国  736 646 543 3264 586 58 586 5464 326 54 468 94...   \n",
      "2                   中国人  646 543 3264 586 58 586 5464 326 54 468 94664 ...   \n",
      "3                  中国人民  543 3264 586 58 586 5464 326 54 468 94664 486 ...   \n",
      "4                 中国人民解  3264 586 58 586 5464 326 54 468 94664 486 736 ...   \n",
      "\n",
      "                                target_char_sequence  \n",
      "0  国人民解放军陆军领导机构中国人民解放军火箭军中国人民解放军战略支援部队成立大会年月日在八一大...  \n",
      "1  人民解放军陆军领导机构中国人民解放军火箭军中国人民解放军战略支援部队成立大会年月日在八一大楼...  \n",
      "2  民解放军陆军领导机构中国人民解放军火箭军中国人民解放军战略支援部队成立大会年月日在八一大楼隆...  \n",
      "3  解放军陆军领导机构中国人民解放军火箭军中国人民解放军战略支援部队成立大会年月日在八一大楼隆重...  \n",
      "4  放军陆军领导机构中国人民解放军火箭军中国人民解放军战略支援部队成立大会年月日在八一大楼隆重举...  \n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame sequences:\")\n",
    "print(df_sequences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca9ffcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utiliser tf.data.Dataset\n",
    "# tf_dataset = tf.data.Dataset.from_tensor_slices((input_t9_sequences, target_char_sequences)).prefetch(tf.data.AUTOTUNE)\n",
    "# tf_dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6434642",
   "metadata": {},
   "source": [
    "## Encoder les données pour Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e326d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization pour le contexte, T9 et cible\n",
    "context_tv = keras.layers.TextVectorization(\n",
    "    output_mode='int',\n",
    "    split='character',\n",
    "    standardize=None,\n",
    "    ragged=True\n",
    ")\n",
    "\n",
    "input_t9_tv = keras.layers.TextVectorization(\n",
    "    output_mode='int',\n",
    "    split='whitespace',\n",
    "    standardize=None,\n",
    "    ragged=True\n",
    ")\n",
    "\n",
    "target_tv = keras.layers.TextVectorization(\n",
    "    output_mode='int',\n",
    "    split='character',\n",
    "    standardize=None,\n",
    "    ragged=True\n",
    ")\n",
    "\n",
    "# Créer tf.data.Dataset\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        df_sequences[\"context_char_sequence\"].values,\n",
    "        df_sequences[\"input_t9_sequence\"].values,\n",
    "        df_sequences[\"target_char_sequence\"].values\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Adapter les vectoriseurs\n",
    "context_ds = tf_dataset.map(lambda ctx, t9, tgt: ctx)\n",
    "t9_ds = tf_dataset.map(lambda ctx, t9, tgt: t9)\n",
    "target_ds = tf_dataset.map(lambda ctx, t9, tgt: tgt)\n",
    "context_tv.adapt(context_ds)\n",
    "input_t9_tv.adapt(t9_ds)\n",
    "target_tv.adapt(target_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d856e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer le dataset\n",
    "@tf.function\n",
    "def transform_ds(ctx, t9, tgt):\n",
    "    vectorized_ctx = context_tv(ctx)\n",
    "    vectorized_t9 = input_t9_tv(t9)\n",
    "    vectorized_tgt = target_tv(tgt)\n",
    "    return (vectorized_ctx, vectorized_t9), vectorized_tgt\n",
    "\n",
    "transformed_tf_dataset = tf_dataset.map(transform_ds, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91e0aee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 100), dtype=int64, numpy=\n",
       " array([[ 10,   5,  27,  78,  38,  23,  43,  21,  43,  28,  31,   3,  90,\n",
       "          10,   5,  27,  78,  38,  23,  43,   5,  13,  43,  10,   5,  27,\n",
       "          78,  38,  23,  43,  14, 116,  15,  24,  26,  76,  59,   3,   9,\n",
       "          17,  36,  54,   8,  16,  97,   2,   9, 149, 108,  10,  21,  20,\n",
       "          10,  22,  10,  47,  56,  52,   3,   5,  44,  46,   2,  10,  47,\n",
       "          43,  18,  46,   2,   2,  12,  51,  30,  21,  43,   5,  13,  43,\n",
       "          14, 116,  15,  24,  26,  76,  74,   7,  43,   8, 103,  15,  41,\n",
       "          60,  86,  39,  23,  10,  47,  11,  10,  47],\n",
       "        [ 28,  10,  47,  43,  18,  46,   2,   2,  12,  51,   8, 157,  10,\n",
       "          47,  43,  18,  12,   8,  34,   9,  25,  58,   7,  89,  68,   5,\n",
       "          23,  11,  43,  76,  66,  11,   6,   2,  13,   2,  13,  42,  77,\n",
       "          23,   6,   4,  97,   9,   2,  50,  23,  10,  47,  10,  47,  43,\n",
       "          18,  11,   2,  46,   2,  18,  88,   4,  14,  42,  43,  72,  39,\n",
       "          48, 179,  43,  76,  11,  79,  68,  14,  86,  68,  49,  17,  68,\n",
       "          13,  80,  48, 179,  43,   4,   3,  29,  13,  80,  11,  41,  56,\n",
       "          48, 179,  28,   3,  13,  80,  11,   5,  23],\n",
       "        [ 53,  10,  22,  10,  47,   3,  21,  13, 131,  18,  24,  17,  10,\n",
       "          22,  10,  47,  73,  14,  19,  70,   4,  39,   3,   6,   2,  12,\n",
       "          51,  58,   7,  19,  79,  23,   6,   3,  21,  11,  17,  21,  43,\n",
       "          52, 177,  39,   2,  52,  12,   8,  53,  10,  47,  73,  14,  62,\n",
       "          37,  80,  10,   5,  23,  49,  62,  37,  80,  62,  37,  16,  55,\n",
       "           5,   9,  20,  23,   6,   4,  97,   9,   2,  50,  10,  22,  10,\n",
       "          47,  56,  52,   3,   5,  44,  46,   2,  10,  47,  43,  18,  46,\n",
       "           2,   2,  12,  51,  33,  32,  10,   4,  55]])>,\n",
       " <tf.Tensor: shape=(3, 100), dtype=int64, numpy=\n",
       " array([[   4,    3,    7,   26,  341,  309,   74,  542,   74,  145,  163,\n",
       "           99,  346,    4,    3,    7,   26,  341,  309,   74,  307,  990,\n",
       "           74,    4,    3,    7,   26,  341,  309,   74,  150,  432,  336,\n",
       "          737,   21,  218,   38,  255,    8,    6,   12,   53,   10,    5,\n",
       "          374,    9,    8, 1053, 1105,   65,   92,   16,    4,   42,    4,\n",
       "           43,   23,  112,  111,    3,   24,   18,   60,    4,   43,   74,\n",
       "           47,   18,   60,   41,   25,   31,  178,  542,   74,  307,  990,\n",
       "           74,  150,  432,  336,  737,   21,  218, 1004,  883,   74,  991,\n",
       "          182,  344,  827, 1222,   96,   36,   69,    4,   43,   13,    4,\n",
       "           43],\n",
       "        [  71,    4,   43,   74,   47,   18,   60,   41,   25,   31,  361,\n",
       "          436,    4,   43,   74,   47,   25,   10,  467,   11,   17,   62,\n",
       "          101,  202,  120,    3,  315,   13,   74,  218,  160,  211,    2,\n",
       "          264,  212,  264,  212,   72,  210,   69,    2,   85,  374,    8,\n",
       "           49,   39,   69,    4,   43,    4,   43,   74,   47,   13,   41,\n",
       "           18,   60,  479,  762,   76,  121,   72,   74,  113,  375,   91,\n",
       "         1116,   74,  218,  211,  480,  120,  121,   96,  120,  134,  291,\n",
       "          120,   81,  167,   91, 1116,   74,  109,  135,  272,   81,  167,\n",
       "           13,  204,  201,   91, 1116,   71,  196,   81,  167,   13,    3,\n",
       "          315],\n",
       "        [ 268,    4,   42,    4,   43,  366,  618,  466,  298,   47,   54,\n",
       "            6,    4,   42,    4,   43,  133,  730,  350,  663,  726,  866,\n",
       "         1760,    2,   41,   25,   31,   62,  101,  446,  157,   69,    2,\n",
       "          366,  618,   13,  291, 2043,  243,  837, 1458,  866,    9,  112,\n",
       "           25,   10,  268,    4,   43,  133,  730,   20,  481,  173,    4,\n",
       "            3,   40,  134,   20,  481,  173,   20,  481,    5,   14,    3,\n",
       "           11,   16,   69,    2,   85,  374,    8,   49,   39,    4,   42,\n",
       "            4,   43,   23,  112,  111,    3,   24,   18,   60,    4,   43,\n",
       "           74,   47,   18,   60,   41,   25,   31,   70,  154,   65,  248,\n",
       "           14]])>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_tf_dataset.padded_batch(3).take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf779c5f",
   "metadata": {},
   "source": [
    "## Split train-valid-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec477403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du train : 16503\n",
      "Taille du validation : 2064\n",
      "Taille du test : 2062\n"
     ]
    }
   ],
   "source": [
    "c = transformed_tf_dataset.reduce(0, lambda x,_:x+1).numpy()\n",
    "\n",
    "shuffled_ds = transformed_tf_dataset.shuffle(buffer_size=c, seed=42)\n",
    "\n",
    "train_size = c * 80 // 100\n",
    "test_size = c * 10 // 100\n",
    "val_size = c - train_size - test_size\n",
    "\n",
    "ds_train = shuffled_ds.take(train_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = shuffled_ds.skip(train_size).take(val_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = shuffled_ds.skip(train_size+val_size).take(test_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Taille du train :\", ds_train.cardinality().numpy())\n",
    "print(\"Taille du validation :\", ds_val.cardinality().numpy())\n",
    "print(\"Taille du test :\", ds_test.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84604ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "870dabbe",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f6c70",
   "metadata": {},
   "source": [
    "Sogou T9 est une méthode d’entrée intelligente qui :\n",
    "\n",
    "- Prend des séquences numériques (ex. : \"94664 486\" pour \"zhong guo\").\n",
    "- Génère des séquences de caractères chinois (ex. : \"中国\").\n",
    "- Utilise le contexte (mots précédents) pour désambiguïser les prédictions.\n",
    "- Est optimisé pour la vitesse et la précision, souvent avec des modèles entraînés sur de vastes corpus.\n",
    "\n",
    "Pour reproduire cela, il faut utiliser un modèle seq2seq avec un encodeur-décodeur (2 entrées) :\n",
    "\n",
    "- Encodeur : Lit la séquence T9 et la compresse en une représentation contextuelle.\n",
    "- Décodeur : Génère la séquence de caractères chinois à partir de cette représentation.\n",
    "\n",
    "[Functional API](https://keras.io/guides/functional_api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169fe750",
   "metadata": {},
   "source": [
    "Schéma conceptuel :\n",
    "\n",
    "[Contexte sinogrammes] -> [LSTM Encodeur Contexte] -> [Représentation contexte] </br>\n",
    "[T9 entrée]           -> [LSTM Encodeur T9]       -> [Représentation T9] </br>\n",
    "[Représentations contexte + T9] -> [Attention] -> [LSTM Décodeur] -> [Sortie caractères]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Encodeur 1 : Contexte en sinogrammes\n",
    "    context_input = Input(shape=(None,), dtype=tf.int32, name=\"context_input\")\n",
    "    context_embedding = Embedding(\n",
    "        input_dim=context_tv.vocabulary_size(),\n",
    "        output_dim=128,\n",
    "        mask_zero=True\n",
    "    )(context_input)\n",
    "    context_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "    context_lstm_out, context_state_h, context_state_c = context_lstm(context_embedding)\n",
    "\n",
    "    # Encodeur 2 : Entrée T9\n",
    "    t9_input = Input(shape=(None,), dtype=tf.int32, name=\"t9_input\")\n",
    "    t9_embedding = Embedding(\n",
    "        input_dim=input_t9_tv.vocabulary_size(),\n",
    "        output_dim=128,\n",
    "        mask_zero=True\n",
    "    )(t9_input)\n",
    "    t9_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "    t9_lstm_out, t9_state_h, t9_state_c = t9_lstm(t9_embedding)\n",
    "\n",
    "    # Combiner les états pour initialiser le décodeur\n",
    "    combined_state_h = Dense(256, activation='tanh')(Concatenate()([context_state_h, t9_state_h]))\n",
    "    combined_state_c = Dense(256, activation='tanh')(Concatenate()([context_state_c, t9_state_c]))\n",
    "\n",
    "    # Décodeur\n",
    "    decoder_input = Input(shape=(None,), dtype=tf.int32, name=\"decoder_input\")\n",
    "    decoder_embedding = Embedding(\n",
    "        input_dim=target_tv.vocabulary_size(),\n",
    "        output_dim=128,\n",
    "        mask_zero=True\n",
    "    )(decoder_input)\n",
    "    decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "    decoder_lstm_out, _, _ = decoder_lstm(\n",
    "        decoder_embedding,\n",
    "        initial_state=[combined_state_h, combined_state_c]\n",
    "    )\n",
    "\n",
    "    # Attention pour lier le contexte et T9 au décodeur\n",
    "    attention = Attention()([decoder_lstm_out, Concatenate()([context_lstm_out, t9_lstm_out])])\n",
    "    decoder_combined = Concatenate()([decoder_lstm_out, attention])\n",
    "\n",
    "    # Couches de prédiction\n",
    "    dense = Dense(512, activation='relu')(decoder_combined)\n",
    "    output = Dense(target_tv.vocabulary_size(), activation='softmax')(dense)\n",
    "\n",
    "    # Modèle complet\n",
    "    model = Model(\n",
    "        inputs=[context_input, t9_input, decoder_input],\n",
    "        outputs=output\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Modèle\n",
    "model = build_model()\n",
    "model.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03663220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec78a4e4",
   "metadata": {},
   "source": [
    "# Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007970b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(context, t9_sequence, max_length=50):\n",
    "    # Vectoriser les entrées\n",
    "    context_vector = context_tv([context])\n",
    "    t9_vector = input_t9_tv([t9_sequence])\n",
    "    \n",
    "    # Initialiser la séquence générée\n",
    "    generated = [target_tv.get_vocabulary().index(\"<START>\")]\n",
    "    output = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        decoder_input = tf.constant([generated], dtype=tf.int32)\n",
    "        pred = model.predict([context_vector, t9_vector, decoder_input], verbose=0)\n",
    "        next_char_idx = tf.argmax(pred[0, -1, :]).numpy()\n",
    "        next_char = target_tv.get_vocabulary()[next_char_idx]\n",
    "        \n",
    "        if next_char == \"<END>\":\n",
    "            break\n",
    "        \n",
    "        output.append(next_char)\n",
    "        generated.append(next_char_idx)\n",
    "    \n",
    "    return \"\".join(output)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "context = \"中国人民\"\n",
    "t9_input = \"543 3264\"  # Pour \"jie fang\"\n",
    "result = generate_text(context, t9_input)\n",
    "print(f\"Contexte: {context}, T9: {t9_input} -> Résultat: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_metal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
