{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a364bb7",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de1c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/perrineqhn/.cache/kagglehub/datasets/noxmoon/chinese-official-daily-news-since-2016/versions/1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import kagglehub\n",
    "import keras\n",
    "import keras_hub\n",
    "import keras_tuner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pkuseg\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from pypinyin import lazy_pinyin\n",
    "from keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    StringLookup,\n",
    "    TextVectorization,\n",
    ")\n",
    "from keras.models import Model\n",
    "import pickle as pk\n",
    "import gc\n",
    "\n",
    "path = kagglehub.dataset_download(\"noxmoon/chinese-official-daily-news-since-2016\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3e85f",
   "metadata": {},
   "source": [
    "# Création du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1de02ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20738 entries, 0 to 20737\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   date      20738 non-null  object\n",
      " 1   tag       20738 non-null  object\n",
      " 2   headline  20738 non-null  object\n",
      " 3   content   20631 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 648.2+ KB\n",
      "None\n",
      "Dataset head:\n",
      "         date   tag                                           headline  \\\n",
      "0  2016-01-01  详细全文  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...   \n",
      "1  2016-01-01  详细全文                             中央军委印发《关于深化国防和军队改革的意见》   \n",
      "2  2016-01-01  详细全文                           《习近平关于严明党的纪律和规矩论述摘编》出版发行   \n",
      "3  2016-01-01  详细全文                                 以实际行动向党中央看齐 向高标准努力   \n",
      "4  2016-01-01  详细全文                                 【年终特稿】关键之年 改革挺进深水区   \n",
      "\n",
      "                                             content  \n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...  \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...  \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...  \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...  \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...  \n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(path+\"/chinese_news.csv\")\n",
    "# Print dataset information\n",
    "print(\"Dataset information:\")\n",
    "print(dataset.info())\n",
    "# Print dataset head\n",
    "print(\"Dataset head:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef64267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after preprocessing:\n",
      "                                             content  \\\n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...   \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...   \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...   \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...   \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会年月日在...  \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。《意见》强调，...  \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...  \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...  \n",
      "4  刚刚过去的年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，集中发力...  \n",
      "0    [中国, 人民, 解放军, 陆军, 领导, 机构, 、, 中国, 人民, 解放军, 火箭军,...\n",
      "1    [经, 中央军委, 主席, 习近平, 批准, ，, 中央军委, 近日, 印发, 了, 《, ...\n",
      "2    [由, 中共中央, 纪律, 检查, 委员会, 、, 中共中央, 文献, 研究室, 编辑, 的...\n",
      "3    [广大, 党员, 干部, 正在, 积极, 学习, 习近平, 总书记, 在, 中央, 政治局,...\n",
      "4    [刚刚, 过去, 的, 年, ，, 是, 全面, 深化, 改革, 的, 关键, 之, 年, ...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Prétraitement de content (suppression des caractères non chinois, normalisation des espaces)\n",
    "def clean_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Garder les caractères chinois et ponctuation chinoise\n",
    "    text = re.sub(r\"[^\\u4e00-\\u9fff\\u3000-\\u303F\\uff00-\\uffef]\", \"\", text)\n",
    "    \n",
    "    # Normaliser les espaces (rare, mais au cas où)\n",
    "    text = text.replace(\" \", \"\").strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remplacer les valeurs manquantes par une chaîne vide\n",
    "dataset[\"content\"] = dataset[\"content\"].fillna(\"\")\n",
    "\n",
    "# Appliquer le prétraitement à la colonne 'content'\n",
    "dataset['cleaned_content'] = dataset['content'].apply(clean_content)\n",
    "\n",
    "# Filtrer les lignes où 'cleaned_content' est vide\n",
    "dataset = dataset[dataset[\"cleaned_content\"].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# Afficher les 5 premières lignes du DataFrame après le prétraitement\n",
    "print(\"Dataset after preprocessing:\")\n",
    "print(dataset[['content', 'cleaned_content']].head())\n",
    "\n",
    "seg = pkuseg.pkuseg()\n",
    "dataset[\"tokens\"] = dataset[\"cleaned_content\"].apply(lambda x: seg.cut(x))\n",
    "\n",
    "# Aperçu\n",
    "print(dataset[\"tokens\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add6bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after generating sequences:\n",
      "                                             content  \\\n",
      "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...   \n",
      "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...   \n",
      "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...   \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...   \n",
      "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...   \n",
      "\n",
      "                             char_pinyin_t9_sequence  \n",
      "0  中|zhong|94664 国|guo|486 人|ren|736 民|min|646 解|...  \n",
      "1  经|jing|5464 中|zhong|94664 央|yang|9264 军|jun|58...  \n",
      "2  由|you|968 中|zhong|94664 共|gong|4664 中|zhong|94...  \n",
      "3  广|guang|48264 大|da|32 党|dang|3264 员|yuan|9826 ...  \n",
      "4  刚|gang|4264 刚|gang|4264 过|guo|486 去|qu|78 的|de...  \n"
     ]
    }
   ],
   "source": [
    "# convert the content column to pinyin\n",
    "t9_map = {\n",
    "    \"a\": \"2\", \"b\": \"2\", \"c\": \"2\",\n",
    "    \"d\": \"3\", \"e\": \"3\", \"f\": \"3\",\n",
    "    \"g\": \"4\", \"h\": \"4\", \"i\": \"4\",\n",
    "    \"j\": \"5\", \"k\": \"5\", \"l\": \"5\",\n",
    "    \"m\": \"6\", \"n\": \"6\", \"o\": \"6\",\n",
    "    \"p\": \"7\", \"q\": \"7\", \"r\": \"7\", \"s\": \"7\",\n",
    "    \"t\": \"8\", \"u\": \"8\", \"v\": \"8\",\n",
    "    \"w\": \"9\", \"x\": \"9\", \"y\": \"9\", \"z\": \"9\",\n",
    "    \"1\": \"1\", \"2\": \"2\", \"3\": \"3\", \"4\": \"4\",\n",
    "    \"5\": \"5\", \"6\": \"6\", \"7\": \"7\", \"8\": \"8\",\n",
    "    \"9\": \"9\", \"0\": \"0\",\n",
    "    \"。\":\"。\", \"，\":\"，\", \"？\":\"？\", \"！\":\"！\",\n",
    "}\n",
    "\n",
    "# Fonction pour convertir une chaîne de caractères en code T9\n",
    "def pinyin_to_t9(text):\n",
    "    t9_code = \"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    for char in text.lower():\n",
    "        t9_code += t9_map.get(char, char)  # Conserver les caractères non mappés\n",
    "    return t9_code\n",
    "\n",
    "def validate_t9(t9_code):\n",
    "    # Vérifie que le code T9 est numérique (ou vide pour ponctuation)\n",
    "    return bool(re.match(r'^[0-9]+$', t9_code)) or t9_code in {\"。\", \"，\", \"？\", \"！\"}\n",
    "\n",
    "def generer_sequence_contextuelle(row):\n",
    "    tokens = row[\"tokens\"]\n",
    "    sequence = []\n",
    "    for token in tokens:\n",
    "        if not isinstance(token, str) or not re.search(r'[\\u4e00-\\u9fff]', token):\n",
    "            continue\n",
    "        for char, py in zip(token, lazy_pinyin(token)):\n",
    "            t9 = pinyin_to_t9(py)\n",
    "            if validate_t9(t9):  # Vérifier que le T9 est valide\n",
    "                sequence.append(f\"{char}|{py}|{t9}\")\n",
    "    return ' '.join(sequence)\n",
    "\n",
    "dataset[\"char_pinyin_t9_sequence\"] = dataset.apply(generer_sequence_contextuelle, axis=1)\n",
    "\n",
    "# Filtrer les lignes où 'char_pinyin_t9_sequence' est vide\n",
    "dataset = dataset[dataset[\"char_pinyin_t9_sequence\"].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# Sauvegarder le fichier\n",
    "dataset[[\"char_pinyin_t9_sequence\"]].to_csv(\"sequences_char_pinyin_t9.csv\", index=False)\n",
    "\n",
    "# Afficher les 5 premières lignes du DataFrame après le prétraitement\n",
    "print(\"Dataset after generating sequences:\")\n",
    "print(dataset[['content', 'char_pinyin_t9_sequence']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc41c160",
   "metadata": {},
   "source": [
    "# Création du dataset pour le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9336abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer en séquences complètes\n",
    "input_t9_sequences = []\n",
    "target_char_sequences = []\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "for seq in dataset[\"char_pinyin_t9_sequence\"]:\n",
    "    triplets = seq.strip().split(\" \")\n",
    "    t9_seq = []\n",
    "    char_seq = []\n",
    "    \n",
    "    # Extraire les paires char|T9 pour chaque phrase\n",
    "    for triplet in triplets[:MAX_SEQUENCE_LENGTH]:  # Tronquer à MAX_SEQUENCE_LENGTH\n",
    "        parts = triplet.split(\"|\")\n",
    "        if len(parts) == 3:\n",
    "            char, _, t9 = parts\n",
    "            if validate_t9(t9):  # Vérifier que le T9 est valide\n",
    "                char_seq.append(char)\n",
    "                t9_seq.append(t9)\n",
    "    \n",
    "    # Ajouter les séquences T9 et caractères si non vides\n",
    "    if t9_seq and char_seq:\n",
    "        input_t9_sequences.append(\" \".join(t9_seq))\n",
    "        target_char_sequences.append(\"\".join(char_seq)) # A voir si on garde les espaces ou pas\n",
    "\n",
    "# Créer un DataFrame\n",
    "df_sequences = pd.DataFrame({\n",
    "    \"input_t9_sequence\": input_t9_sequences,\n",
    "    \"target_char_sequence\": target_char_sequences\n",
    "})\n",
    "\n",
    "# Filtrer les séquences vides (par précaution)\n",
    "df_sequences = df_sequences[df_sequences[\"input_t9_sequence\"].str.strip() != \"\"]\n",
    "df_sequences = df_sequences[df_sequences[\"target_char_sequence\"].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f096bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame sequences:\n",
      "                                   input_t9_sequence  \\\n",
      "0  94664 486 736 646 543 3264 586 58 586 5464 326...   \n",
      "1  5464 94664 9264 586 934 948 94 94 546 7464 74 ...   \n",
      "2  968 94664 4664 94664 9264 54 58 5426 242 934 9...   \n",
      "3  48264 32 3264 9826 426 28 94364 924 54 54 983 ...   \n",
      "4  4264 4264 486 78 33 6426 744 7826 6426 7436 48...   \n",
      "\n",
      "                                target_char_sequence  \n",
      "0  中国人民解放军陆军领导机构中国人民解放军火箭军中国人民解放军战略支援部队成立大会年月日在八一...  \n",
      "1  经中央军委主席习近平批准中央军委近日印发了关于深化国防和军队改革的意见意见强调党的十八大以来...  \n",
      "2  由中共中央纪律检查委员会中共中央文献研究室编辑的习近平关于严明党的纪律和规矩论述摘编一书近日...  \n",
      "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话大家纷纷表示要把践...  \n",
      "4  刚刚过去的年是全面深化改革的关键之年改革集中发力在制约经济社会发展的深层次矛盾集中发力在妨碍...  \n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame sequences:\")\n",
    "print(df_sequences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca9ffcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 18:25:28.440063: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2025-04-15 18:25:28.440100: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2025-04-15 18:25:28.440105: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744734328.440310   10344 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744734328.440485   10344 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b'94664 486 736 646 543 3264 586 58 586 5464 326 54 468 94664 486 736 646 543 3264 586 486 5426 586 94664 486 736 646 543 3264 586 9426 583 944 9826 28 384 24364 54 32 484 6426 983 74 924 22 94 32 568 5664 94664 58 9464 94664 4664 94664 9264 9664 748 54 486 542 948 94 94664 9264 586 934 948 94 94 546 7464 94264 58 586 486 5426 586 9426 583 944 9826 28 384 7468 98 586 74 2464 944 986 24 324 2426 3264 94664 9264 43 94664 9264'>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe4\\xb8\\xad\\xe5\\x9b\\xbd\\xe4\\xba\\xba\\xe6\\xb0\\x91\\xe8\\xa7\\xa3\\xe6\\x94\\xbe\\xe5\\x86\\x9b\\xe9\\x99\\x86\\xe5\\x86\\x9b\\xe9\\xa2\\x86\\xe5\\xaf\\xbc\\xe6\\x9c\\xba\\xe6\\x9e\\x84\\xe4\\xb8\\xad\\xe5\\x9b\\xbd\\xe4\\xba\\xba\\xe6\\xb0\\x91\\xe8\\xa7\\xa3\\xe6\\x94\\xbe\\xe5\\x86\\x9b\\xe7\\x81\\xab\\xe7\\xae\\xad\\xe5\\x86\\x9b\\xe4\\xb8\\xad\\xe5\\x9b\\xbd\\xe4\\xba\\xba\\xe6\\xb0\\x91\\xe8\\xa7\\xa3\\xe6\\x94\\xbe\\xe5\\x86\\x9b\\xe6\\x88\\x98\\xe7\\x95\\xa5\\xe6\\x94\\xaf\\xe6\\x8f\\xb4\\xe9\\x83\\xa8\\xe9\\x98\\x9f\\xe6\\x88\\x90\\xe7\\xab\\x8b\\xe5\\xa4\\xa7\\xe4\\xbc\\x9a\\xe5\\xb9\\xb4\\xe6\\x9c\\x88\\xe6\\x97\\xa5\\xe5\\x9c\\xa8\\xe5\\x85\\xab\\xe4\\xb8\\x80\\xe5\\xa4\\xa7\\xe6\\xa5\\xbc\\xe9\\x9a\\x86\\xe9\\x87\\x8d\\xe4\\xb8\\xbe\\xe8\\xa1\\x8c\\xe4\\xb8\\xad\\xe5\\x85\\xb1\\xe4\\xb8\\xad\\xe5\\xa4\\xae\\xe6\\x80\\xbb\\xe4\\xb9\\xa6\\xe8\\xae\\xb0\\xe5\\x9b\\xbd\\xe5\\xae\\xb6\\xe4\\xb8\\xbb\\xe5\\xb8\\xad\\xe4\\xb8\\xad\\xe5\\xa4\\xae\\xe5\\x86\\x9b\\xe5\\xa7\\x94\\xe4\\xb8\\xbb\\xe5\\xb8\\xad\\xe4\\xb9\\xa0\\xe8\\xbf\\x91\\xe5\\xb9\\xb3\\xe5\\x90\\x91\\xe9\\x99\\x86\\xe5\\x86\\x9b\\xe7\\x81\\xab\\xe7\\xae\\xad\\xe5\\x86\\x9b\\xe6\\x88\\x98\\xe7\\x95\\xa5\\xe6\\x94\\xaf\\xe6\\x8f\\xb4\\xe9\\x83\\xa8\\xe9\\x98\\x9f\\xe6\\x8e\\x88\\xe4\\xba\\x88\\xe5\\x86\\x9b\\xe6\\x97\\x97\\xe5\\xb9\\xb6\\xe8\\x87\\xb4\\xe8\\xae\\xad\\xe8\\xaf\\x8d\\xe4\\xbb\\xa3\\xe8\\xa1\\xa8\\xe5\\x85\\x9a\\xe4\\xb8\\xad\\xe5\\xa4\\xae\\xe5\\x92\\x8c\\xe4\\xb8\\xad\\xe5\\xa4\\xae'>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utiliser tf.data.Dataset\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((df_sequences['input_t9_sequence'], df_sequences['target_char_sequence'])).prefetch(tf.data.AUTOTUNE)\n",
    "tf_dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6434642",
   "metadata": {},
   "source": [
    "## Encoder les données pour Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02e326d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 18:26:08.356557: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-04-15 18:26:49.077237: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization\n",
    "input_tv = keras.layers.TextVectorization(output_mode='int',\n",
    "                                          split='character',\n",
    "                                          standardize=None,\n",
    "                                          ragged=True,)\n",
    "\n",
    "target_tv = keras.layers.TextVectorization(output_mode='int',\n",
    "                                           split='character',\n",
    "                                           standardize=None,\n",
    "                                           ragged=True,)\n",
    "\n",
    "tmp_t9_ds = tf_dataset.map(lambda t9, target: tf.strings.reduce_join(tf.strings.split(t9, \" \"), separator=\"\"))\n",
    "t9_ds = tf_dataset.map(lambda t9, target: t9)\n",
    "target_ds = tf_dataset.map(lambda t9, target: target)\n",
    "input_tv.adapt(tmp_t9_ds)\n",
    "target_tv.adapt(target_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "042f07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('4'), np.str_('6'), np.str_('2'), np.str_('3'), np.str_('8'), np.str_('9'), np.str_('5'), np.str_('7'), np.str_('。')]\n",
      "['', '[UNK]', np.str_('的'), np.str_('国'), np.str_('中'), np.str_('在'), np.str_('会'), np.str_('人'), np.str_('大'), np.str_('一'), np.str_('日'), np.str_('发'), np.str_('年'), np.str_('和'), np.str_('全'), np.str_('天'), np.str_('行'), np.str_('了'), np.str_('主'), np.str_('地'), np.str_('出'), np.str_('部'), np.str_('新'), np.str_('总'), np.str_('家'), np.str_('近'), np.str_('民'), np.str_('政'), np.str_('今'), np.str_('是'), np.str_('为'), np.str_('平'), np.str_('作'), np.str_('上'), np.str_('开'), np.str_('进'), np.str_('表'), np.str_('展'), np.str_('成'), np.str_('来'), np.str_('方'), np.str_('习'), np.str_('共'), np.str_('央'), np.str_('对'), np.str_('有'), np.str_('时'), np.str_('委'), np.str_('动'), np.str_('以'), np.str_('合'), np.str_('个'), np.str_('议'), np.str_('月'), np.str_('员'), np.str_('多'), np.str_('长'), np.str_('生'), np.str_('务'), np.str_('将'), np.str_('席'), np.str_('区'), np.str_('关'), np.str_('要'), np.str_('同'), np.str_('重'), np.str_('工'), np.str_('公'), np.str_('前'), np.str_('党'), np.str_('高'), np.str_('经'), np.str_('强'), np.str_('业'), np.str_('军'), np.str_('理'), np.str_('实'), np.str_('第'), np.str_('等'), np.str_('斯'), np.str_('次'), np.str_('建'), np.str_('到'), np.str_('这'), np.str_('利'), np.str_('十'), np.str_('示'), np.str_('海'), np.str_('美'), np.str_('法'), np.str_('北'), np.str_('统'), np.str_('举'), np.str_('加'), np.str_('通'), np.str_('体'), np.str_('代'), np.str_('克'), np.str_('院'), np.str_('机'), np.str_('布'), np.str_('于'), np.str_('亚'), np.str_('号'), np.str_('组'), np.str_('我'), np.str_('不'), np.str_('两'), np.str_('当'), np.str_('事'), np.str_('下'), np.str_('记'), np.str_('书'), np.str_('目'), np.str_('联'), np.str_('特'), np.str_('过'), np.str_('分'), np.str_('面'), np.str_('与'), np.str_('化'), np.str_('现'), np.str_('西'), np.str_('市'), np.str_('持'), np.str_('治'), np.str_('学'), np.str_('南'), np.str_('安'), np.str_('尔'), np.str_('名'), np.str_('首'), np.str_('内'), np.str_('文'), np.str_('正'), np.str_('力'), np.str_('系'), np.str_('最'), np.str_('产'), np.str_('自'), np.str_('京'), np.str_('外'), np.str_('报'), np.str_('三'), np.str_('道'), np.str_('领'), np.str_('路'), np.str_('里'), np.str_('后'), np.str_('各'), np.str_('战'), np.str_('定'), np.str_('制'), np.str_('局'), np.str_('度'), np.str_('交'), np.str_('间'), np.str_('明'), np.str_('入'), np.str_('际'), np.str_('改'), np.str_('问'), np.str_('指'), np.str_('导'), np.str_('式'), np.str_('本'), np.str_('期'), np.str_('设'), np.str_('府'), np.str_('场'), np.str_('拉'), np.str_('题'), np.str_('点'), np.str_('社'), np.str_('达'), np.str_('电'), np.str_('届'), np.str_('保'), np.str_('向'), np.str_('从'), np.str_('推'), np.str_('江'), np.str_('并'), np.str_('俄'), np.str_('万'), np.str_('能'), np.str_('起'), np.str_('常'), np.str_('就'), np.str_('已'), np.str_('东'), np.str_('罗'), np.str_('德'), np.str_('比'), np.str_('巴'), np.str_('山'), np.str_('济'), np.str_('提'), np.str_('基'), np.str_('者'), np.str_('增'), np.str_('用'), np.str_('深'), np.str_('台'), np.str_('运'), np.str_('心'), np.str_('团'), np.str_('协'), np.str_('说'), np.str_('科'), np.str_('调'), np.str_('革'), np.str_('见'), np.str_('续'), np.str_('界'), np.str_('好'), np.str_('至'), np.str_('受'), np.str_('队'), np.str_('其'), np.str_('活'), np.str_('织'), np.str_('造'), np.str_('位'), np.str_('伊'), np.str_('们'), np.str_('计'), np.str_('及'), np.str_('数'), np.str_('世'), np.str_('省'), np.str_('李'), np.str_('二'), np.str_('他'), np.str_('办'), np.str_('任'), np.str_('门'), np.str_('水'), np.str_('得'), np.str_('决'), np.str_('金'), np.str_('据'), np.str_('午'), np.str_('论'), np.str_('资'), np.str_('项'), np.str_('创'), np.str_('带'), np.str_('视'), np.str_('华'), np.str_('村'), np.str_('车'), np.str_('施'), np.str_('小'), np.str_('之'), np.str_('立'), np.str_('信'), np.str_('气'), np.str_('宣'), np.str_('相'), np.str_('空'), np.str_('程'), np.str_('击'), np.str_('节'), np.str_('意'), np.str_('选'), np.str_('流'), np.str_('约'), np.str_('由'), np.str_('列'), np.str_('广'), np.str_('结'), np.str_('量'), np.str_('州'), np.str_('参'), np.str_('马'), np.str_('案'), np.str_('元'), np.str_('话'), np.str_('阿'), np.str_('农'), np.str_('叙'), np.str_('也'), np.str_('级'), np.str_('讲'), np.str_('被'), np.str_('色'), np.str_('都'), np.str_('四'), np.str_('集'), np.str_('城'), np.str_('规'), np.str_('访'), np.str_('取'), np.str_('五'), np.str_('此'), np.str_('精'), np.str_('子'), np.str_('查'), np.str_('网'), np.str_('域'), np.str_('境'), np.str_('物'), np.str_('应'), np.str_('众'), np.str_('专'), np.str_('航'), np.str_('火'), np.str_('着'), np.str_('放'), np.str_('件'), np.str_('贫'), np.str_('谈'), np.str_('性'), np.str_('技'), np.str_('防'), np.str_('张'), np.str_('风'), np.str_('接'), np.str_('品'), np.str_('线'), np.str_('警'), np.str_('义'), np.str_('认'), np.str_('坚'), np.str_('可'), np.str_('球'), np.str_('欧'), np.str_('亿'), np.str_('极'), np.str_('反'), np.str_('朗'), np.str_('九'), np.str_('商'), np.str_('赛'), np.str_('企'), np.str_('支'), np.str_('情'), np.str_('别'), np.str_('步'), np.str_('武'), np.str_('解'), np.str_('护'), np.str_('投'), np.str_('致'), np.str_('所'), np.str_('构'), np.str_('客'), np.str_('核'), np.str_('降'), np.str_('研'), np.str_('尼'), np.str_('普'), np.str_('土'), np.str_('幕'), np.str_('收'), np.str_('处'), np.str_('传'), np.str_('还'), np.str_('回'), np.str_('双'), np.str_('批'), np.str_('告'), np.str_('口'), np.str_('装'), np.str_('铁'), np.str_('纪'), np.str_('先'), np.str_('神'), np.str_('晚'), np.str_('米'), np.str_('去'), np.str_('求'), np.str_('周'), np.str_('八'), np.str_('标'), np.str_('非'), np.str_('始'), np.str_('互'), np.str_('升'), np.str_('河'), np.str_('获'), np.str_('打'), np.str_('引'), np.str_('英'), np.str_('态'), np.str_('春'), np.str_('游'), np.str_('果'), np.str_('袭'), np.str_('播'), np.str_('志'), np.str_('预'), np.str_('干'), np.str_('落'), np.str_('手'), np.str_('群'), np.str_('环'), np.str_('古'), np.str_('士'), np.str_('型'), np.str_('兰'), np.str_('完'), np.str_('县'), np.str_('署'), np.str_('显'), np.str_('责'), np.str_('韩'), np.str_('术'), np.str_('召'), np.str_('盟'), np.str_('使'), np.str_('速'), np.str_('少'), np.str_('格'), np.str_('划'), np.str_('确'), np.str_('连'), np.str_('演'), np.str_('林'), np.str_('教'), np.str_('管'), np.str_('更'), np.str_('声'), np.str_('维'), np.str_('势'), np.str_('奥'), np.str_('影'), np.str_('源'), np.str_('历'), np.str_('百'), np.str_('原'), np.str_('略'), np.str_('副'), np.str_('老'), np.str_('难'), np.str_('准'), np.str_('迎'), np.str_('卫'), np.str_('服'), np.str_('超'), np.str_('雨'), np.str_('卡'), np.str_('堂'), np.str_('育'), np.str_('继'), np.str_('严'), np.str_('审'), np.str_('种'), np.str_('港'), np.str_('飞'), np.str_('称'), np.str_('走'), np.str_('权'), np.str_('司'), np.str_('闻'), np.str_('贯'), np.str_('因'), np.str_('医'), np.str_('条'), np.str_('边'), np.str_('消'), np.str_('察'), np.str_('夫'), np.str_('旅'), np.str_('响'), np.str_('检'), np.str_('印'), np.str_('友'), np.str_('试'), np.str_('青'), np.str_('易'), np.str_('亡'), np.str_('包'), np.str_('沙'), np.str_('伤'), np.str_('哈'), np.str_('供'), np.str_('峰'), np.str_('围'), np.str_('命'), np.str_('版'), np.str_('死'), np.str_('坦'), np.str_('贸'), np.str_('王'), np.str_('稳'), np.str_('端'), np.str_('积'), np.str_('官'), np.str_('频'), np.str_('彻'), np.str_('洲'), np.str_('转'), np.str_('单'), np.str_('族'), np.str_('复'), np.str_('苏'), np.str_('策'), np.str_('监'), np.str_('突'), np.str_('救'), np.str_('快'), np.str_('质'), np.str_('媒'), np.str_('备'), np.str_('启'), np.str_('形'), np.str_('让'), np.str_('断'), np.str_('兴'), np.str_('星'), np.str_('变'), np.str_('验'), np.str_('朝'), np.str_('价'), np.str_('越'), np.str_('章'), np.str_('息'), np.str_('段'), np.str_('勒'), np.str_('层'), np.str_('该'), np.str_('坛'), np.str_('站'), np.str_('采'), np.str_('直'), np.str_('然'), np.str_('评'), np.str_('看'), np.str_('思'), np.str_('观'), np.str_('营'), np.str_('塔'), np.str_('困'), np.str_('乡'), np.str_('座'), np.str_('模'), np.str_('根'), np.str_('史'), np.str_('份'), np.str_('执'), np.str_('陆'), np.str_('给'), np.str_('富'), np.str_('头'), np.str_('而'), np.str_('福'), np.str_('无'), np.str_('庆'), np.str_('清'), np.str_('往'), np.str_('灾'), np.str_('范'), np.str_('湖'), np.str_('岛'), np.str_('弹'), np.str_('功'), np.str_('效'), np.str_('脱'), np.str_('热'), np.str_('想'), np.str_('兵'), np.str_('优'), np.str_('温'), np.str_('龙'), np.str_('融'), np.str_('证'), np.str_('塞'), np.str_('居'), np.str_('宁'), np.str_('再'), np.str_('把'), np.str_('做'), np.str_('但'), np.str_('未'), np.str_('念'), np.str_('博'), np.str_('遇'), np.str_('费'), np.str_('随'), np.str_('耳'), np.str_('季'), np.str_('征'), np.str_('每'), np.str_('措'), np.str_('考'), np.str_('负'), np.str_('仪'), np.str_('感'), np.str_('图'), np.str_('驻'), np.str_('职'), np.str_('税'), np.str_('萨'), np.str_('雪'), np.str_('班'), np.str_('票'), np.str_('欢'), np.str_('助'), np.str_('昨'), np.str_('六'), np.str_('综'), np.str_('类'), np.str_('康'), np.str_('些'), np.str_('没'), np.str_('故'), np.str_('整'), np.str_('纳'), np.str_('半'), np.str_('言'), np.str_('丽'), np.str_('光'), np.str_('况'), np.str_('船'), np.str_('知'), np.str_('紧'), np.str_('律'), np.str_('烈'), np.str_('切'), np.str_('身'), np.str_('洋'), np.str_('象'), np.str_('红'), np.str_('涉'), np.str_('射'), np.str_('伙'), np.str_('善'), np.str_('馆'), np.str_('轮'), np.str_('祝'), np.str_('愿'), np.str_('识'), np.str_('黄'), np.str_('括'), np.str_('修'), np.str_('云'), np.str_('贺'), np.str_('藏'), np.str_('千'), np.str_('派'), np.str_('控'), np.str_('库'), np.str_('望'), np.str_('签'), np.str_('促'), np.str_('真'), np.str_('候'), np.str_('埃'), np.str_('扶'), np.str_('率'), np.str_('离'), np.str_('鲜'), np.str_('破'), np.str_('暴'), np.str_('遭'), np.str_('蒙'), np.str_('益'), np.str_('测'), np.str_('希'), np.str_('请'), np.str_('景'), np.str_('究'), np.str_('户'), np.str_('几'), np.str_('器'), np.str_('香'), np.str_('停'), np.str_('厅'), np.str_('银'), np.str_('如'), np.str_('吉'), np.str_('川'), np.str_('伴'), np.str_('镇'), np.str_('恐'), np.str_('女'), np.str_('震'), np.str_('校'), np.str_('攻'), np.str_('具'), np.str_('炸'), np.str_('依'), np.str_('远'), np.str_('白'), np.str_('送'), np.str_('财'), np.str_('录'), np.str_('底'), np.str_('爆'), np.str_('亲'), np.str_('阳'), np.str_('黑'), np.str_('园'), np.str_('值'), np.str_('障'), np.str_('片'), np.str_('花'), np.str_('移'), np.str_('病'), np.str_('失'), np.str_('智'), np.str_('乐'), np.str_('艺'), np.str_('络'), np.str_('索'), np.str_('承'), np.str_('减'), np.str_('讨'), np.str_('排'), np.str_('罪'), np.str_('终'), np.str_('威'), np.str_('雷'), np.str_('住'), np.str_('鲁'), np.str_('健'), np.str_('探'), np.str_('房'), np.str_('载'), np.str_('波'), np.str_('嫌'), np.str_('需'), np.str_('油'), np.str_('瓦'), np.str_('室'), np.str_('岁'), np.str_('只'), np.str_('督'), np.str_('献'), np.str_('介'), np.str_('争'), np.str_('太'), np.str_('满'), np.str_('津'), np.str_('听'), np.str_('援'), np.str_('乌'), np.str_('照'), np.str_('七'), np.str_('聚'), np.str_('宾'), np.str_('儿'), np.str_('货'), np.str_('即'), np.str_('冬'), np.str_('巡'), np.str_('初'), np.str_('临'), np.str_('急'), np.str_('础'), np.str_('密'), np.str_('刚'), np.str_('额'), np.str_('险'), np.str_('澳'), np.str_('注'), np.str_('抗'), np.str_('则'), np.str_('令'), np.str_('足'), np.str_('绕'), np.str_('沿'), np.str_('闭'), np.str_('登'), np.str_('抵'), np.str_('邦'), np.str_('容'), np.str_('样'), np.str_('违'), np.str_('草'), np.str_('疆'), np.str_('桥'), np.str_('冲'), np.str_('例'), np.str_('输'), np.str_('置'), np.str_('绿'), np.str_('犯'), np.str_('除'), np.str_('石'), np.str_('抓'), np.str_('哥'), np.str_('秘'), np.str_('舰'), np.str_('斗'), np.str_('贵'), np.str_('截'), np.str_('低'), np.str_('架'), np.str_('束'), np.str_('担'), np.str_('奖'), np.str_('判'), np.str_('练'), np.str_('针'), np.str_('晨'), np.str_('绍'), np.str_('待'), np.str_('食'), np.str_('胜'), np.str_('浙'), np.str_('努'), np.str_('牌'), np.str_('梅'), np.str_('害'), np.str_('奋'), np.str_('俞'), np.str_('疗'), np.str_('岸'), np.str_('仍'), np.str_('邀'), np.str_('币'), np.str_('疑'), np.str_('宪'), np.str_('阶'), np.str_('早'), np.str_('挥'), np.str_('较'), np.str_('假'), np.str_('附'), np.str_('冰'), np.str_('按'), np.str_('师'), np.str_('夏'), np.str_('典'), np.str_('训'), np.str_('瑞'), np.str_('均'), np.str_('扩'), np.str_('扎'), np.str_('刘'), np.str_('止'), np.str_('托'), np.str_('裁'), np.str_('莫'), np.str_('述'), np.str_('购'), np.str_('怖'), np.str_('爱'), np.str_('仅'), np.str_('顺'), np.str_('览'), np.str_('曾'), np.str_('凌'), np.str_('才'), np.str_('药'), np.str_('何'), np.str_('伦'), np.str_('伟'), np.str_('祖'), np.str_('晤'), np.str_('存'), np.str_('男'), np.str_('盖'), np.str_('留'), np.str_('庭'), np.str_('良'), np.str_('延'), np.str_('充'), np.str_('毒'), np.str_('跨'), np.str_('幅'), np.str_('必'), np.str_('剧'), np.str_('编'), np.str_('丰'), np.str_('很'), np.str_('诚'), np.str_('守'), np.str_('菲'), np.str_('姆'), np.str_('久'), np.str_('逐'), np.str_('属'), np.str_('泰'), np.str_('汇'), np.str_('许'), np.str_('田'), np.str_('森'), np.str_('补'), np.str_('养'), np.str_('予'), np.str_('黎'), np.str_('洪'), np.str_('或'), np.str_('限'), np.str_('乘'), np.str_('申'), np.str_('枪'), np.str_('字'), np.str_('惠'), np.str_('劳'), np.str_('摩'), np.str_('款'), np.str_('捕'), np.str_('危'), np.str_('昌'), np.str_('又'), np.str_('染'), np.str_('什'), np.str_('污'), np.str_('另'), np.str_('余'), np.str_('压'), np.str_('短'), np.str_('培'), np.str_('享'), np.str_('退'), np.str_('歌'), np.str_('右'), np.str_('占'), np.str_('语'), np.str_('荣'), np.str_('禁'), np.str_('激'), np.str_('汽'), np.str_('野'), np.str_('毁'), np.str_('卢'), np.str_('迪'), np.str_('吨'), np.str_('读'), np.str_('牙'), np.str_('梦'), np.str_('那'), np.str_('汉'), np.str_('算'), np.str_('返'), np.str_('固'), np.str_('侧'), np.str_('湾'), np.str_('诺'), np.str_('树'), np.str_('辆'), np.str_('洛'), np.str_('竞'), np.str_('汗'), np.str_('寨'), np.str_('呼'), np.str_('售'), np.str_('慰'), np.str_('遗'), np.str_('甘'), np.str_('筑'), np.str_('配'), np.str_('穆'), np.str_('己'), np.str_('姓'), np.str_('追'), np.str_('状'), np.str_('伯'), np.str_('践'), np.str_('礼'), np.str_('独'), np.str_('宫'), np.str_('赴'), np.str_('振'), np.str_('宝'), np.str_('赢'), np.str_('角'), np.str_('肃'), np.str_('徽'), np.str_('销'), np.str_('赞'), np.str_('奇'), np.str_('夜'), np.str_('私'), np.str_('换'), np.str_('刻'), np.str_('杭'), np.str_('眼'), np.str_('吸'), np.str_('冠'), np.str_('陈'), np.str_('残'), np.str_('左'), np.str_('冷'), np.str_('扬'), np.str_('默'), np.str_('汪'), np.str_('陕'), np.str_('么'), np.str_('齐'), np.str_('盛'), np.str_('轨'), np.str_('纷'), np.str_('序'), np.str_('累'), np.str_('箭'), np.str_('旗'), np.str_('损'), np.str_('恩'), np.str_('赫'), np.str_('覆'), np.str_('涨'), np.str_('杜'), np.str_('雄'), np.str_('曼'), np.str_('料'), np.str_('拿'), np.str_('秀'), np.str_('熊'), np.str_('授'), np.str_('丹'), np.str_('靠'), np.str_('店'), np.str_('繁'), np.str_('帮'), np.str_('零'), np.str_('贝'), np.str_('砖'), np.str_('永'), np.str_('辽'), np.str_('舞'), np.str_('便'), np.str_('邻'), np.str_('距'), np.str_('谊'), np.str_('素'), np.str_('音'), np.str_('潜'), np.str_('贡'), np.str_('键'), np.str_('滑'), np.str_('夺'), np.str_('诉'), np.str_('篇'), np.str_('粮'), np.str_('拥'), np.str_('耶'), np.str_('亮'), np.str_('童'), np.str_('否'), np.str_('杀'), np.str_('阅'), np.str_('钢'), np.str_('她'), np.str_('彩'), np.str_('巨'), np.str_('猫'), np.str_('旨'), np.str_('尽'), np.str_('圆'), np.str_('板'), np.str_('须'), np.str_('阁'), np.str_('迈'), np.str_('街'), np.str_('微'), np.str_('蓝'), np.str_('散'), np.str_('楼'), np.str_('潮'), np.str_('某'), np.str_('它'), np.str_('订'), np.str_('胡'), np.str_('秋'), np.str_('画'), np.str_('寻'), np.str_('写'), np.str_('您'), np.str_('植'), np.str_('杰'), np.str_('倍'), np.str_('辞'), np.str_('撤'), np.str_('归'), np.str_('鱼'), np.str_('股'), np.str_('庄'), np.str_('像'), np.str_('丝'), np.str_('迹'), np.str_('课'), np.str_('讯'), np.str_('暂'), np.str_('搜'), np.str_('途'), np.str_('肯'), np.str_('敦'), np.str_('储'), np.str_('倡'), np.str_('坡'), np.str_('雅'), np.str_('纽'), np.str_('牢'), np.str_('栗'), np.str_('妇'), np.str_('适'), np.str_('木'), np.str_('缅'), np.str_('谋'), np.str_('莱'), np.str_('径'), np.str_('隧'), np.str_('映'), np.str_('败'), np.str_('答'), np.str_('找'), np.str_('怀'), np.str_('墨'), np.str_('堡'), np.str_('隆'), np.str_('著'), np.str_('杨'), np.str_('患'), np.str_('岐'), np.str_('钟'), np.str_('艘'), np.str_('煤'), np.str_('招'), np.str_('抢'), np.str_('你'), np.str_('筹'), np.str_('玉'), np.str_('柬'), np.str_('摄'), np.str_('幸'), np.str_('曲'), np.str_('付'), np.str_('麦'), np.str_('焦'), np.str_('封'), np.str_('舟'), np.str_('倒'), np.str_('符'), np.str_('梁'), np.str_('圣'), np.str_('免'), np.str_('侵'), np.str_('韦'), np.str_('释'), np.str_('枚'), np.str_('挑'), np.str_('厂'), np.str_('顿'), np.str_('拍'), np.str_('宜'), np.str_('沟'), np.str_('圳'), np.str_('佳'), np.str_('阵'), np.str_('杯'), np.str_('悉'), np.str_('厚'), np.str_('驱'), np.str_('细'), np.str_('宗'), np.str_('厦'), np.str_('疾'), np.str_('渔'), np.str_('泛'), np.str_('绩'), np.str_('捷'), np.str_('赏'), np.str_('缩'), np.str_('勇'), np.str_('迁'), np.str_('蒂'), np.str_('洁'), np.str_('携'), np.str_('驶'), np.str_('珠'), np.str_('暖'), np.str_('嘉'), np.str_('尊'), np.str_('鼓'), np.str_('晓'), np.str_('顾'), np.str_('沃'), np.str_('朋'), np.str_('沪'), np.str_('母'), np.str_('揭'), np.str_('恢'), np.str_('励'), np.str_('缓'), np.str_('浓'), np.str_('旦'), np.str_('兹'), np.str_('亩'), np.str_('毕'), np.str_('轻'), np.str_('喜'), np.str_('吁'), np.str_('刑'), np.str_('誉'), np.str_('燃'), np.str_('杂'), np.str_('皮'), np.str_('疫'), np.str_('背'), np.str_('穿'), np.str_('握'), np.str_('腊'), np.str_('休'), np.str_('荷'), np.str_('胞'), np.str_('析'), np.str_('呈'), np.str_('桑'), np.str_('霍'), np.str_('搭'), np.str_('孩'), np.str_('绝'), np.str_('朴'), np.str_('挝'), np.str_('戏'), np.str_('郑'), np.str_('峡'), np.str_('味'), np.str_('渡'), np.str_('沈'), np.str_('卷'), np.str_('岗'), np.str_('贷'), np.str_('骗'), np.str_('颗'), np.str_('颇'), np.str_('词'), np.str_('灯'), np.str_('滨'), np.str_('暨'), np.str_('艇'), np.str_('废'), np.str_('赵'), np.str_('腐'), np.str_('罕'), np.str_('拔'), np.str_('屋'), np.str_('露'), np.str_('矿'), np.str_('牧'), np.str_('寒'), np.str_('顶'), np.str_('脑'), np.str_('汛'), np.str_('套'), np.str_('喀'), np.str_('兼'), np.str_('驾'), np.str_('逮'), np.str_('炮'), np.str_('佩'), np.str_('仲'), np.str_('酒'), np.str_('谢'), np.str_('彰'), np.str_('乱'), np.str_('陪'), np.str_('挚'), np.str_('择'), np.str_('埔'), np.str_('踪'), np.str_('跃'), np.str_('珍'), np.str_('尚'), np.str_('丁'), np.str_('跑'), np.str_('泉'), np.str_('晋'), np.str_('佛'), np.str_('胁'), np.str_('缺'), np.str_('诊'), np.str_('却'), np.str_('冀'), np.str_('逃'), np.str_('纲'), np.str_('伍'), np.str_('拓'), np.str_('忘'), np.str_('坐'), np.str_('透'), np.str_('搬'), np.str_('恶'), np.str_('苗'), np.str_('够'), np.str_('辉'), np.str_('翻'), np.str_('盘'), np.str_('甸'), np.str_('扫'), np.str_('彭'), np.str_('契'), np.str_('俗'), np.str_('迅'), np.str_('泽'), np.str_('昆'), np.str_('忙'), np.str_('壮'), np.str_('递'), np.str_('弱'), np.str_('籍'), np.str_('材'), np.str_('喷'), np.str_('硕'), np.str_('泊'), np.str_('沉'), np.str_('买'), np.str_('避'), np.str_('槿'), np.str_('档'), np.str_('颁'), np.str_('且'), np.str_('迫'), np.str_('烟'), np.str_('既'), np.str_('异'), np.str_('叶'), np.str_('遍'), np.str_('松'), np.str_('挂'), np.str_('夕'), np.str_('厄'), np.str_('撞'), np.str_('宽'), np.str_('宇'), np.str_('涵'), np.str_('末'), np.str_('履'), np.str_('卜'), np.str_('侨'), np.str_('阐'), np.str_('锦'), np.str_('锋'), np.str_('浪'), np.str_('廷'), np.str_('借'), np.str_('钱'), np.str_('莎'), np.str_('孙'), np.str_('简'), np.str_('岩'), np.str_('坏'), np.str_('怎'), np.str_('巩'), np.str_('辛'), np.str_('缴'), np.str_('疏'), np.str_('玛'), np.str_('毛'), np.str_('慧'), np.str_('遵'), np.str_('誓'), np.str_('彼'), np.str_('塌'), np.str_('趋'), np.str_('血'), np.str_('绸'), np.str_('鸟'), np.str_('虽'), np.str_('毫'), np.str_('拜'), np.str_('诈'), np.str_('郊'), np.str_('若'), np.str_('芬'), np.str_('硬'), np.str_('舆'), np.str_('灭'), np.str_('朱'), np.str_('戈'), np.str_('耕'), np.str_('紫'), np.str_('谷'), np.str_('码'), np.str_('洞'), np.str_('氏'), np.str_('奉'), np.str_('湿'), np.str_('忠'), np.str_('坝'), np.str_('债'), np.str_('侦'), np.str_('谴'), np.str_('瓜'), np.str_('旧'), np.str_('剂'), np.str_('凝'), np.str_('跟'), np.str_('苦'), np.str_('块'), np.str_('艾'), np.str_('租'), np.str_('漠'), np.str_('渐'), np.str_('弘'), np.str_('钓'), np.str_('润'), np.str_('撑'), np.str_('宏'), np.str_('吹'), np.str_('叫'), np.str_('刊'), np.str_('伏'), np.str_('鸣'), np.str_('餐'), np.str_('估'), np.str_('觉'), np.str_('撒'), np.str_('徐'), np.str_('坠'), np.str_('诞'), np.str_('虎'), np.str_('差'), np.str_('唯'), np.str_('卖'), np.str_('偏'), np.str_('隔'), np.str_('阻'), np.str_('葡'), np.str_('敬'), np.str_('媛'), np.str_('册'), np.str_('艰'), np.str_('罚'), np.str_('潘'), np.str_('榜'), np.str_('旬'), np.str_('帷'), np.str_('谱'), np.str_('臣'), np.str_('秒'), np.str_('孟'), np.str_('唱'), np.str_('赶'), np.str_('贾'), np.str_('似'), np.str_('鳌'), np.str_('翼'), np.str_('稿'), np.str_('操'), np.str_('倾'), np.str_('井'), np.str_('轰'), np.str_('衡'), np.str_('箱'), np.str_('勤'), np.str_('雾'), np.str_('诗'), np.str_('役'), np.str_('弃'), np.str_('帕'), np.str_('奔'), np.str_('奏'), np.str_('壤'), np.str_('掌'), np.str_('唐'), np.str_('哪'), np.str_('哀'), np.str_('链'), np.str_('软'), np.str_('询'), np.str_('盾'), np.str_('摘'), np.str_('隐'), np.str_('栋'), np.str_('斤'), np.str_('仗'), np.str_('鸡'), np.str_('蓬'), np.str_('纵'), np.str_('磋'), np.str_('厘'), np.str_('厉'), np.str_('龄'), np.str_('菜'), np.str_('氛'), np.str_('殊'), np.str_('悼'), np.str_('仁'), np.str_('酋'), np.str_('祥'), np.str_('戒'), np.str_('岭'), np.str_('勋'), np.str_('凡'), np.str_('铺'), np.str_('貌'), np.str_('稻'), np.str_('柏'), np.str_('劲'), np.str_('镜'), np.str_('茨'), np.str_('芝'), np.str_('池'), np.str_('劾'), np.str_('贿'), np.str_('舱'), np.str_('煌'), np.str_('涝'), np.str_('旋'), np.str_('宅'), np.str_('凯'), np.str_('鹰'), np.str_('猴'), np.str_('栏'), np.str_('旺'), np.str_('掉'), np.str_('拟'), np.str_('寅'), np.str_('铜'), np.str_('谓'), np.str_('竹'), np.str_('甲'), np.str_('父'), np.str_('横'), np.str_('吃'), np.str_('畅'), np.str_('牵'), np.str_('塑'), np.str_('误'), np.str_('添'), np.str_('昂'), np.str_('廊'), np.str_('吴'), np.str_('匈'), np.str_('邮'), np.str_('萄'), np.str_('绘'), np.str_('泳'), np.str_('嫩'), np.str_('含'), np.str_('踏'), np.str_('脉'), np.str_('淮'), np.str_('贴'), np.str_('幼'), np.str_('址'), np.str_('翰'), np.str_('弟'), np.str_('卿'), np.str_('冈'), np.str_('陵'), np.str_('袖'), np.str_('殖'), np.str_('械'), np.str_('扣'), np.str_('慈'), np.str_('屏'), np.str_('圈'), np.str_('冻'), np.str_('偷'), np.str_('骨'), np.str_('锁'), np.str_('醒'), np.str_('烧'), np.str_('渠'), np.str_('浩'), np.str_('框'), np.str_('挪'), np.str_('尤'), np.str_('偿'), np.str_('郭'), np.str_('茶'), np.str_('脚'), np.str_('泥'), np.str_('呢'), np.str_('饮'), np.str_('赤'), np.str_('贩'), np.str_('触'), np.str_('虫'), np.str_('虚'), np.str_('砺'), np.str_('炭'), np.str_('漫'), np.str_('混'), np.str_('弗'), np.str_('俊'), np.str_('阮'), np.str_('逝'), np.str_('荒'), np.str_('琴'), np.str_('折'), np.str_('崔'), np.str_('尖'), np.str_('宴'), np.str_('乎'), np.str_('跳'), np.str_('拼'), np.str_('劣'), np.str_('剑'), np.str_('陷'), np.str_('阔'), np.str_('错'), np.str_('遥'), np.str_('辖'), np.str_('摆'), np.str_('尾'), np.str_('剩'), np.str_('馈'), np.str_('鉴'), np.str_('砥'), np.str_('牛'), np.str_('棚'), np.str_('敌'), np.str_('拦'), np.str_('抽'), np.str_('恰'), np.str_('盐'), np.str_('猎'), np.str_('浮'), np.str_('戴'), np.str_('辩'), np.str_('赣'), np.str_('裂'), np.str_('溪'), np.str_('泄'), np.str_('曝'), np.str_('垃'), np.str_('净'), np.str_('腾'), np.str_('笔'), np.str_('灵'), np.str_('滞'), np.str_('毅'), np.str_('敏'), np.str_('掘'), np.str_('寺'), np.str_('伐'), np.str_('飓'), np.str_('迟'), np.str_('跌'), np.str_('豹'), np.str_('毙'), np.str_('暑'), np.str_('旱'), np.str_('押'), np.str_('圾'), np.str_('逆'), np.str_('谨'), np.str_('窗'), np.str_('液'), np.str_('欣'), np.str_('楚'), np.str_('栖'), np.str_('拆'), np.str_('惩'), np.str_('循'), np.str_('崇'), np.str_('孔'), np.str_('匹'), np.str_('券'), np.str_('辰'), np.str_('般'), np.str_('肥'), np.str_('秩'), np.str_('症'), np.str_('拒'), np.str_('宋'), np.str_('墙'), np.str_('胎'), np.str_('羊'), np.str_('缔'), np.str_('洽'), np.str_('歼'), np.str_('捐'), np.str_('坊'), np.str_('勃'), np.str_('劫'), np.str_('乔'), np.str_('替'), np.str_('扰'), np.str_('尘'), np.str_('篪'), np.str_('秉'), np.str_('祭'), np.str_('敢'), np.str_('御'), np.str_('凭'), np.str_('凤'), np.str_('饭'), np.str_('赖'), np.str_('蛋'), np.str_('盗'), np.str_('玻'), np.str_('旁'), np.str_('扑'), np.str_('刷'), np.str_('佐'), np.str_('顷'), np.str_('静'), np.str_('闹'), np.str_('遣'), np.str_('辟'), np.str_('辈'), np.str_('狂'), np.str_('浦'), np.str_('楷'), np.str_('摧'), np.str_('惊'), np.str_('峻'), np.str_('奠'), np.str_('仰'), np.str_('迷'), np.str_('译'), np.str_('蔓'), np.str_('肉'), np.str_('绳'), np.str_('纸'), np.str_('慢'), np.str_('庙'), np.str_('吕'), np.str_('吊'), np.str_('勘'), np.str_('仓'), np.str_('乒'), np.str_('辅'), np.str_('赋'), np.str_('谐'), np.str_('秦'), np.str_('皇'), np.str_('描'), np.str_('忧'), np.str_('孝'), np.str_('函'), np.str_('丧'), np.str_('碍'), np.str_('矛'), np.str_('熟'), np.str_('涌'), np.str_('柱'), np.str_('徒'), np.str_('床'), np.str_('趟'), np.str_('诸'), np.str_('篮'), np.str_('甚'), np.str_('歧'), np.str_('棋'), np.str_('曹'), np.str_('廖'), np.str_('寄'), np.str_('堵'), np.str_('鄂'), np.str_('捞'), np.str_('嘱'), np.str_('刀'), np.str_('鹤'), np.str_('锡'), np.str_('贤'), np.str_('谁'), np.str_('讼'), np.str_('董'), np.str_('湘'), np.str_('揽'), np.str_('慕'), np.str_('兄'), np.str_('允'), np.str_('鲸'), np.str_('雹'), np.str_('滩'), np.str_('梯'), np.str_('屠'), np.str_('墓'), np.str_('辑'), np.str_('虑'), np.str_('舶'), np.str_('耗'), np.str_('纠'), np.str_('牲'), np.str_('歇'), np.str_('橙'), np.str_('挖'), np.str_('廉'), np.str_('娃'), np.str_('奶'), np.str_('堆'), np.str_('卓'), np.str_('凉'), np.str_('赠'), np.str_('磁'), np.str_('碑'), np.str_('灰'), np.str_('帜'), np.str_('娥'), np.str_('妈'), np.str_('塘'), np.str_('刺'), np.str_('聘'), np.str_('粤'), np.str_('穷'), np.str_('癌'), np.str_('瀑'), np.str_('澜'), np.str_('淹'), np.str_('弥'), np.str_('娜'), np.str_('麻'), np.str_('闲'), np.str_('逢'), np.str_('豪'), np.str_('礁'), np.str_('盼'), np.str_('奈'), np.str_('壁'), np.str_('堤'), np.str_('伸'), np.str_('钦'), np.str_('绵'), np.str_('氧'), np.str_('株'), np.str_('悬'), np.str_('堪'), np.str_('傍'), np.str_('乙'), np.str_('魅'), np.str_('逻'), np.str_('账'), np.str_('裕'), np.str_('荐'), np.str_('牺'), np.str_('炉'), np.str_('潭'), np.str_('妥'), np.str_('匠'), np.str_('冒'), np.str_('舍'), np.str_('禽'), np.str_('瞩'), np.str_('痛'), np.str_('猛'), np.str_('炳'), np.str_('桃'), np.str_('擦'), np.str_('拳'), np.str_('拱'), np.str_('拨'), np.str_('披'), np.str_('宵'), np.str_('孜'), np.str_('妻'), np.str_('埋'), np.str_('叠'), np.str_('乃'), np.str_('阜'), np.str_('沂'), np.str_('暗'), np.str_('攀'), np.str_('拖'), np.str_('巷'), np.str_('乳'), np.str_('黔'), np.str_('遂'), np.str_('虹'), np.str_('葬'), np.str_('睦'), np.str_('猪'), np.str_('犹'), np.str_('炼'), np.str_('灌'), np.str_('滚'), np.str_('湄'), np.str_('桂'), np.str_('柳'), np.str_('杆'), np.str_('挺'), np.str_('懈'), np.str_('填'), np.str_('哲'), np.str_('驳'), np.str_('逼'), np.str_('肩'), np.str_('粉'), np.str_('碰'), np.str_('盆'), np.str_('桌'), np.str_('枢'), np.str_('拘'), np.str_('巧'), np.str_('夷'), np.str_('凶'), np.str_('伪'), np.str_('丑'), np.str_('鹅'), np.str_('页'), np.str_('陶'), np.str_('闯'), np.str_('铸'), np.str_('鄱'), np.str_('耀'), np.str_('缘'), np.str_('窝'), np.str_('稀'), np.str_('番'), np.str_('渝'), np.str_('涅'), np.str_('恒'), np.str_('屿'), np.str_('嘎'), np.str_('吐'), np.str_('骑'), np.str_('阴'), np.str_('锐'), np.str_('郡'), np.str_('遏'), np.str_('贪'), np.str_('豆'), np.str_('诠'), np.str_('胆'), np.str_('祉'), np.str_('碳'), np.str_('碌'), np.str_('玩'), np.str_('湛'), np.str_('洗'), np.str_('搞'), np.str_('懂'), np.str_('忆'), np.str_('彝'), np.str_('咨'), np.str_('厢'), np.str_('乓'), np.str_('骸'), np.str_('辨'), np.str_('赔'), np.str_('赁'), np.str_('袋'), np.str_('脸'), np.str_('绪'), np.str_('滕'), np.str_('汤'), np.str_('晶'), np.str_('晴'), np.str_('坎'), np.str_('唁'), np.str_('吾'), np.str_('乏'), np.str_('鹏'), np.str_('铭'), np.str_('辐'), np.str_('蹈'), np.str_('衣'), np.str_('薛'), np.str_('蔬'), np.str_('芯'), np.str_('纯'), np.str_('粒'), np.str_('焕'), np.str_('抚'), np.str_('宿'), np.str_('姜'), np.str_('壶'), np.str_('魂'), np.str_('衔'), np.str_('荡'), np.str_('茂'), np.str_('脏'), np.str_('肆'), np.str_('聊'), np.str_('渊'), np.str_('掩'), np.str_('婚'), np.str_('句'), np.str_('俱'), np.str_('骚'), np.str_('迄'), np.str_('虐'), np.str_('脊'), np.str_('肖'), np.str_('狱'), np.str_('漏'), np.str_('淡'), np.str_('棉'), np.str_('杉'), np.str_('晰'), np.str_('擎'), np.str_('挫'), np.str_('愁'), np.str_('寿'), np.str_('堰'), np.str_('侯'), np.str_('铝'), np.str_('钻'), np.str_('赃'), np.str_('详'), np.str_('莞'), np.str_('瞬'), np.str_('牡'), np.str_('炎'), np.str_('摇'), np.str_('搏'), np.str_('抬'), np.str_('慎'), np.str_('悠'), np.str_('鹿'), np.str_('瞄'), np.str_('斌'), np.str_('捣'), np.str_('庞'), np.str_('帝'), np.str_('厕'), np.str_('削'), np.str_('赌'), np.str_('莲'), np.str_('腹'), np.str_('胸'), np.str_('绎'), np.str_('笼'), np.str_('琼'), np.str_('爬'), np.str_('濒'), np.str_('浆'), np.str_('殷'), np.str_('幻'), np.str_('尝'), np.str_('圭'), np.str_('冯'), np.str_('骤'), np.str_('饱'), np.str_('闪'), np.str_('轩'), np.str_('蔡'), np.str_('苹'), np.str_('芳'), np.str_('窃'), np.str_('睡'), np.str_('熔'), np.str_('滥'), np.str_('涛'), np.str_('沧'), np.str_('汶'), np.str_('歉'), np.str_('欠'), np.str_('弄'), np.str_('崩'), np.str_('孤'), np.str_('啸'), np.str_('吞'), np.str_('仿'), np.str_('仔'), np.str_('顽'), np.str_('薄'), np.str_('蓄'), np.str_('缝'), np.str_('窄'), np.str_('秽'), np.str_('碧'), np.str_('矩'), np.str_('瓶'), np.str_('炬'), np.str_('滋'), np.str_('溢'), np.str_('淫'), np.str_('抱'), np.str_('扮'), np.str_('徙'), np.str_('帅'), np.str_('婴'), np.str_('夹'), np.str_('夯'), np.str_('垒'), np.str_('坑'), np.str_('募'), np.str_('兑'), np.str_('鲍'), np.str_('饶'), np.str_('饰'), np.str_('雁'), np.str_('锻'), np.str_('郁'), np.str_('诵'), np.str_('裔'), np.str_('衰'), np.str_('腿'), np.str_('缉'), np.str_('粹'), np.str_('磨'), np.str_('畔'), np.str_('晒'), np.str_('斜'), np.str_('捍'), np.str_('愈'), np.str_('岳'), np.str_('嫦'), np.str_('姚'), np.str_('唤'), np.str_('雕'), np.str_('镕'), np.str_('萍'), np.str_('舒'), np.str_('筛'), np.str_('矶'), np.str_('燕'), np.str_('浅'), np.str_('毯'), np.str_('概'), np.str_('昭'), np.str_('掀'), np.str_('怒'), np.str_('弊'), np.str_('帽'), np.str_('姿'), np.str_('坍'), np.str_('串'), np.str_('颜'), np.str_('邓'), np.str_('赂'), np.str_('谣'), np.str_('莉'), np.str_('艳'), np.str_('罢'), np.str_('祁'), np.str_('瞰'), np.str_('潍'), np.str_('涯'), np.str_('柔'), np.str_('杠'), np.str_('挡'), np.str_('惜'), np.str_('孕'), np.str_('垂'), np.str_('君'), np.str_('卸'), np.str_('匿'), np.str_('匣'), np.str_('勾'), np.str_('勉'), np.str_('剿'), np.str_('割'), np.str_('丘'), np.str_('飘'), np.str_('颖'), np.str_('颈'), np.str_('靓'), np.str_('闽'), np.str_('裹'), np.str_('袁'), np.str_('芦'), np.str_('舌'), np.str_('翔'), np.str_('罐'), np.str_('纤'), np.str_('糖'), np.str_('璃'), np.str_('漳'), np.str_('滴'), np.str_('楠'), np.str_('梳'), np.str_('桶'), np.str_('昼'), np.str_('斡'), np.str_('斐'), np.str_('摸'), np.str_('帆'), np.str_('岘'), np.str_('宙'), np.str_('孵'), np.str_('奎'), np.str_('垮'), np.str_('坪'), np.str_('催'), np.str_('亭'), np.str_('靖'), np.str_('辜'), np.str_('趣'), np.str_('豚'), np.str_('衷'), np.str_('薪'), np.str_('莹'), np.str_('羽'), np.str_('畜'), np.str_('燥'), np.str_('淘'), np.str_('殿'), np.str_('棒'), np.str_('柴'), np.str_('插'), np.str_('屈'), np.str_('尺'), np.str_('丛'), np.str_('鼻'), np.str_('鸭'), np.str_('锅'), np.str_('藤'), np.str_('腔'), np.str_('肝'), np.str_('绽'), np.str_('笑'), np.str_('瞻'), np.str_('瞒'), np.str_('狠'), np.str_('渣'), np.str_('汰'), np.str_('欺'), np.str_('椅'), np.str_('斑'), np.str_('悦'), np.str_('寓'), np.str_('宛'), np.str_('娅'), np.str_('姐'), np.str_('哨'), np.str_('呦'), np.str_('僚'), np.str_('丙'), np.str_('霸'), np.str_('隶'), np.str_('陇'), np.str_('闸'), np.str_('酷'), np.str_('茫'), np.str_('苍'), np.str_('芜'), np.str_('腰'), np.str_('胶'), np.str_('肺'), np.str_('绑'), np.str_('纬'), np.str_('砂'), np.str_('盯'), np.str_('琳'), np.str_('爸'), np.str_('烂'), np.str_('滇'), np.str_('淄'), np.str_('榻'), np.str_('梨'), np.str_('柯'), np.str_('枝'), np.str_('坤'), np.str_('吧'), np.str_('卧'), np.str_('傅'), np.str_('鸽'), np.str_('雌'), np.str_('郎'), np.str_('遮'), np.str_('轴'), np.str_('菏'), np.str_('舷'), np.str_('羚'), np.str_('璨'), np.str_('璀'), np.str_('棵'), np.str_('敲'), np.str_('撰'), np.str_('捧'), np.str_('惯'), np.str_('妮'), np.str_('囊'), np.str_('嘴'), np.str_('啃'), np.str_('咱'), np.str_('凸'), np.str_('俭'), np.str_('亏'), np.str_('饼'), np.str_('逛'), np.str_('衍'), np.str_('蔚'), np.str_('蓉'), np.str_('菌'), np.str_('绣'), np.str_('纹'), np.str_('篡'), np.str_('祈'), np.str_('盲'), np.str_('瘫'), np.str_('畏'), np.str_('玲'), np.str_('牟'), np.str_('烦'), np.str_('渤'), np.str_('桨'), np.str_('拐'), np.str_('忽'), np.str_('弯'), np.str_('崖'), np.str_('寸'), np.str_('壳'), np.str_('垄'), np.str_('伞'), np.str_('麓'), np.str_('魁'), np.str_('饲'), np.str_('镑'), np.str_('辱'), np.str_('踞'), np.str_('谍'), np.str_('蕴'), np.str_('臂'), np.str_('耐'), np.str_('缆'), np.str_('纺'), np.str_('筝'), np.str_('穴'), np.str_('碎'), np.str_('硫'), np.str_('盎'), np.str_('珊'), np.str_('犁'), np.str_('焊'), np.str_('烛'), np.str_('渗'), np.str_('淀'), np.str_('洒'), np.str_('桐'), np.str_('憾'), np.str_('悟'), np.str_('怕'), np.str_('屡'), np.str_('墟'), np.str_('喝'), np.str_('啤'), np.str_('卵'), np.str_('俾'), np.str_('龟'), np.str_('鼠'), np.str_('鲟'), np.str_('骆'), np.str_('颂'), np.str_('雯'), np.str_('陛'), np.str_('豫'), np.str_('衅'), np.str_('蛟'), np.str_('臭'), np.str_('胀'), np.str_('绚'), np.str_('祸'), np.str_('狮'), np.str_('炒'), np.str_('溶'), np.str_('溜'), np.str_('浴'), np.str_('杏'), np.str_('挽'), np.str_('扭'), np.str_('戚'), np.str_('惨'), np.str_('彬'), np.str_('帐'), np.str_('巢'), np.str_('崭'), np.str_('屯'), np.str_('尸'), np.str_('冕'), np.str_('亥'), np.str_('魏'), np.str_('驰'), np.str_('韧'), np.str_('霾'), np.str_('锤'), np.str_('钉'), np.str_('酬'), np.str_('遴'), np.str_('逊'), np.str_('蹲'), np.str_('蝉'), np.str_('蚊'), np.str_('蒋'), np.str_('芒'), np.str_('脐'), np.str_('肇'), np.str_('聆'), np.str_('篷'), np.str_('竺'), np.str_('竭'), np.str_('眠'), np.str_('瘤'), np.str_('狭'), np.str_('爽'), np.str_('洼'), np.str_('槛'), np.str_('旭'), np.str_('搁'), np.str_('悔'), np.str_('弦'), np.str_('庐'), np.str_('崽'), np.str_('娱'), np.str_('妹'), np.str_('剥'), np.str_('儒'), np.str_('傲'), np.str_('仙'), np.str_('亨'), np.str_('齿'), np.str_('鹭'), np.str_('鹃'), np.str_('鸿'), np.str_('骄'), np.str_('鞭'), np.str_('霞'), np.str_('陀'), np.str_('闷'), np.str_('踩'), np.str_('诱'), np.str_('衢'), np.str_('蝶'), np.str_('蕾'), np.str_('萌'), np.str_('茅'), np.str_('苑'), np.str_('肿'), np.str_('翁'), np.str_('竟'), np.str_('禄'), np.str_('睹'), np.str_('眷'), np.str_('甜'), np.str_('琪'), np.str_('漂'), np.str_('溉'), np.str_('溃'), np.str_('洱'), np.str_('泡'), np.str_('氢'), np.str_('欲'), np.str_('榴'), np.str_('椒'), np.str_('栽'), np.str_('朵'), np.str_('挨'), np.str_('惕'), np.str_('娄'), np.str_('妙'), np.str_('墉'), np.str_('垦'), np.str_('囚'), np.str_('喊'), np.str_('劝'), np.str_('凰'), np.str_('凇'), np.str_('冶'), np.str_('丈'), np.str_('雇'), np.str_('钳'), np.str_('轿'), np.str_('趁'), np.str_('赦'), np.str_('谭'), np.str_('觅'), np.str_('蜂'), np.str_('蛇'), np.str_('蚌'), np.str_('葫'), np.str_('萧'), np.str_('舵'), np.str_('脆'), np.str_('缠'), np.str_('粗'), np.str_('眉'), np.str_('皖'), np.str_('焚'), np.str_('澄'), np.str_('泼'), np.str_('汕'), np.str_('氟'), np.str_('殉'), np.str_('梭'), np.str_('枯'), np.str_('摊'), np.str_('挠'), np.str_('拾'), np.str_('慑'), np.str_('惧'), np.str_('恪'), np.str_('彦'), np.str_('幌'), np.str_('崛'), np.str_('崎'), np.str_('岷'), np.str_('娘'), np.str_('妄'), np.str_('垣'), np.str_('咬'), np.str_('吗'), np.str_('叹'), np.str_('叮'), np.str_('叉'), np.str_('兽'), np.str_('兆'), np.str_('俯'), np.str_('俘'), np.str_('馨'), np.str_('鞍'), np.str_('锣'), np.str_('铀'), np.str_('钩'), np.str_('钥'), np.str_('酵'), np.str_('郸'), np.str_('邯'), np.str_('跆'), np.str_('詹'), np.str_('蜜'), np.str_('葱'), np.str_('菊'), np.str_('芮'), np.str_('腈'), np.str_('聪'), np.str_('羌'), np.str_('粽'), np.str_('窟'), np.str_('磅'), np.str_('碱'), np.str_('疲'), np.str_('瑚'), np.str_('猩'), np.str_('爷'), np.str_('涂'), np.str_('浏'), np.str_('泸'), np.str_('汝'), np.str_('橘'), np.str_('樟'), np.str_('桩'), np.str_('桁'), np.str_('柜'), np.str_('朽'), np.str_('晖'), np.str_('昔'), np.str_('擅'), np.str_('摔'), np.str_('掠'), np.str_('慌'), np.str_('愚'), np.str_('悲'), np.str_('忍'), np.str_('奴'), np.str_('喻'), np.str_('哺'), np.str_('冉'), np.str_('僵'), np.str_('亟'), np.str_('乍'), np.str_('丢'), np.str_('鹦'), np.str_('魔'), np.str_('饥'), np.str_('飚'), np.str_('颠'), np.str_('靶'), np.str_('雀'), np.str_('隙'), np.str_('陨'), np.str_('阪'), np.str_('钞'), np.str_('醇'), np.str_('酿'), np.str_('酸'), np.str_('邸'), np.str_('邵'), np.str_('逾'), np.str_('踊'), np.str_('跻'), np.str_('蚀'), np.str_('蔽'), np.str_('蒸'), np.str_('葆'), np.str_('荆'), np.str_('肢'), np.str_('罩'), np.str_('糊'), np.str_('竣'), np.str_('秧'), np.str_('睛'), np.str_('盈'), np.str_('痪'), np.str_('狐'), np.str_('犬'), np.str_('焰'), np.str_('烤'), np.str_('灿'), np.str_('潼'), np.str_('漓'), np.str_('溯'), np.str_('涪'), np.str_('浜'), np.str_('浇'), np.str_('汾'), np.str_('汀'), np.str_('毗'), np.str_('樊'), np.str_('札'), np.str_('朔'), np.str_('斥'), np.str_('撬'), np.str_('掷'), np.str_('抛'), np.str_('悄'), np.str_('尹'), np.str_('嫁'), np.str_('姑'), np.str_('妨'), np.str_('奸'), np.str_('圩'), np.str_('喉'), np.str_('咏'), np.str_('卒'), np.str_('卉'), np.str_('匆'), np.str_('剖'), np.str_('俩'), np.str_('侮'), np.str_('仑'), np.str_('鹳'), np.str_('饷'), np.str_('韵'), np.str_('锂'), np.str_('铿'), np.str_('铅'), np.str_('钧'), np.str_('醉'), np.str_('邹'), np.str_('邱'), np.str_('邢'), np.str_('逸'), np.str_('躲'), np.str_('蹦'), np.str_('贞'), np.str_('豁'), np.str_('菱'), np.str_('菅'), np.str_('荔'), np.str_('脂'), np.str_('耿'), np.str_('耍'), np.str_('翠'), np.str_('筒'), np.str_('笋'), np.str_('秸'), np.str_('秆'), np.str_('禅'), np.str_('祀'), np.str_('礴'), np.str_('碗'), np.str_('矮'), np.str_('痕'), np.str_('瓷'), np.str_('猜'), np.str_('狼'), np.str_('熠'), np.str_('烽'), np.str_('烯'), np.str_('淖'), np.str_('淑'), np.str_('泪'), np.str_('沥'), np.str_('汲'), np.str_('橡'), np.str_('樱'), np.str_('榆'), np.str_('棱'), np.str_('棠'), np.str_('柩'), np.str_('昏'), np.str_('撕'), np.str_('挤'), np.str_('抑'), np.str_('扇'), np.str_('戎'), np.str_('愧'), np.str_('惑'), np.str_('巅'), np.str_('尧'), np.str_('奢'), np.str_('埠'), np.str_('啦'), np.str_('唇'), np.str_('哇'), np.str_('咸'), np.str_('叛'), np.str_('厨'), np.str_('刮'), np.str_('僧'), np.str_('偶'), np.str_('侗'), np.str_('佑'), np.str_('乾'), np.str_('龚'), np.str_('鼎'), np.str_('鹊'), np.str_('驿'), np.str_('隽'), np.str_('隅'), np.str_('陌'), np.str_('阎'), np.str_('邑'), np.str_('辣'), np.str_('蹄'), np.str_('贬'), np.str_('谦'), np.str_('谜'), np.str_('襄'), np.str_('裸'), np.str_('袂'), np.str_('蟒'), np.str_('蒲'), np.str_('菇'), np.str_('芽'), np.str_('芙'), np.str_('膜'), np.str_('肾'), np.str_('罹'), np.str_('绥'), np.str_('纱'), np.str_('糕'), np.str_('穹'), np.str_('禹'), np.str_('磊'), np.str_('矢'), np.str_('盔'), np.str_('瑙'), np.str_('猕'), np.str_('狙'), np.str_('犀'), np.str_('爵'), np.str_('熄'), np.str_('瀛'), np.str_('漪'), np.str_('渭'), np.str_('淳'), np.str_('涡'), np.str_('浸'), np.str_('泗'), np.str_('沫'), np.str_('歪'), np.str_('梵'), np.str_('栈'), np.str_('柑'), np.str_('曙'), np.str_('撸'), np.str_('捏'), np.str_('抹'), np.str_('扛'), np.str_('扔'), np.str_('戍'), np.str_('愤'), np.str_('恼'), np.str_('弓'), np.str_('峪'), np.str_('峙'), np.str_('岱'), np.str_('岚'), np.str_('尿'), np.str_('婺'), np.str_('婆'), np.str_('娇'), np.str_('妃'), np.str_('墅'), np.str_('坞'), np.str_('喆'), np.str_('叨'), np.str_('勿'), np.str_('删'), np.str_('侠'), np.str_('伽'), np.str_('仕'), np.str_('黛'), np.str_('鹮'), np.str_('鸥'), np.str_('骼'), np.str_('骅'), np.str_('韬'), np.str_('鞋'), np.str_('霓'), np.str_('隋'), np.str_('陲'), np.str_('陋'), np.str_('阱'), np.str_('阖'), np.str_('锵'), np.str_('锚'), np.str_('铲'), np.str_('鑫'), np.str_('郴'), np.str_('迭'), np.str_('趵'), np.str_('谎'), np.str_('衬'), np.str_('蟹'), np.str_('螺'), np.str_('蜀'), np.str_('葛'), np.str_('聋'), np.str_('翅'), np.str_('羯'), np.str_('羁'), np.str_('缮'), np.str_('纂'), np.str_('籽'), np.str_('籼'), np.str_('筋'), np.str_('竖'), np.str_('窜'), np.str_('砸'), np.str_('砍'), np.str_('盏'), np.str_('皑'), np.str_('瘦'), np.str_('疮'), np.str_('甫'), np.str_('璧'), np.str_('瑰'), np.str_('獭'), np.str_('狗'), np.str_('烷'), np.str_('濮'), np.str_('渚'), np.str_('淤'), np.str_('洄'), np.str_('沁'), np.str_('氮'), np.str_('殴'), np.str_('槽'), np.str_('枣'), np.str_('晟'), np.str_('斩'), np.str_('攒'), np.str_('掖'), np.str_('掏'), np.str_('捉'), np.str_('挣'), np.str_('拢'), np.str_('抉'), np.str_('扳'), np.str_('庸'), np.str_('庇'), np.str_('巾'), np.str_('巍'), np.str_('嬉'), np.str_('喧'), np.str_('啥'), np.str_('吓'), np.str_('匙'), np.str_('勐'), np.str_('凿'), np.str_('冼'), np.str_('冥'), np.str_('兢'), np.str_('僻'), np.str_('俺'), np.str_('丸'), np.str_('鹬'), np.str_('鹉'), np.str_('鲇'), np.str_('髓'), np.str_('驼'), np.str_('驴'), np.str_('饿'), np.str_('鞠'), np.str_('霜'), np.str_('闫'), np.str_('铃'), np.str_('钊'), np.str_('釜'), np.str_('酝'), np.str_('酉'), np.str_('郝'), np.str_('逞'), np.str_('迸'), np.str_('躯'), np.str_('跋'), np.str_('谌'), np.str_('谅'), np.str_('讴'), np.str_('虾'), np.str_('蒿'), np.str_('蒜'), np.str_('葵'), np.str_('萎'), np.str_('莓'), np.str_('茵'), np.str_('茬'), np.str_('芭'), np.str_('膏'), np.str_('胚'), np.str_('肠'), np.str_('聂'), np.str_('耘'), np.str_('羲'), np.str_('缤'), np.str_('绒'), np.str_('簿'), np.str_('秤'), np.str_('祠'), np.str_('睿'), np.str_('盒'), np.str_('疯'), np.str_('畲'), np.str_('町'), np.str_('瑟'), np.str_('琛'), np.str_('珲'), np.str_('珀'), np.str_('猿'), np.str_('煽'), np.str_('烫'), np.str_('炯'), np.str_('瀚'), np.str_('漯'), np.str_('滦'), np.str_('渺'), np.str_('渴'), np.str_('渥'), np.str_('渎'), np.str_('淌'), np.str_('浑'), np.str_('洙'), np.str_('泻'), np.str_('沭'), np.str_('檬'), np.str_('椎'), np.str_('柿'), np.str_('柠'), np.str_('枭'), np.str_('晾'), np.str_('晕'), np.str_('擘'), np.str_('捆'), np.str_('挟'), np.str_('拧'), np.str_('抒'), np.str_('慷'), np.str_('愉'), np.str_('恨'), np.str_('怡'), np.str_('幽'), np.str_('媚'), np.str_('娶'), np.str_('娟'), np.str_('姣'), np.str_('妍'), np.str_('妆'), np.str_('夸'), np.str_('墩'), np.str_('坯'), np.str_('喇'), np.str_('啡'), np.str_('啊'), np.str_('哄'), np.str_('咖'), np.str_('呆'), np.str_('叔'), np.str_('劼'), np.str_('剪'), np.str_('剔'), np.str_('兜'), np.str_('仇'), np.str_('麋'), np.str_('鲷'), np.str_('魄'), np.str_('驭'), np.str_('韶'), np.str_('靡'), np.str_('霆'), np.str_('闇'), np.str_('镰'), np.str_('钙'), np.str_('邪'), np.str_('辙'), np.str_('躺'), np.str_('躬'), np.str_('跤'), np.str_('谆'), np.str_('襟'), np.str_('袜'), np.str_('蝴'), np.str_('蛮'), np.str_('蚕'), np.str_('虞'), np.str_('蕲'), np.str_('萃'), np.str_('莺'), np.str_('莘'), np.str_('荧'), np.str_('荀'), np.str_('茸'), np.str_('茜'), np.str_('茏'), np.str_('茁'), np.str_('苛'), np.str_('舛'), np.str_('腭'), np.str_('胃'), np.str_('肌'), np.str_('耻'), np.str_('翟'), np.str_('翘'), np.str_('缨'), np.str_('缚'), np.str_('缕'), np.str_('粥'), np.str_('竿'), np.str_('窑'), np.str_('稼'), np.str_('禾'), np.str_('祯'), np.str_('磷'), np.str_('磐'), np.str_('碾'), np.str_('砀'), np.str_('睐'), np.str_('皙'), np.str_('瘠'), np.str_('痍'), np.str_('甩'), np.str_('瑾'), np.str_('瑶'), np.str_('瑜'), np.str_('瑕'), np.str_('琨'), np.str_('琅'), np.str_('玄'), np.str_('炙'), np.str_('澎'), np.str_('潟'), np.str_('漆'), np.str_('溱'), np.str_('湟'), np.str_('淞'), np.str_('浊'), np.str_('歹'), np.str_('榭'), np.str_('梧'), np.str_('梗'), np.str_('栅'), np.str_('昕'), np.str_('昊'), np.str_('斋'), np.str_('抨'), np.str_('懒'), np.str_('憬'), np.str_('憧'), np.str_('怠'), np.str_('弈'), np.str_('幢'), np.str_('嵌'), np.str_('崂'), np.str_('尕'), np.str_('宰'), np.str_('孚'), np.str_('婷'), np.str_('姻'), np.str_('姬'), np.str_('姨'), np.str_('妲'), np.str_('囤'), np.str_('噬'), np.str_('喘'), np.str_('哮'), np.str_('咽'), np.str_('咳'), np.str_('呵'), np.str_('匾'), np.str_('劈'), np.str_('刹'), np.str_('凑'), np.str_('兔'), np.str_('侈'), np.str_('亦'), np.str_('麝'), np.str_('鲨'), np.str_('骏'), np.str_('骂'), np.str_('饺'), np.str_('饵'), np.str_('飙'), np.str_('颍'), np.str_('靳'), np.str_('雏'), np.str_('雍'), np.str_('雉'), np.str_('锈'), np.str_('锄'), np.str_('铧'), np.str_('铉'), np.str_('铂'), np.str_('钾'), np.str_('钚'), np.str_('酮'), np.str_('郧'), np.str_('邛'), np.str_('辕'), np.str_('辍'), np.str_('赚'), np.str_('赉'), np.str_('赈'), np.str_('谤'), np.str_('谒'), np.str_('诫'), np.str_('诀'), np.str_('讷'), np.str_('覃'), np.str_('褪'), np.str_('裳'), np.str_('袤'), np.str_('螃'), np.str_('蜥'), np.str_('蜗'), np.str_('蜕'), np.str_('蛤'), np.str_('蛙'), np.str_('藻'), np.str_('蕙'), np.str_('蔑'), np.str_('菁'), np.str_('莽'), np.str_('莆'), np.str_('荟'), np.str_('茹'), np.str_('茧'), np.str_('茄'), np.str_('舾'), np.str_('舸'), np.str_('臧'), np.str_('膨'), np.str_('膛'), np.str_('膀'), np.str_('脖'), np.str_('胺'), np.str_('胖'), np.str_('肚'), np.str_('羞'), np.str_('罔'), np.str_('缭'), np.str_('糟'), np.str_('粳'), np.str_('簇'), np.str_('篙'), np.str_('篁'), np.str_('笛'), np.str_('笙'), np.str_('笃'), np.str_('穗'), np.str_('稍'), np.str_('秭'), np.str_('硅'), np.str_('砟'), np.str_('矸'), np.str_('矫'), np.str_('疼'), np.str_('疟'), np.str_('畿'), np.str_('畸'), np.str_('瓮'), np.str_('璐'), np.str_('琵'), np.str_('琦'), np.str_('琢'), np.str_('玫'), np.str_('犊'), np.str_('爪'), np.str_('熬'), np.str_('烃'), np.str_('炽'), np.str_('炕'), np.str_('濠'), np.str_('澡'), np.str_('滁'), np.str_('湍'), np.str_('湃'), np.str_('渑'), np.str_('淬'), np.str_('淋'), np.str_('泾'), np.str_('沱'), np.str_('汹'), np.str_('汨'), np.str_('汁'), np.str_('氯'), np.str_('毖'), np.str_('檀'), np.str_('橄'), np.str_('榄'), np.str_('楹'), np.str_('椋'), np.str_('椁'), np.str_('棕'), np.str_('棍'), np.str_('栩'), np.str_('枕'), np.str_('杓'), np.str_('曦'), np.str_('晔'), np.str_('昱'), np.str_('斧'), np.str_('斛'), np.str_('敷'), np.str_('敞'), np.str_('敛'), np.str_('揪'), np.str_('拯'), np.str_('抄'), np.str_('扯'), np.str_('憨'), np.str_('惦'), np.str_('悖'), np.str_('恺'), np.str_('恋'), np.str_('怪'), np.str_('怨'), np.str_('彪'), np.str_('帖'), np.str_('巫'), np.str_('崃'), np.str_('峭'), np.str_('峨'), np.str_('岫'), np.str_('屹'), np.str_('寮'), np.str_('寡'), np.str_('嫣'), np.str_('嫂'), np.str_('媲'), np.str_('姹'), np.str_('姥'), np.str_('増'), np.str_('垸'), np.str_('垛'), np.str_('噪'), np.str_('嘹'), np.str_('嗽'), np.str_('喂'), np.str_('咎'), np.str_('咀'), np.str_('呛'), np.str_('吻'), np.str_('卑'), np.str_('匮'), np.str_('勠'), np.str_('刃'), np.str_('冤'), np.str_('兮'), np.str_('傣'), np.str_('俏'), np.str_('仆'), np.str_('乞'), np.str_('丕'), np.str_('〇'), np.str_('鼹'), np.str_('黜'), np.str_('麟'), np.str_('鹘'), np.str_('鹇'), np.str_('鸮'), np.str_('鸨'), np.str_('鳇'), np.str_('鲲'), np.str_('鲢'), np.str_('馅'), np.str_('颅'), np.str_('霁'), np.str_('隘'), np.str_('陡'), np.str_('陂'), np.str_('阡'), np.str_('阀'), np.str_('闵'), np.str_('镶'), np.str_('锹'), np.str_('锷'), np.str_('锯'), np.str_('锆'), np.str_('铮'), np.str_('铢'), np.str_('钰'), np.str_('钎'), np.str_('酶'), np.str_('酱'), np.str_('鄄'), np.str_('邃'), np.str_('逗'), np.str_('迦'), np.str_('辗'), np.str_('蹭'), np.str_('蹒'), np.str_('跚'), np.str_('赡'), np.str_('赎'), np.str_('豌'), np.str_('谚'), np.str_('谏'), np.str_('诽'), np.str_('诡'), np.str_('诋'), np.str_('觐'), np.str_('褒'), np.str_('裴'), np.str_('袒'), np.str_('蝇'), np.str_('蜘'), np.str_('蛛'), np.str_('虏'), np.str_('蘑'), np.str_('藐'), np.str_('薯'), np.str_('薇'), np.str_('蔗'), np.str_('蓿'), np.str_('蓟'), np.str_('蒌'), np.str_('萝'), np.str_('荫'), np.str_('茭'), np.str_('苟'), np.str_('苜'), np.str_('芹'), np.str_('芗'), np.str_('艏'), np.str_('舜'), np.str_('臻'), np.str_('臆'), np.str_('腻'), np.str_('腺'), np.str_('腥'), np.str_('腑'), np.str_('耽'), np.str_('耋'), np.str_('耄'), np.str_('翊'), np.str_('绞'), np.str_('紊'), np.str_('糙'), np.str_('粱'), np.str_('粪'), np.str_('粘'), np.str_('粑'), np.str_('簸'), np.str_('簧'), np.str_('篆'), np.str_('箕'), np.str_('箍'), np.str_('筷'), np.str_('筠'), np.str_('窦'), np.str_('窘'), np.str_('窒'), np.str_('稽'), np.str_('稠'), np.str_('稔'), np.str_('祺'), np.str_('祷'), np.str_('祚'), np.str_('磲'), np.str_('砾'), np.str_('砚'), np.str_('砗'), np.str_('砌'), np.str_('矣'), np.str_('矗'), np.str_('瞭'), np.str_('瞅'), np.str_('盱'), np.str_('皓'), np.str_('皎'), np.str_('皋'), np.str_('皂'), np.str_('瘩'), np.str_('瘘'), np.str_('瘁'), np.str_('痹'), np.str_('痊'), np.str_('疽'), np.str_('疵'), np.str_('疣'), np.str_('疙'), np.str_('畴'), np.str_('瓴'), np.str_('璞'), np.str_('瑛'), np.str_('瑁'), np.str_('琶'), np.str_('琤'), np.str_('珺'), np.str_('珉'), np.str_('玺'), np.str_('玳'), np.str_('玮'), np.str_('玥'), np.str_('獾'), np.str_('猝'), np.str_('犸'), np.str_('犍'), np.str_('熏'), np.str_('煎'), np.str_('烨'), np.str_('烙'), np.str_('烘'), np.str_('烁'), np.str_('炜'), np.str_('灶'), np.str_('濡'), np.str_('潺'), np.str_('潢'), np.str_('潞'), np.str_('潇'), np.str_('滤'), np.str_('溧'), np.str_('溅'), np.str_('涣'), np.str_('涟'), np.str_('浚'), np.str_('泣'), np.str_('泌'), np.str_('沾'), np.str_('沽'), np.str_('沛'), np.str_('沐'), np.str_('沌'), np.str_('沅'), np.str_('汩'), np.str_('汞'), np.str_('汐'), np.str_('氰'), np.str_('殡'), np.str_('殆'), np.str_('歙'), np.str_('橇'), np.str_('槐'), np.str_('榨'), np.str_('榉'), np.str_('楫'), np.str_('楞'), np.str_('棘'), np.str_('梆'), np.str_('栓'), np.str_('曌'), np.str_('暹'), np.str_('暧'), np.str_('暄'), np.str_('晗'), np.str_('昧'), np.str_('昇'), np.str_('敖'), np.str_('擞'), np.str_('搓'), np.str_('搅'), np.str_('揣'), np.str_('掺'), np.str_('掬'), np.str_('拂'), np.str_('抖'), np.str_('扒'), np.str_('戮'), np.str_('戟'), np.str_('戌'), np.str_('戊'), np.str_('愫'), np.str_('惬'), np.str_('惟'), np.str_('恤'), np.str_('彤'), np.str_('弩'), np.str_('弧'), np.str_('庹'), np.str_('庵'), np.str_('庚'), np.str_('帼'), np.str_('巯'), np.str_('崮'), np.str_('崤'), np.str_('岢'), np.str_('岖'), np.str_('岑'), np.str_('岌'), np.str_('屑'), np.str_('尴'), np.str_('尬'), np.str_('寂'), np.str_('媳'), np.str_('婵'), np.str_('姊'), np.str_('奂'), np.str_('夙'), np.str_('壕'), np.str_('墒'), np.str_('塬'), np.str_('垴'), np.str_('垭'), np.str_('垩'), np.str_('坳'), np.str_('坭'), np.str_('坨'), np.str_('坂'), np.str_('圪'), np.str_('圜'), np.str_('嚣'), np.str_('嘌'), np.str_('唆'), np.str_('哗'), np.str_('咯'), np.str_('咚'), np.str_('咙'), np.str_('咒'), np.str_('咄'), np.str_('呤'), np.str_('呕'), np.str_('厌'), np.str_('卞'), np.str_('卅'), np.str_('匀'), np.str_('凳'), np.str_('儋'), np.str_('僳'), np.str_('傈'), np.str_('倦'), np.str_('倚'), np.str_('俣'), np.str_('侥'), np.str_('侣'), np.str_('佬'), np.str_('佤'), np.str_('仡'), np.str_('亓'), np.str_('。'), np.str_('龛'), np.str_('黯'), np.str_('黏'), np.str_('鹫'), np.str_('鹞'), np.str_('鹕'), np.str_('鹋'), np.str_('鹈'), np.str_('鸻'), np.str_('鸸'), np.str_('鸶'), np.str_('鸠'), np.str_('鳞'), np.str_('鳙'), np.str_('鲽'), np.str_('鲑'), np.str_('鲈'), np.str_('鬼'), np.str_('髹'), np.str_('髋'), np.str_('骢'), np.str_('骡'), np.str_('骞'), np.str_('骇'), np.str_('驹'), np.str_('驯'), np.str_('馼'), np.str_('馒'), np.str_('饪'), np.str_('颤'), np.str_('颐'), np.str_('颌'), np.str_('颀'), np.str_('韭'), np.str_('鞑'), np.str_('靼'), np.str_('靴'), np.str_('雒'), np.str_('陟'), np.str_('陉'), np.str_('阊'), np.str_('阈'), np.str_('镛'), np.str_('镗'), np.str_('镖'), np.str_('镍'), np.str_('镌'), np.str_('镂'), np.str_('镁'), np.str_('锭'), np.str_('锨'), np.str_('铱'), np.str_('铡'), np.str_('铆'), np.str_('钯'), np.str_('钮'), np.str_('钣'), np.str_('钠'), np.str_('钒'), np.str_('鏖'), np.str_('銮'), np.str_('醛'), np.str_('醋'), np.str_('酯'), np.str_('酪'), np.str_('酥'), np.str_('酚'), np.str_('酌'), np.str_('鄢'), np.str_('邳'), np.str_('邰'), np.str_('邨'), np.str_('逄'), np.str_('迥'), np.str_('迂'), np.str_('辄'), np.str_('轶'), np.str_('轧'), np.str_('蹿'), np.str_('蹚'), np.str_('蹊'), np.str_('踢'), np.str_('跷'), np.str_('趴'), np.str_('赓'), np.str_('赐'), np.str_('贻'), np.str_('贲'), np.str_('豢'), np.str_('谲'), np.str_('谬'), np.str_('谧'), np.str_('诲'), np.str_('诟'), np.str_('诙'), np.str_('讹'), np.str_('讶'), np.str_('讧'), np.str_('謇'), np.str_('褥'), np.str_('裤'), np.str_('袍'), np.str_('衫'), np.str_('蠢'), np.str_('蠡'), np.str_('蟑'), np.str_('螂'), np.str_('蝠'), np.str_('蝙'), np.str_('蝗'), np.str_('蜿'), np.str_('蜴'), np.str_('蜒'), np.str_('蜇'), np.str_('蛰'), np.str_('蛎'), np.str_('蚂'), np.str_('蚁'), np.str_('虔'), np.str_('藩'), np.str_('藜'), np.str_('藉'), np.str_('蕨'), np.str_('蕃'), np.str_('蔺'), np.str_('蔷'), np.str_('蔫'), np.str_('蓼'), np.str_('蓓'), np.str_('萱'), np.str_('萦'), np.str_('莪'), np.str_('荻'), np.str_('荏'), np.str_('茗'), np.str_('茉'), np.str_('苯'), np.str_('苫'), np.str_('苔'), np.str_('苒'), np.str_('芸'), np.str_('芷'), np.str_('芥'), np.str_('芍'), np.str_('臀'), np.str_('膺'), np.str_('膳'), np.str_('膝'), np.str_('膊'), np.str_('腕'), np.str_('腓'), np.str_('脾'), np.str_('胳'), np.str_('胰'), np.str_('胭'), np.str_('肼'), np.str_('肤'), np.str_('聩'), np.str_('耸'), np.str_('翱'), np.str_('翡'), np.str_('翚'), np.str_('翎'), np.str_('羹'), np.str_('羔'), np.str_('缙'), np.str_('绶'), np.str_('绫'), np.str_('绢'), np.str_('绊'), np.str_('綦'), np.str_('粲'), np.str_('粟'), np.str_('篱'), np.str_('篝'), np.str_('箔'), np.str_('箐'), np.str_('窥'), np.str_('窍'), np.str_('稷'), np.str_('稞'), np.str_('秃'), np.str_('碴'), np.str_('硚'), np.str_('硖'), np.str_('砼'), np.str_('瞪'), np.str_('瞧'), np.str_('瞎'), np.str_('睬'), np.str_('睫'), np.str_('眺'), np.str_('盥'), np.str_('皿'), np.str_('皱'), np.str_('皆'), np.str_('瘾'), np.str_('瘟'), np.str_('痼'), np.str_('痴'), np.str_('痰'), np.str_('痉'), np.str_('痈'), np.str_('疴'), np.str_('疱'), np.str_('疃'), np.str_('甬'), np.str_('瓣'), np.str_('瓒'), np.str_('璠'), np.str_('琥'), np.str_('珮'), np.str_('珞'), np.str_('珑'), np.str_('珏'), np.str_('珂'), np.str_('玷'), np.str_('玖'), np.str_('猾'), np.str_('猞'), np.str_('猁'), np.str_('狩'), np.str_('狡'), np.str_('犇'), np.str_('牦'), np.str_('牍'), np.str_('爹'), np.str_('熨'), np.str_('熙'), np.str_('煞'), np.str_('煊'), np.str_('烹'), np.str_('烬'), np.str_('炊'), np.str_('炅'), np.str_('灼'), np.str_('灸'), np.str_('濑'), np.str_('澧'), np.str_('澙'), np.str_('澈'), np.str_('潩'), np.str_('漾'), np.str_('漩'), np.str_('滔'), np.str_('溺'), np.str_('渲'), np.str_('渟'), np.str_('淼'), np.str_('淦'), np.str_('涿'), np.str_('涸'), np.str_('涮'), np.str_('涩'), np.str_('涧'), np.str_('涤'), np.str_('涔'), np.str_('浠'), np.str_('洵'), np.str_('泵'), np.str_('泞'), np.str_('泓'), np.str_('泅'), np.str_('沸'), np.str_('沤'), np.str_('汊'), np.str_('氨'), np.str_('氦'), np.str_('氙'), np.str_('毋'), np.str_('殇'), np.str_('殃'), np.str_('檐'), np.str_('槟'), np.str_('榊'), np.str_('榈'), np.str_('楂'), np.str_('椿'), np.str_('椭'), np.str_('棺'), np.str_('棓'), np.str_('梢'), np.str_('梓'), np.str_('桷'), np.str_('桦'), np.str_('桓'), np.str_('栾'), np.str_('栃'), np.str_('柞'), np.str_('柚'), np.str_('柘'), np.str_('枸'), np.str_('枫'), np.str_('杞'), np.str_('杖'), np.str_('晳'), np.str_('晦'), np.str_('晏'), np.str_('晁'), np.str_('昺'), np.str_('旻'), np.str_('旷'), np.str_('斟'), np.str_('斓'), np.str_('攸'), np.str_('攫'), np.str_('攥'), np.str_('擒'), np.str_('撼'), np.str_('撮'), np.str_('摒'), np.str_('掐'), np.str_('捭'), np.str_('捡'), np.str_('捂'), np.str_('挛'), np.str_('挎'), np.str_('拽'), np.str_('拣'), np.str_('拎'), np.str_('扼'), np.str_('扁'), np.str_('懿'), np.str_('慨'), np.str_('惹'), np.str_('惋'), np.str_('悯'), np.str_('悍'), np.str_('恳'), np.str_('恫'), np.str_('恙'), np.str_('忌'), np.str_('徜'), np.str_('徘'), np.str_('徊'), np.str_('徉'), np.str_('弋'), np.str_('弁'), np.str_('帛'), np.str_('帚'), np.str_('帘'), np.str_('巳'), np.str_('嵘'), np.str_('嵖'), np.str_('崆'), np.str_('峥'), np.str_('峒'), np.str_('岬'), np.str_('岔'), np.str_('岈'), np.str_('寰'), np.str_('寞'), np.str_('寐'), np.str_('宕'), np.str_('孳'), np.str_('孪'), np.str_('嬗'), np.str_('嫱'), np.str_('婿'), np.str_('婧'), np.str_('娴'), np.str_('娲'), np.str_('娠'), np.str_('姸'), np.str_('妊'), np.str_('奘'), np.str_('奕'), np.str_('壑'), np.str_('堑'), np.str_('埸'), np.str_('埭'), np.str_('埤'), np.str_('埂'), np.str_('垅'), np.str_('坻'), np.str_('坷'), np.str_('坬'), np.str_('坟'), np.str_('囱'), np.str_('嚼'), np.str_('噶'), np.str_('嘛'), np.str_('喙'), np.str_('唢'), np.str_('唠'), np.str_('哎'), np.str_('咪'), np.str_('咛'), np.str_('咐'), np.str_('咆'), np.str_('呐'), np.str_('吼'), np.str_('吟'), np.str_('吏'), np.str_('吆'), np.str_('叻'), np.str_('叭'), np.str_('叩'), np.str_('叟'), np.str_('叁'), np.str_('厮'), np.str_('卤'), np.str_('匪'), np.str_('匕'), np.str_('勺'), np.str_('刈'), np.str_('凹'), np.str_('凛'), np.str_('冢'), np.str_('偕'), np.str_('偎'), np.str_('倻'), np.str_('倪'), np.str_('倩'), np.str_('侬'), np.str_('侏'), np.str_('侍'), np.str_('侄'), np.str_('佣'), np.str_('伺'), np.str_('伶'), np.str_('伎'), np.str_('仉'), np.str_('仃'), np.str_('亵'), np.str_('亢'), np.str_('丫'), np.str_('、')]\n"
     ]
    }
   ],
   "source": [
    "# Le vocabulaire pour t9 input\n",
    "print(input_tv.get_vocabulary())\n",
    "print(target_tv.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e36e7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def split_and_vectorize(t9_seq, char_seq, context_size=5):\n",
    "    \n",
    "    # Split chaque séquence T9 (par espace)\n",
    "    t9_parts = tf.strings.split(t9_seq, \" \")  # tf.TensorShape([None])\n",
    "\n",
    "    # Split la séquence de caractères (par caractère UTF-8)\n",
    "    char_parts = tf.strings.unicode_split(char_seq, \"UTF-8\")\n",
    "\n",
    "    # Padder la séquence de caractères pour avoir le contexte pour tous les caractères\n",
    "    char_seq_padded = tf.strings.join([tf.constant(\"     \"), char_seq])\n",
    "    char_parts_padded = tf.strings.unicode_split(char_seq_padded, \"UTF-8\")\n",
    "\n",
    "    # Vérification de la même longueur entre la séquence de t9 et celle de caractères\n",
    "    assert_op = tf.debugging.assert_equal(tf.shape(t9_parts)[0], tf.shape(char_parts)[0])\n",
    "\n",
    "    with tf.control_dependencies([assert_op]):\n",
    "        \n",
    "        # Vectorisation : chaque chiffre dans chaque t9_part est un caractère\n",
    "        vectorized_t9 = input_tv(t9_parts)       # RaggedTensor: [nb_sous_seq, longueur_t9]\n",
    "        vectorized_target = target_tv(char_parts)  # RaggedTensor: [nb_sous_seq, 1]\n",
    "\n",
    "        # Convertit RaggedTensor en Tensor avec padding (0 par défaut, qui sera ignoré lors du Embedding Layer, mask_zero=True)\n",
    "        t9_tensor = vectorized_t9.to_tensor(default_value=0)\n",
    "        target_tensor = vectorized_target.to_tensor(default_value=0)\n",
    "        target_tensor = tf.squeeze(target_tensor, axis=-1)\n",
    "        \n",
    "        # Contexte\n",
    "        # Vectorisation du target padded\n",
    "        vectorized_target_padded = target_tv(char_parts_padded).to_tensor(default_value=0)\n",
    "        # Avoir la longueur de caractères après le padding\n",
    "        padded_seq_len = tf.shape(vectorized_target_padded)[0]\n",
    "        # Initialiser un tenseur pour le contexte\n",
    "        contexts = tf.TensorArray(dtype=tf.int64, size=padded_seq_len - context_size)\n",
    "        # Parcourir chaque caractère sauf le dernier\n",
    "        for i in tf.range(padded_seq_len - context_size):\n",
    "            # Récupérer dynamiquement un contexte de taille fixe\n",
    "            context = vectorized_target_padded[i:i+context_size]\n",
    "            # Écrire dans la position correspondante du tenseur array\n",
    "            contexts = contexts.write(i, context)\n",
    "        # Piler le tenseur\n",
    "        contexts_tensor = contexts.stack()\n",
    "        contexts_tensor = tf.squeeze(contexts_tensor, axis=-1)\n",
    "\n",
    "        return {\"t9_input\": t9_tensor, \"context_input\": contexts_tensor}, target_tensor\n",
    "\n",
    "transformed_dataset = tf_dataset.map(\n",
    "    lambda t9, target: split_and_vectorize(t9, target),\n",
    "    num_parallel_calls=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e36310b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'t9_input': <tf.Tensor: shape=(256, 100, 6), dtype=int64, numpy=\n",
       "  array([[[7, 2, 3, 3, 2, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [9, 5, 3, 0, 0, 0],\n",
       "          ...,\n",
       "          [2, 5, 0, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [7, 4, 3, 2, 0, 0]],\n",
       "  \n",
       "         [[8, 2, 3, 2, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [7, 4, 3, 2, 0, 0],\n",
       "          ...,\n",
       "          [2, 5, 0, 0, 0, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [5, 4, 3, 2, 0, 0]],\n",
       "  \n",
       "         [[7, 3, 6, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [2, 3, 3, 2, 0, 0],\n",
       "          ...,\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [9, 2, 2, 0, 0, 0],\n",
       "          [9, 6, 4, 3, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[7, 2, 5, 3, 2, 0],\n",
       "          [7, 4, 2, 0, 0, 0],\n",
       "          [7, 2, 2, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[7, 5, 2, 0, 0, 0],\n",
       "          [9, 2, 0, 0, 0, 0],\n",
       "          [6, 2, 4, 3, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[9, 2, 3, 6, 0, 0],\n",
       "          [8, 2, 3, 0, 0, 0],\n",
       "          [9, 2, 0, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]]])>,\n",
       "  'context_input': <tf.Tensor: shape=(256, 100, 5), dtype=int64, numpy=\n",
       "  array([[[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,    4],\n",
       "          [   1,    1,    1,    4,    3],\n",
       "          ...,\n",
       "          [  96,   36,   69,    4,   43],\n",
       "          [  36,   69,    4,   43,   13],\n",
       "          [  69,    4,   43,   13,    4]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   71],\n",
       "          [   1,    1,    1,   71,    4],\n",
       "          ...,\n",
       "          [1116,   71,  196,   81,  167],\n",
       "          [  71,  196,   81,  167,   13],\n",
       "          [ 196,   81,  167,   13,    3]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  268],\n",
       "          [   1,    1,    1,  268,    4],\n",
       "          ...,\n",
       "          [  41,   25,   31,   70,  154],\n",
       "          [  25,   31,   70,  154,   65],\n",
       "          [  31,   70,  154,   65,  248]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  134],\n",
       "          [   1,    1,    1,  134,    5],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   30],\n",
       "          [   1,    1,    1,   30,  166],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  217],\n",
       "          [   1,    1,    1,  217,   25],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]]])>},\n",
       " <tf.Tensor: shape=(256, 100), dtype=int64, numpy=\n",
       " array([[  4,   3,   7, ...,  13,   4,  43],\n",
       "        [ 71,   4,  43, ...,  13,   3, 315],\n",
       "        [268,   4,  42, ...,  65, 248,  14],\n",
       "        ...,\n",
       "        [134,   5, 541, ...,   0,   0,   0],\n",
       "        [ 30, 166,  15, ...,   0,   0,   0],\n",
       "        [217,  25, 166, ...,   0,   0,   0]])>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_dataset.padded_batch(256).take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf779c5f",
   "metadata": {},
   "source": [
    "## Split train-valid-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec477403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 18:26:49.373046: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du train : 16503\n",
      "Taille du validation : 2064\n",
      "Taille du test : 2062\n"
     ]
    }
   ],
   "source": [
    "c = transformed_dataset.reduce(0, lambda x,_:x+1).numpy()\n",
    "\n",
    "shuffled_ds = transformed_dataset.shuffle(buffer_size=c, seed=42)\n",
    "\n",
    "train_size = c * 80 // 100\n",
    "test_size = c * 10 // 100\n",
    "val_size = c - train_size - test_size\n",
    "\n",
    "ds_train = shuffled_ds.take(train_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = shuffled_ds.skip(train_size).take(val_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = shuffled_ds.skip(train_size+val_size).take(test_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Taille du train :\", ds_train.cardinality().numpy())\n",
    "print(\"Taille du validation :\", ds_val.cardinality().numpy())\n",
    "print(\"Taille du test :\", ds_test.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e252e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 5\n",
    "\n",
    "ds_train_padded = ds_train.padded_batch(\n",
    "    128,\n",
    "    padded_shapes=(\n",
    "        {\n",
    "            \"t9_input\": [MAX_SEQUENCE_LENGTH, 6],\n",
    "            \"context_input\": [MAX_SEQUENCE_LENGTH, context_size]\n",
    "        },\n",
    "        [MAX_SEQUENCE_LENGTH]  # Pour les étiquettes\n",
    "    ),\n",
    "    padding_values=(\n",
    "        {\n",
    "            \"t9_input\": tf.constant(0, dtype=tf.int64),\n",
    "            \"context_input\": tf.constant(0, dtype=tf.int64)\n",
    "        },\n",
    "        tf.constant(0, dtype=tf.int64)  # Pour les étiquettes\n",
    "    ),\n",
    "    drop_remainder=True,\n",
    ").map(lambda x, y: (x, y, y>0)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_val_padded = ds_val.padded_batch(\n",
    "    128,\n",
    "    padded_shapes=(\n",
    "        {\n",
    "            \"t9_input\": [MAX_SEQUENCE_LENGTH, 6],\n",
    "            \"context_input\": [MAX_SEQUENCE_LENGTH, context_size]\n",
    "        },\n",
    "        [MAX_SEQUENCE_LENGTH]  # Pour les étiquettes\n",
    "    ),\n",
    "    padding_values=(\n",
    "        {\n",
    "            \"t9_input\": tf.constant(0, dtype=tf.int64),\n",
    "            \"context_input\": tf.constant(0, dtype=tf.int64)\n",
    "        },\n",
    "        tf.constant(0, dtype=tf.int64)  # Pour les étiquettes\n",
    "    ),\n",
    "    drop_remainder=True,\n",
    ").map(lambda x, y: (x, y, y>0)).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_test_padded = ds_test.padded_batch(\n",
    "    128,\n",
    "    padded_shapes=(\n",
    "        {\n",
    "            \"t9_input\": [MAX_SEQUENCE_LENGTH, 6],\n",
    "            \"context_input\": [MAX_SEQUENCE_LENGTH, context_size]\n",
    "        },\n",
    "        [MAX_SEQUENCE_LENGTH]  # Pour les étiquettes\n",
    "    ),\n",
    "    padding_values=(\n",
    "        {\n",
    "            \"t9_input\": tf.constant(0, dtype=tf.int64),\n",
    "            \"context_input\": tf.constant(0, dtype=tf.int64)\n",
    "        },\n",
    "        tf.constant(0, dtype=tf.int64)  # Pour les étiquettes\n",
    "    ),\n",
    "    drop_remainder=True,\n",
    ").map(lambda x, y: (x, y, y>0)).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd73ac4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'t9_input': <tf.Tensor: shape=(128, 100, 6), dtype=int64, numpy=\n",
       "  array([[[7, 2, 3, 3, 2, 0],\n",
       "          [7, 4, 3, 2, 0, 0],\n",
       "          [7, 6, 4, 3, 0, 0],\n",
       "          ...,\n",
       "          [9, 2, 2, 0, 0, 0],\n",
       "          [7, 3, 3, 2, 0, 0],\n",
       "          [2, 5, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[7, 2, 0, 0, 0, 0],\n",
       "          [8, 2, 3, 0, 0, 0],\n",
       "          [9, 2, 3, 2, 0, 0],\n",
       "          ...,\n",
       "          [9, 2, 2, 0, 0, 0],\n",
       "          [8, 2, 4, 3, 0, 0],\n",
       "          [7, 4, 3, 0, 0, 0]],\n",
       "  \n",
       "         [[7, 4, 2, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          [7, 4, 3, 2, 0, 0],\n",
       "          ...,\n",
       "          [7, 6, 5, 0, 0, 0],\n",
       "          [6, 4, 3, 0, 0, 0],\n",
       "          [7, 4, 3, 2, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[8, 2, 0, 0, 0, 0],\n",
       "          [7, 2, 5, 0, 0, 0],\n",
       "          [8, 2, 3, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[7, 3, 6, 0, 0, 0],\n",
       "          [7, 4, 2, 0, 0, 0],\n",
       "          [7, 5, 3, 0, 0, 0],\n",
       "          ...,\n",
       "          [7, 2, 6, 0, 0, 0],\n",
       "          [7, 6, 0, 0, 0, 0],\n",
       "          [2, 6, 3, 0, 0, 0]],\n",
       "  \n",
       "         [[9, 6, 4, 3, 0, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [9, 5, 3, 0, 0, 0],\n",
       "          ...,\n",
       "          [4, 2, 6, 0, 0, 0],\n",
       "          [2, 6, 4, 3, 0, 0],\n",
       "          [7, 6, 0, 0, 0, 0]]])>,\n",
       "  'context_input': <tf.Tensor: shape=(128, 100, 5), dtype=int64, numpy=\n",
       "  array([[[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,    4],\n",
       "          [   1,    1,    1,    4,   43],\n",
       "          ...,\n",
       "          [  28,  369,  103,    5,   43],\n",
       "          [ 369,  103,    5,   43,  248],\n",
       "          [ 103,    5,   43,  248,  602]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   41],\n",
       "          [   1,    1,    1,   41,   25],\n",
       "          ...,\n",
       "          [  57,  385,  636,  496,   22],\n",
       "          [ 385,  636,  496,   22,   76],\n",
       "          [ 636,  496,   22,   76,  953]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,    5],\n",
       "          [   1,    1,    1,    5,    4],\n",
       "          ...,\n",
       "          [1491,  779,  711,  101,   12],\n",
       "          [ 779,  711,  101,   12,   53],\n",
       "          [ 711,  101,   12,   53, 1270]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  111],\n",
       "          [   1,    1,    1,  111,  199],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  268],\n",
       "          [   1,    1,    1,  268,  141],\n",
       "          ...,\n",
       "          [ 299,  705,  923,  199,   45],\n",
       "          [ 705,  923,  199,   45,  599],\n",
       "          [ 923,  199,   45,  599,  101]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   14],\n",
       "          [   1,    1,    1,   14,    3],\n",
       "          ...,\n",
       "          [ 284,  278,    6,   52,  162],\n",
       "          [ 278,    6,   52,  162,   20],\n",
       "          [   6,   52,  162,   20,   62]]])>},\n",
       " <tf.Tensor: shape=(128, 100), dtype=int64, numpy=\n",
       " array([[   4,   43,  258, ...,  248,  602,   50],\n",
       "        [  41,   25,   31, ...,   76,  953,  972],\n",
       "        [   5,    4,   43, ...,   53, 1270,  551],\n",
       "        ...,\n",
       "        [ 111,  199,   28, ...,    0,    0,    0],\n",
       "        [ 268,  141,  133, ...,  599,  101,    3],\n",
       "        [  14,    3,    7, ...,   20,   62,  101]])>,\n",
       " <tf.Tensor: shape=(128, 100), dtype=bool, numpy=\n",
       " array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]])>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_padded.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd0d15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf.io.gfile.exists(\"data\"):\n",
    "    tf.io.gfile.makedirs(\"data\")\n",
    "\n",
    "# Sauvegarder les jeux de données\n",
    "tf.data.Dataset.save(ds_train_padded, \"data/train_dataset\")\n",
    "tf.data.Dataset.save(ds_val_padded, \"data/val_dataset\")\n",
    "tf.data.Dataset.save(ds_test_padded, \"data/test_dataset\")\n",
    "\n",
    "# Sauvegarder les vectorisateurs\n",
    "with open(\"data/input_vectorizer.pkl\", \"wb\") as f:\n",
    "    pk.dump(input_tv, f)\n",
    "\n",
    "with open(\"data/target_vectorizer.pkl\", \"wb\") as f:\n",
    "    pk.dump(target_tv, f)\n",
    "\n",
    "# Sauvegarder les paramètres\n",
    "params = {\n",
    "    \"MAX_SEQUENCE_LENGTH\": MAX_SEQUENCE_LENGTH,\n",
    "    \"context_size\": context_size,\n",
    "    \"input_vocab_size\": len(input_tv.get_vocabulary()),\n",
    "    \"target_vocab_size\": len(target_tv.get_vocabulary()),\n",
    "}\n",
    "\n",
    "with open(\"data/params.pkl\", \"wb\") as f:\n",
    "    pk.dump(params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9815bd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'t9_input': <tf.Tensor: shape=(128, 100, 6), dtype=int64, numpy=\n",
       "  array([[[9, 2, 0, 0, 0, 0],\n",
       "          [9, 2, 4, 3, 0, 0],\n",
       "          [7, 4, 2, 0, 0, 0],\n",
       "          ...,\n",
       "          [8, 2, 4, 3, 0, 0],\n",
       "          [9, 2, 5, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0]],\n",
       "  \n",
       "         [[7, 5, 3, 0, 0, 0],\n",
       "          [2, 6, 4, 0, 0, 0],\n",
       "          [4, 6, 0, 0, 0, 0],\n",
       "          ...,\n",
       "          [9, 6, 3, 0, 0, 0],\n",
       "          [8, 6, 0, 0, 0, 0],\n",
       "          [4, 4, 3, 0, 0, 0]],\n",
       "  \n",
       "         [[3, 2, 4, 3, 0, 0],\n",
       "          [8, 4, 2, 0, 0, 0],\n",
       "          [6, 4, 0, 0, 0, 0],\n",
       "          ...,\n",
       "          [7, 5, 3, 0, 0, 0],\n",
       "          [2, 6, 4, 0, 0, 0],\n",
       "          [5, 3, 6, 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[7, 2, 3, 2, 0, 0],\n",
       "          [4, 2, 0, 0, 0, 0],\n",
       "          [9, 4, 2, 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[2, 6, 3, 0, 0, 0],\n",
       "          [8, 2, 4, 0, 0, 0],\n",
       "          [7, 2, 6, 0, 0, 0],\n",
       "          ...,\n",
       "          [7, 4, 2, 0, 0, 0],\n",
       "          [5, 5, 0, 0, 0, 0],\n",
       "          [8, 2, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[9, 6, 2, 0, 0, 0],\n",
       "          [2, 3, 6, 0, 0, 0],\n",
       "          [8, 2, 0, 0, 0, 0],\n",
       "          ...,\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [2, 5, 0, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0]]])>,\n",
       "  'context_input': <tf.Tensor: shape=(128, 100, 5), dtype=int64, numpy=\n",
       "  array([[[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   10],\n",
       "          [   1,    1,    1,   10,   68],\n",
       "          ...,\n",
       "          [ 526,   99,   62,   69,    2],\n",
       "          [  99,   62,   69,    2,   81],\n",
       "          [  62,   69,    2,   81,  167]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  133],\n",
       "          [   1,    1,    1,  133,  120],\n",
       "          ...,\n",
       "          [  75,  341,    2,   63,  372],\n",
       "          [ 341,    2,   63,  372,  345],\n",
       "          [   2,   63,  372,  345,   92]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   12],\n",
       "          [   1,    1,    1,   12,   39],\n",
       "          ...,\n",
       "          [ 169,  441,  116,  148, 1404],\n",
       "          [ 441,  116,  148, 1404,  133],\n",
       "          [ 116,  148, 1404,  133,  249]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  303],\n",
       "          [   1,    1,    1,  303,  649],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,    3],\n",
       "          [   1,    1,    1,    3,   24],\n",
       "          ...,\n",
       "          [ 247,   39,   76,   76,    5],\n",
       "          [  39,   76,   76,    5,    5],\n",
       "          [  76,   76,    5,    5,    2]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  580],\n",
       "          [   1,    1,    1,  580,  148],\n",
       "          ...,\n",
       "          [ 488,  329,  256,  103,    4],\n",
       "          [ 329,  256,  103,    4,    3],\n",
       "          [ 256,  103,    4,    3,   13]]])>},\n",
       " <tf.Tensor: shape=(128, 100), dtype=int64, numpy=\n",
       " array([[ 10,  68,   5, ...,  81, 167,  65],\n",
       "        [133, 120,  21, ..., 345,  92, 234],\n",
       "        [ 12,  39, 233, ..., 133, 249, 287],\n",
       "        ...,\n",
       "        [303, 649, 568, ...,   0,   0,   0],\n",
       "        [  3,  24,  18, ...,   5,   2,  84],\n",
       "        [580, 148, 231, ...,   3,  13,   4]])>,\n",
       " <tf.Tensor: shape=(128, 100), dtype=bool, numpy=\n",
       " array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]])>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_padded.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870dabbe",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f6c70",
   "metadata": {},
   "source": [
    "Sogou T9 est une méthode d’entrée intelligente qui :\n",
    "\n",
    "- Prend des séquences numériques (ex. : \"94664 486\" pour \"zhong guo\").\n",
    "- Génère des séquences de caractères chinois (ex. : \"中国\").\n",
    "- Utilise le contexte (mots précédents) pour désambiguïser les prédictions.\n",
    "- Est optimisé pour la vitesse et la précision, souvent avec des modèles entraînés sur de vastes corpus.\n",
    "\n",
    "Pour reproduire cela, il faut utiliser un modèle seq2seq avec un encodeur-décodeur (2 entrées) :\n",
    "\n",
    "- Encodeur : Lit la séquence T9 et la compresse en une représentation contextuelle.\n",
    "- Décodeur : Génère la séquence de caractères chinois à partir de cette représentation.\n",
    "\n",
    "[Functional API](https://keras.io/guides/functional_api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a8cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744792685.506897 1808324 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744792685.506919 1808324 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données\n",
    "def load_dataset(path):\n",
    "    return tf.data.Dataset.load(path)\n",
    "\n",
    "def load_vectorizer(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pk.load(f)\n",
    "    \n",
    "def load_params(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pk.load(f)\n",
    "    \n",
    "# Chargement des jeux de données\n",
    "ds_train_padded = load_dataset(\"data/train_dataset\")\n",
    "ds_val_padded = load_dataset(\"data/val_dataset\")\n",
    "ds_test_padded = load_dataset(\"data/test_dataset\")\n",
    "\n",
    "# Chargement des vectorisateurs\n",
    "input_tv = load_vectorizer(\"data/input_vectorizer.pkl\")\n",
    "target_tv = load_vectorizer(\"data/target_vectorizer.pkl\")\n",
    "\n",
    "# Chargement des paramètres\n",
    "params = load_params(\"data/params.pkl\")\n",
    "context_size = params[\"context_size\"]\n",
    "MAX_SEQUENCE_LENGTH = params[\"MAX_SEQUENCE_LENGTH\"]\n",
    "input_vocab_size = params[\"input_vocab_size\"]\n",
    "target_vocab_size = params[\"target_vocab_size\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7826a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'t9_input': <tf.Tensor: shape=(128, 100, 6), dtype=int64, numpy=\n",
       "  array([[[7, 3, 6, 0, 0, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [8, 2, 4, 0, 0, 0],\n",
       "          ...,\n",
       "          [8, 2, 3, 0, 0, 0],\n",
       "          [8, 6, 0, 0, 0, 0],\n",
       "          [8, 2, 4, 3, 2, 0]],\n",
       "  \n",
       "         [[9, 2, 0, 0, 0, 0],\n",
       "          [9, 2, 4, 3, 0, 0],\n",
       "          [8, 2, 3, 2, 0, 0],\n",
       "          ...,\n",
       "          [5, 4, 0, 0, 0, 0],\n",
       "          [8, 6, 5, 0, 0, 0],\n",
       "          [4, 5, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[5, 2, 0, 0, 0, 0],\n",
       "          [8, 2, 5, 0, 0, 0],\n",
       "          [7, 2, 3, 3, 2, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[3, 5, 2, 0, 0, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          [2, 6, 3, 0, 0, 0],\n",
       "          ...,\n",
       "          [8, 5, 0, 0, 0, 0],\n",
       "          [4, 6, 0, 0, 0, 0],\n",
       "          [5, 5, 3, 0, 0, 0]],\n",
       "  \n",
       "         [[7, 2, 5, 3, 2, 0],\n",
       "          [7, 4, 2, 0, 0, 0],\n",
       "          [7, 2, 4, 3, 2, 0],\n",
       "          ...,\n",
       "          [9, 2, 2, 0, 0, 0],\n",
       "          [7, 2, 4, 0, 0, 0],\n",
       "          [5, 5, 0, 0, 0, 0]],\n",
       "  \n",
       "         [[7, 5, 2, 0, 0, 0],\n",
       "          [3, 5, 2, 0, 0, 0],\n",
       "          [9, 6, 2, 0, 0, 0],\n",
       "          ...,\n",
       "          [2, 6, 2, 0, 0, 0],\n",
       "          [7, 2, 0, 0, 0, 0],\n",
       "          [9, 2, 0, 0, 0, 0]]])>,\n",
       "  'context_input': <tf.Tensor: shape=(128, 100, 5), dtype=int64, numpy=\n",
       "  array([[[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  268],\n",
       "          [   1,    1,    1,  268,    3],\n",
       "          ...,\n",
       "          [ 151,  223,   28,   12,    2],\n",
       "          [ 223,   28,   12,    2,  240],\n",
       "          [  28,   12,    2,  240, 1990]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   10],\n",
       "          [   1,    1,    1,   10,   68],\n",
       "          ...,\n",
       "          [  44,   69,    4,   43,   65],\n",
       "          [  69,    4,   43,   65,    8],\n",
       "          [   4,   43,   65,    8,  239]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   77],\n",
       "          [   1,    1,    1,   77,  176],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0,    0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   88],\n",
       "          [   1,    1,    1,   88,    3],\n",
       "          ...,\n",
       "          [ 721,  680,  262, 1068,  359],\n",
       "          [ 680,  262, 1068,  359,   17],\n",
       "          [ 262, 1068,  359,   17,   21]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,  134],\n",
       "          [   1,    1,    1,  134,    5],\n",
       "          ...,\n",
       "          [  49,   69,    5,   22,  507],\n",
       "          [  69,    5,   22,  507,  425],\n",
       "          [   5,   22,  507,  425,  110]],\n",
       "  \n",
       "         [[   1,    1,    1,    1,    1],\n",
       "          [   1,    1,    1,    1,   47],\n",
       "          [   1,    1,    1,   47,  132],\n",
       "          ...,\n",
       "          [ 425,  409,   34,  115,  338],\n",
       "          [ 409,   34,  115,  338,    6],\n",
       "          [  34,  115,  338,    6,   52]]])>},\n",
       " <tf.Tensor: shape=(128, 100), dtype=int64, numpy=\n",
       " array([[ 268,    3,   24, ...,  240, 1990,  793],\n",
       "        [  10,   68,   71, ...,    8,  239,  498],\n",
       "        [  77,  176,    4, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  88,    3,    3, ...,   17,   21,  117],\n",
       "        [ 134,    5,  667, ...,  425,  110,    2],\n",
       "        [  47,  132,  828, ...,    6,   52,  219]])>,\n",
       " <tf.Tensor: shape=(128, 100), dtype=bool, numpy=\n",
       " array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]])>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_padded.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c445d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer to mask embeddings for token ID 1\n",
    "class MaskToken1Layer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MaskToken1Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        embeddings, input_ids = inputs\n",
    "        # Créer un masque où input_ids != 1\n",
    "        mask_token_1 = tf.cast(tf.not_equal(input_ids, 1), tf.float32)\n",
    "        \n",
    "        # Développer le masque pour faire correspondre la forme des integers\n",
    "        mask_token_1 = tf.expand_dims(mask_token_1, -1) # [batch_size, seq_len, 1]\n",
    "\n",
    "        # Appliquer un masque aux intégres\n",
    "        masked_embeddings = embeddings * mask_token_1\n",
    "\n",
    "        return masked_embeddings\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        embeddings, input_ids = inputs\n",
    "        # Since input_ids is a KerasTensor, we rely on the Embedding's mask_zero\n",
    "        # and let Keras propagate the mask. We don't need to compute a new mask here\n",
    "        # because mask_zero=True in the Embedding layer already handles token ID 0,\n",
    "        # and token ID 1 is handled in call().\n",
    "        if mask is not None:\n",
    "            # Use the incoming mask (from mask_zero=True) if available\n",
    "            return mask\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127e2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_tv, output_tv, emb_size, gru_size, context_size=5):\n",
    "    # Inputs\n",
    "    t9_input = keras.layers.Input(shape=(None, 6), name=\"t9_input\")  # (batch, seq_len, t9_len)\n",
    "    context_input = keras.layers.Input(shape=(None, context_size), name=\"context_input\")  # (batch, seq_len, context_len)\n",
    "\n",
    "    # ===== T9 processing =====\n",
    "    t9_emb = keras.layers.TimeDistributed(\n",
    "        keras.layers.Embedding(input_dim=input_tv.vocabulary_size(),\n",
    "                         output_dim=emb_size,\n",
    "                         mask_zero=True),\n",
    "        name=\"t9_embedding\")(t9_input)\n",
    "    \n",
    "    t9_emb = keras.layers.Dropout(0.3, name=\"t9_embedding_dropout\")(t9_emb)\n",
    "\n",
    "    t9_encoded = keras.layers.TimeDistributed(\n",
    "        keras.layers.GRU(gru_size,\n",
    "                   activation=\"tanh\",\n",
    "                   recurrent_activation=\"sigmoid\",\n",
    "                   dropout=0.3,\n",
    "                   return_sequences=False),\n",
    "        name=\"t9_gru\")(t9_emb)\n",
    "\n",
    "    t9_encoded = keras.layers.LayerNormalization(name=\"t9_encoding_norm\")(t9_encoded)\n",
    "\n",
    "    # ===== Context processing =====\n",
    "    context_emb = keras.layers.TimeDistributed(\n",
    "        keras.layers.Embedding(input_dim=output_tv.vocabulary_size(),\n",
    "                         output_dim=emb_size,\n",
    "                         mask_zero=True),\n",
    "        name=\"context_embedding\")(context_input)\n",
    "    \n",
    "    context_emb = keras.layers.Dropout(0.3, name=\"context_embedding_dropout\")(context_emb)\n",
    "    \n",
    "\n",
    "    context_encoded = keras.layers.TimeDistributed(\n",
    "        keras.layers.GRU(gru_size,\n",
    "                   activation=\"tanh\",\n",
    "                   recurrent_activation=\"sigmoid\",\n",
    "                   dropout=0.3,\n",
    "                   return_sequences=False),\n",
    "        name=\"context_gru\")(context_emb)\n",
    "\n",
    "    context_encoded = keras.layers.LayerNormalization(name=\"context_encoding_norm\")(context_encoded)\n",
    "\n",
    "    # ===== Merge & Output =====\n",
    "    merged = keras.layers.Concatenate(axis=-1, name=\"merged_features\")([t9_encoded, context_encoded])\n",
    "\n",
    "    output = keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(output_tv.vocabulary_size(), activation=\"softmax\"),\n",
    "        name=\"softmax\")(merged)\n",
    "\n",
    "    model = keras.Model(inputs={\"t9_input\": t9_input, \"context_input\": context_input}, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            keras_hub.metrics.Perplexity(from_logits=False, mask_token_id=0),\n",
    "            keras.metrics.SparseCategoricalAccuracy()],\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d7e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(input_tv, target_tv, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1765b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - loss: 7.4783 - perplexity: 3389.0205 - sparse_categorical_accuracy: 0.0078 - weighted_sparse_categorical_accuracy: 0.0085\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 858ms/step - loss: 6.5749 - perplexity: 1274.2211 - sparse_categorical_accuracy: 0.0291 - weighted_sparse_categorical_accuracy: 0.0316\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 763ms/step - loss: 6.0597 - perplexity: 721.2537 - sparse_categorical_accuracy: 0.0308 - weighted_sparse_categorical_accuracy: 0.0335\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 686ms/step - loss: 5.9522 - perplexity: 641.8267 - sparse_categorical_accuracy: 0.0360 - weighted_sparse_categorical_accuracy: 0.0391\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 676ms/step - loss: 5.7592 - perplexity: 520.8516 - sparse_categorical_accuracy: 0.0561 - weighted_sparse_categorical_accuracy: 0.0609\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 711ms/step - loss: 5.4584 - perplexity: 376.0449 - sparse_categorical_accuracy: 0.0855 - weighted_sparse_categorical_accuracy: 0.0928\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 696ms/step - loss: 5.0755 - perplexity: 248.2450 - sparse_categorical_accuracy: 0.1219 - weighted_sparse_categorical_accuracy: 0.1323\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 733ms/step - loss: 4.6745 - perplexity: 160.6102 - sparse_categorical_accuracy: 0.1540 - weighted_sparse_categorical_accuracy: 0.1672\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 694ms/step - loss: 4.2828 - perplexity: 104.9611 - sparse_categorical_accuracy: 0.2075 - weighted_sparse_categorical_accuracy: 0.2253\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 732ms/step - loss: 3.8971 - perplexity: 69.0342 - sparse_categorical_accuracy: 0.2649 - weighted_sparse_categorical_accuracy: 0.2876\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 747ms/step - loss: 3.5352 - perplexity: 46.5851 - sparse_categorical_accuracy: 0.3166 - weighted_sparse_categorical_accuracy: 0.3438\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 708ms/step - loss: 3.2074 - perplexity: 32.6217 - sparse_categorical_accuracy: 0.3630 - weighted_sparse_categorical_accuracy: 0.3941\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 698ms/step - loss: 2.9162 - perplexity: 23.7675 - sparse_categorical_accuracy: 0.4030 - weighted_sparse_categorical_accuracy: 0.4376\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 702ms/step - loss: 2.6641 - perplexity: 18.0724 - sparse_categorical_accuracy: 0.4365 - weighted_sparse_categorical_accuracy: 0.4740\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 700ms/step - loss: 2.4415 - perplexity: 14.1883 - sparse_categorical_accuracy: 0.4667 - weighted_sparse_categorical_accuracy: 0.5067\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 704ms/step - loss: 2.2441 - perplexity: 11.4494 - sparse_categorical_accuracy: 0.4969 - weighted_sparse_categorical_accuracy: 0.5396\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 703ms/step - loss: 2.0671 - perplexity: 9.4459 - sparse_categorical_accuracy: 0.5222 - weighted_sparse_categorical_accuracy: 0.5671\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 704ms/step - loss: 1.9115 - perplexity: 7.9760 - sparse_categorical_accuracy: 0.5438 - weighted_sparse_categorical_accuracy: 0.5905\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 673ms/step - loss: 1.7736 - perplexity: 6.8663 - sparse_categorical_accuracy: 0.5621 - weighted_sparse_categorical_accuracy: 0.6104\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 674ms/step - loss: 1.6540 - perplexity: 6.0299 - sparse_categorical_accuracy: 0.5775 - weighted_sparse_categorical_accuracy: 0.6271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31de9d760>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train_padded.take(10), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bdc93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle\n",
    "model.save(\"model_simple.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Libérer les ressources\n",
    "tf.keras.backend.clear_session()  # Réinitialise la session TensorFlow\n",
    "gc.collect()  # Force le garbage collector pour libérer la mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1c936",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "def build_model_with_1_mask(input_tv, output_tv, emb_size, gru_size, context_size=5):\n",
    "    # Inputs\n",
    "    t9_input = keras.layers.Input(shape=(None, 6), name=\"t9_input\")  # (batch, seq_len, t9_len)\n",
    "    context_input = keras.layers.Input(shape=(None, context_size), name=\"context_input\")  # (batch, seq_len, context_len)\n",
    "\n",
    "    # ===== T9 processing =====\n",
    "    t9_emb = keras.layers.TimeDistributed(\n",
    "        keras.layers.Embedding(input_dim=input_tv.vocabulary_size(),\n",
    "                         output_dim=emb_size,\n",
    "                         mask_zero=True),\n",
    "        name=\"t9_embedding\")(t9_input)\n",
    "    \n",
    "    t9_emb = MaskToken1Layer(name=\"t9_mask_token_1\")([t9_emb, t9_input])\n",
    "\n",
    "    t9_emb = keras.layers.Dropout(0.3, name=\"t9_embedding_dropout\")(t9_emb)\n",
    "\n",
    "    t9_encoded = keras.layers.TimeDistributed(\n",
    "        keras.layers.GRU(gru_size,\n",
    "                   activation=\"tanh\",\n",
    "                   recurrent_activation=\"sigmoid\",\n",
    "                   dropout=0.3,\n",
    "                   return_sequences=False),\n",
    "        name=\"t9_gru\")(t9_emb)\n",
    "\n",
    "    t9_encoded = keras.layers.LayerNormalization(name=\"t9_encoding_norm\")(t9_encoded)\n",
    "\n",
    "    # ===== Context processing =====\n",
    "    context_emb = keras.layers.TimeDistributed(\n",
    "        keras.layers.Embedding(input_dim=output_tv.vocabulary_size(),\n",
    "                         output_dim=emb_size,\n",
    "                         mask_zero=True),\n",
    "        name=\"context_embedding\")(context_input)\n",
    "    \n",
    "    context_emb = MaskToken1Layer(name=\"context_mask_token_1\")([context_emb, context_input])\n",
    "\n",
    "    context_emb = keras.layers.Dropout(0.3, name=\"context_embedding_dropout\")(context_emb)\n",
    "    \n",
    "\n",
    "    context_encoded = keras.layers.TimeDistributed(\n",
    "        keras.layers.GRU(gru_size,\n",
    "                   activation=\"tanh\",\n",
    "                   recurrent_activation=\"sigmoid\",\n",
    "                   dropout=0.3,\n",
    "                   return_sequences=False),\n",
    "        name=\"context_gru\")(context_emb)\n",
    "\n",
    "    context_encoded = keras.layers.LayerNormalization(name=\"context_encoding_norm\")(context_encoded)\n",
    "\n",
    "    # ===== Merge & Output =====\n",
    "    merged = keras.layers.Concatenate(axis=-1, name=\"merged_features\")([t9_encoded, context_encoded])\n",
    "\n",
    "    output = keras.layers.TimeDistributed(\n",
    "        keras.layers.Dense(output_tv.vocabulary_size(), activation=\"softmax\"),\n",
    "        name=\"softmax\")(merged)\n",
    "\n",
    "    model = keras.Model(inputs={\"t9_input\": t9_input, \"context_input\": context_input}, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            keras_hub.metrics.Perplexity(from_logits=False, mask_token_id=0),\n",
    "            keras.metrics.SparseCategoricalAccuracy()],\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5233f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model_with_1_mask = build_model_with_1_mask(input_tv, target_tv, 64, 64)\n",
    "model_with_1_mask.fit(ds_train_padded.take(10), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ec019",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_1_mask.save(\"model_with_1_mask.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaca4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355676dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0850c04",
   "metadata": {},
   "source": [
    "# Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c4364e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation du modèle simple...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 260ms/step - loss: 1.6904 - perplexity: 6.3192 - sparse_categorical_accuracy: 0.5626 - weighted_sparse_categorical_accuracy: 0.6135\n",
      "\n",
      "Évaluation du modèle avec 1 masqué...\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 287ms/step - accuracy: 0.5610 - loss: 1.6768 - perplexity: 6.2259 - sparse_categorical_accuracy: 0.6117\n",
      "\n",
      "Résultats :\n",
      "Modèle simple - Perte : 1.6757, Précision : 0.5639\n",
      "Modèle avec 1 masqué - Perte : 1.6614, Précision : 0.6145\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(model_simple, model_with_1_mask, ds_test):\n",
    "    print(\"Évaluation du modèle simple...\")\n",
    "    metrics_simple = model_simple.evaluate(ds_test, verbose=1, return_dict=True)\n",
    "    # print(\"Metrics returned for model_simple:\", metrics_simple)\n",
    "\n",
    "    print(\"\\nÉvaluation du modèle avec 1 masqué...\")\n",
    "    metrics_1_mask = model_with_1_mask.evaluate(ds_test, verbose=1, return_dict=True)\n",
    "    # print(\"Metrics returned for model_with_1_mask:\", metrics_1_mask)\n",
    "    \n",
    "    # Extract metrics\n",
    "    loss_simple = metrics_simple['loss']\n",
    "    perplexity_simple = metrics_simple.get('perplexity', 0.0)  # Use .get() to handle missing metrics\n",
    "    acc_simple = metrics_simple.get('sparse_categorical_accuracy', 0.0)\n",
    "    weighted_acc_simple = metrics_simple.get('weighted_sparse_categorical_accuracy', 0.0)\n",
    "\n",
    "    loss_1_mask = metrics_1_mask['loss']\n",
    "    perplexity_1_mask = metrics_1_mask.get('perplexity', 0.0)\n",
    "    acc_1_mask = metrics_1_mask.get('sparse_categorical_accuracy', 0.0)\n",
    "    weighted_acc_1_mask = metrics_1_mask.get('weighted_sparse_categorical_accuracy', 0.0)\n",
    "\n",
    "    print(f\"\\nRésultats :\")\n",
    "    print(f\"Modèle simple - Perte : {loss_simple:.4f}, Précision : {acc_simple:.4f}\")\n",
    "    print(f\"Modèle avec 1 masqué - Perte : {loss_1_mask:.4f}, Précision : {acc_1_mask:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"simple\": {\"loss\": loss_simple, \"accuracy\": acc_simple, \"perplexity\": perplexity_simple, \"weighted_accuracy\": weighted_acc_simple},\n",
    "        \"with_1_mask\": {\"loss\": loss_1_mask, \"accuracy\": acc_1_mask, \"perplexity\": perplexity_1_mask, \"weighted_accuracy\": weighted_acc_1_mask},\n",
    "    }\n",
    "\n",
    "# Charger les deux modèles depuis les fichiers\n",
    "model_simple = keras.models.load_model(\"model_simple.keras\")\n",
    "model_with_1_mask = keras.models.load_model(\"model_with_1_mask.keras\", custom_objects={\"MaskToken1Layer\": MaskToken1Layer})\n",
    "\n",
    "# Évaluer\n",
    "results = evaluate_models(model_simple, model_with_1_mask, ds_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78a4e4",
   "metadata": {},
   "source": [
    "# Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "007970b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_t9_sequence, context_chars, input_tv, target_tv, max_length=100, context_size=5):\n",
    "    \"\"\"\n",
    "    Génère une séquence de caractères chinois à partir d'une séquence T9 et d'un contexte en sinogrammes.\n",
    "    \n",
    "    Args:\n",
    "        model: Modèle Keras entraîné.\n",
    "        input_t9_sequence: Chaîne de séquences T9 séparées par des espaces (ex. \"94664 486\").\n",
    "        context_chars: Liste de caractères chinois pour le contexte initial (ex. [\"经\", \"央\"]).\n",
    "        input_tv: Couche TextVectorization pour les entrées T9.\n",
    "        target_tv: Couche TextVectorization pour les caractères cibles.\n",
    "        max_length: Longueur maximale de la séquence générée.\n",
    "        context_size: Taille du contexte (nombre de caractères précédents utilisés).\n",
    "    \n",
    "    Returns:\n",
    "        Chaîne de caractères chinois générée.\n",
    "    \"\"\"\n",
    "    # Préparer l'entrée T9\n",
    "    t9_tokens = input_t9_sequence.strip().split(\" \")\n",
    "    t9_tokens = t9_tokens[:max_length]  # Limiter à max_length\n",
    "    if not t9_tokens:\n",
    "        return \"\"\n",
    "    \n",
    "    # Vectoriser les tokens T9\n",
    "    t9_vectorized = input_tv(t9_tokens).to_tensor(default_value=0, shape=(len(t9_tokens), 6))\n",
    "    t9_vectorized = tf.expand_dims(t9_vectorized, axis=0)  # Shape: (1, seq_len, 6)\n",
    "    \n",
    "    # Initialiser le contexte avec les caractères fournis\n",
    "    context = []\n",
    "    if context_chars:\n",
    "        # Vectoriser les caractères de contexte\n",
    "        context_ids = target_tv(context_chars).to_tensor(default_value=0).numpy().flatten().tolist()\n",
    "        context.extend(context_ids)\n",
    "    # Remplir avec des zéros si le contexte est trop court\n",
    "    while len(context) < context_size:\n",
    "        context.insert(0, 0)  # Padding avec 0 (sera masqué)\n",
    "    # Tronquer si le contexte est trop long\n",
    "    context = context[-context_size:]\n",
    "    \n",
    "    # Initialiser la séquence générée\n",
    "    generated_chars = []\n",
    "    \n",
    "    # Générer caractère par caractère\n",
    "    for i in range(len(t9_tokens)):\n",
    "        # Préparer le contexte\n",
    "        context_tensor = tf.constant([context[-context_size:]], dtype=tf.int64)\n",
    "        context_tensor = tf.expand_dims(context_tensor, axis=1)  # Shape: (1, 1, context_size)\n",
    "        context_tensor = tf.repeat(context_tensor, repeats=tf.shape(t9_vectorized)[1], axis=1)\n",
    "        \n",
    "        # Prédire le caractère suivant\n",
    "        inputs = {\n",
    "            \"t9_input\": t9_vectorized,\n",
    "            \"context_input\": context_tensor\n",
    "        }\n",
    "        predictions = model.predict(inputs, verbose=0)  # Shape: (1, seq_len, vocab_size)\n",
    "        \n",
    "        # Obtenir la prédiction pour la position actuelle\n",
    "        pred_char_idx = np.argmax(predictions[0, i], axis=-1)\n",
    "        pred_char = target_tv.get_vocabulary()[pred_char_idx]\n",
    "        \n",
    "        # Ajouter le caractère généré (sauf si c'est un token spécial)\n",
    "        if pred_char not in ['', '[UNK]']:\n",
    "            generated_chars.append(pred_char)\n",
    "        \n",
    "        # Mettre à jour le contexte\n",
    "        context.append(int(pred_char_idx))\n",
    "        if len(context) > context_size:\n",
    "            context.pop(0)\n",
    "    \n",
    "    # Convertir la liste de caractères en chaîne\n",
    "    return ''.join(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "776d22ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case: Générer '中国' avec contexte '经央'\n",
      "Expected: 中国\n",
      "Generated: 重国\n",
      "--------------------------------------------------\n",
      "WARNING:tensorflow:Using a while_loop for converting CudnnRNNV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting CudnnRNNV3 cause there is no registered converter for this op.\n",
      "Test case: Générer '经中央' avec contexte '广大'\n",
      "Expected: 经中央\n",
      "Generated: 精众网\n",
      "--------------------------------------------------\n",
      "Test case: Générer '广大党' avec contexte '中国'\n",
      "Expected: 广大党\n",
      "Generated: 广大方\n",
      "--------------------------------------------------\n",
      "Test case: Tester le masquage de l'ID de token 1 avec '中国' et le contexte '党'\n",
      "Expected: 中国\n",
      "Generated: 组织国\n",
      "--------------------------------------------------\n",
      "Test case: Tester le masquage de l'ID de token 0 avec '中国' et le contexte '经央广'\n",
      "Expected: 中国\n",
      "Generated: 路中国\n",
      "--------------------------------------------------\n",
      "Test case: Générer '中国经中央' sans contexte\n",
      "Expected: 中国经中央\n",
      "Generated: 中国经重要\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"t9_input\": \"94664 486\",  # 中国\n",
    "        \"context_chars\": [\"经\", \"央\"],  # Contexte: caractères précédents\n",
    "        \"description\": \"Générer '中国' avec contexte '经央'\",\n",
    "        \"expected\": \"中国\"\n",
    "    },\n",
    "    {\n",
    "        \"t9_input\": \"5464 94664 9264\",  # 经中央\n",
    "        \"context_chars\": [\"广\", \"大\"],  # Contexte: caractères précédents\n",
    "        \"description\": \"Générer '经中央' avec contexte '广大'\",\n",
    "        \"expected\": \"经中央\"\n",
    "    },\n",
    "    {\n",
    "        \"t9_input\": \"48264 32 3264\",  # 广大党\n",
    "        \"context_chars\": [\"中\", \"国\"],  # Contexte: caractères précédents\n",
    "        \"description\": \"Générer '广大党' avec contexte '中国'\",\n",
    "        \"expected\": \"广大党\"\n",
    "    },\n",
    "    {\n",
    "        \"t9_input\": \"1 94664 486\",  # masked token + 中国\n",
    "        \"context_chars\": [\"党\"],  # Contexte minimum\n",
    "        \"description\": \"Tester le masquage de l'ID de token 1 avec '中国' et le contexte '党'\",\n",
    "        \"expected\": \"中国\"  # Token ID 1 should be ignored\n",
    "    },\n",
    "    {\n",
    "        \"t9_input\": \"0 94664 486\",  # Padding + 中国\n",
    "        \"context_chars\": [\"经\", \"央\", \"广\"],  # Contexte long\n",
    "        \"description\": \"Tester le masquage de l'ID de token 0 avec '中国' et le contexte '经央广'\",\n",
    "        \"expected\": \"中国\"  # Token ID 0 should be ignored\n",
    "    },\n",
    "    {\n",
    "        \"t9_input\": \"94664 486 5464 94664 9264\",  # 中国经中央\n",
    "        \"context_chars\": [],  # Aucun contexte\n",
    "        \"description\": \"Générer '中国经中央' sans contexte\",\n",
    "        \"expected\": \"中国经中央\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for test_case in test_cases:\n",
    "    generated_text = generate_text(\n",
    "        model_simple,\n",
    "        test_case[\"t9_input\"],\n",
    "        test_case[\"context_chars\"],\n",
    "        input_tv,\n",
    "        target_tv\n",
    "    )\n",
    "    print(f\"Test case: {test_case['description']}\")\n",
    "    print(f\"Expected: {test_case['expected']}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995ba12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case: Générer '中国' avec contexte '经央'\n",
      "Expected: 中国\n",
      "Generated: 中国\n",
      "--------------------------------------------------\n",
      "WARNING:tensorflow:Using a while_loop for converting CudnnRNNV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting CudnnRNNV3 cause there is no registered converter for this op.\n",
      "Test case: Générer '经中央' avec contexte '广大'\n",
      "Expected: 经中央\n",
      "Generated: 精中央\n",
      "--------------------------------------------------\n",
      "Test case: Générer '广大党' avec contexte '中国'\n",
      "Expected: 广大党\n",
      "Generated: 广大防\n",
      "--------------------------------------------------\n",
      "Test case: Tester le masquage de l'ID de token 1 avec '中国' et le contexte '党'\n",
      "Expected: 中国\n",
      "Generated: 组织国\n",
      "--------------------------------------------------\n",
      "Test case: Tester le masquage de l'ID de token 0 avec '中国' et le contexte '经央广'\n",
      "Expected: 中国\n",
      "Generated: 务中国\n",
      "--------------------------------------------------\n",
      "Test case: Générer '中国经中央' sans contexte\n",
      "Expected: 中国经中央\n",
      "Generated: 中国经中央\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for test_case in test_cases:\n",
    "    generated_text = generate_text(\n",
    "        model_with_1_mask,\n",
    "        test_case[\"t9_input\"],\n",
    "        test_case[\"context_chars\"],\n",
    "        input_tv,\n",
    "        target_tv\n",
    "    )\n",
    "    print(f\"Test case: {test_case['description']}\")\n",
    "    print(f\"Expected: {test_case['expected']}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_metal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
